{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNpSu0EWJFMo",
        "outputId": "75bd901c-1552-483b-d406-9c4063a7ae6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing old incompatible checkpoints...\n",
            "Removing old resutls...\n"
          ]
        }
      ],
      "source": [
        "# Delete old incompatible checkpoints and start fresh\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "checkpoint_dir = '/content/checkpoints'\n",
        "results_dir = '/content/results'\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"Removing old incompatible checkpoints...\")\n",
        "    shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "if os.path.exists(results_dir):\n",
        "    print(\"Removing old resutls...\")\n",
        "    shutil.rmtree(results_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5l146FW4VC_A"
      },
      "outputs": [],
      "source": [
        "mkdir checkpoints results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hpN9iT_UaYB",
        "outputId": "2981e79c-a571-4ca7-f3e6-ffa3aa6f0964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mresults\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "uQen0Pmky7y-",
        "outputId": "81be5376-76a3-4ac8-9768-bc108ce1ccbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "VQ-VAE EMOJI GENERATION - PHASE 1: CODEBOOK TRAINING\n",
            "================================================================================\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './emoji_data'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2380630540.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2380630540.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0mdownload_emoji_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './emoji_data'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "VQ-VAE Emoji Generation - Phase 1: Codebook Training\n",
        "Focus on achieving good codebook utilization (50-60%) before proceeding\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration Management\n",
        "# ============================================================================\n",
        "\n",
        "# Define all configurations in JSON format\n",
        "EXPERIMENT_CONFIGS = {\n",
        "    # Paths\n",
        "    \"data_dir\": \"./emoji_data\",\n",
        "    \"checkpoint_dir\": \"./checkpoints\",\n",
        "    \"results_dir\": \"./results\",\n",
        "\n",
        "    # Data\n",
        "    \"image_size\": 64,\n",
        "    \"batch_size\": 64,\n",
        "    \"num_workers\": 2,\n",
        "\n",
        "    # VQ-VAE Architecture\n",
        "    \"num_hiddens\": 128,\n",
        "    \"num_residual_hiddens\": 32,\n",
        "    \"num_residual_layers\": 2,\n",
        "    \"embedding_dim\": 64,\n",
        "    \"num_embeddings\": 256,\n",
        "    \"commitment_cost\": 0.01,\n",
        "    \"decay\": 0.95,\n",
        "\n",
        "    # Training\n",
        "    \"num_epochs_vqvae\": 100,\n",
        "    \"learning_rate_vqvae\": 3e-4,\n",
        "\n",
        "    # Codebook monitoring\n",
        "    \"min_codebook_usage\": 50.0,\n",
        "    \"check_usage_every\": 5,\n",
        "\n",
        "    # Experiment metadata\n",
        "    \"experiment_name\": \"commitment_cost_10\",\n",
        "    \"notes\": \"Decreasing commitment cost to 10\"\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class that loads from JSON dict\"\"\"\n",
        "    def __init__(self, config_dict):\n",
        "        for key, value in config_dict.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "        # Add device (not in JSON as it's system-dependent)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Convert config back to dictionary for CSV export\"\"\"\n",
        "        return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}\n",
        "\n",
        "config = Config(EXPERIMENT_CONFIGS)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Dataset\n",
        "# ============================================================================\n",
        "\n",
        "class EmojiDataset(Dataset):\n",
        "    def __init__(self, data_dir, image_size=64, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.image_size = image_size\n",
        "        self.image_files = [f for f in os.listdir(data_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((image_size, image_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        return self.transform(image)\n",
        "\n",
        "def download_emoji_dataset():\n",
        "    print(\"Downloading emoji dataset...\")\n",
        "    try:\n",
        "        from datasets import load_dataset\n",
        "        dataset = load_dataset(\"valhalla/emoji-dataset\", split=\"train\")\n",
        "\n",
        "        print(f\"Downloaded {len(dataset)} emojis\")\n",
        "        print(\"Saving images to disk...\")\n",
        "\n",
        "        for idx, item in enumerate(tqdm(dataset)):\n",
        "            img = item['image']\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')\n",
        "            img.save(os.path.join(config.data_dir, f'emoji_{idx:05d}.png'))\n",
        "\n",
        "        print(f\"Saved {len(dataset)} emoji images to {config.data_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading dataset: {e}\")\n",
        "\n",
        "# ============================================================================\n",
        "# VQ-VAE Components\n",
        "# ============================================================================\n",
        "\n",
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay=0.99, epsilon=1e-5):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.commitment_cost = commitment_cost\n",
        "        self.decay = decay\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        embed = torch.randn(num_embeddings, embedding_dim)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(num_embeddings))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        input_shape = inputs.shape\n",
        "        flat_input = inputs.permute(0, 2, 3, 1).contiguous().view(-1, self.embedding_dim)\n",
        "\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self.embed**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self.embed.t()))\n",
        "\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        quantized = torch.matmul(encodings, self.embed)\n",
        "\n",
        "        if self.training:\n",
        "            self.cluster_size.data.mul_(self.decay).add_(encodings.sum(0), alpha=1 - self.decay)\n",
        "            dw = torch.matmul(encodings.t(), flat_input)\n",
        "            self.embed_avg.data.mul_(self.decay).add_(dw, alpha=1 - self.decay)\n",
        "\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = ((self.cluster_size + self.epsilon) / (n + self.num_embeddings * self.epsilon) * n)\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), flat_input)\n",
        "        loss = self.commitment_cost * e_latent_loss\n",
        "        quantized = flat_input + (quantized - flat_input).detach()\n",
        "\n",
        "        quantized = quantized.view(input_shape[0], input_shape[2], input_shape[3], self.embedding_dim)\n",
        "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        return quantized, loss, perplexity, encoding_indices\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels, num_residual_hiddens, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_residual_hiddens),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(num_residual_hiddens, num_hiddens, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_hiddens)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            ResidualBlock(in_channels, num_hiddens, num_residual_hiddens)\n",
        "            for _ in range(num_residual_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, num_hiddens // 2, 4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(num_hiddens // 2, num_hiddens, 4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(num_hiddens, num_hiddens, 3, padding=1)\n",
        "        self.residual_stack = ResidualStack(num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        return self.residual_stack(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(embedding_dim, num_hiddens, 3, padding=1)\n",
        "        self.residual_stack = ResidualStack(num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
        "        self.conv_trans1 = nn.ConvTranspose2d(num_hiddens, num_hiddens // 2, 4, stride=2, padding=1)\n",
        "        self.conv_trans2 = nn.ConvTranspose2d(num_hiddens // 2, 3, 4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.residual_stack(x)\n",
        "        x = F.relu(self.conv_trans1(x))\n",
        "        return torch.tanh(self.conv_trans2(x))\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(3, config.num_hiddens, config.num_residual_layers, config.num_residual_hiddens)\n",
        "        self.pre_vq_conv = nn.Conv2d(config.num_hiddens, config.embedding_dim, 1)\n",
        "        self.vq = VectorQuantizerEMA(config.num_embeddings, config.embedding_dim, config.commitment_cost, decay=config.decay)\n",
        "        self.decoder = Decoder(config.embedding_dim, config.num_hiddens, config.num_residual_layers, config.num_residual_hiddens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        z = self.pre_vq_conv(z)\n",
        "        quantized, vq_loss, perplexity, encoding_indices = self.vq(z)\n",
        "        x_recon = self.decoder(quantized)\n",
        "        return x_recon, vq_loss, perplexity, encoding_indices\n",
        "\n",
        "    def encode(self, x):\n",
        "        z = self.encoder(x)\n",
        "        z = self.pre_vq_conv(z)\n",
        "        _, _, _, encoding_indices = self.vq(z)\n",
        "        B = x.shape[0]\n",
        "        return encoding_indices.view(B, -1)\n",
        "\n",
        "# ============================================================================\n",
        "# Metrics\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_mse(original, reconstructed):\n",
        "    return F.mse_loss(reconstructed, original).item()\n",
        "\n",
        "def calculate_psnr(original, reconstructed, max_val=2.0):\n",
        "    mse = F.mse_loss(reconstructed, original)\n",
        "    return (10 * torch.log10(max_val**2 / mse)).item()\n",
        "\n",
        "def calculate_ssim(original, reconstructed):\n",
        "    orig_np = (original.detach().cpu().numpy() + 1) / 2\n",
        "    recon_np = (reconstructed.detach().cpu().numpy() + 1) / 2\n",
        "    ssim_scores = [ssim(orig_np[i].transpose(1, 2, 0), recon_np[i].transpose(1, 2, 0),\n",
        "                       channel_axis=2, data_range=1.0) for i in range(orig_np.shape[0])]\n",
        "    return np.mean(ssim_scores)\n",
        "\n",
        "def calculate_codebook_usage(model, dataloader, device):\n",
        "    \"\"\"Calculate codebook utilization percentage\"\"\"\n",
        "    model.eval()\n",
        "    all_codes = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(device)\n",
        "            codes = model.encode(batch)\n",
        "            all_codes.append(codes.cpu())\n",
        "\n",
        "    all_codes = torch.cat(all_codes, dim=0).numpy().flatten()\n",
        "    unique_codes = len(np.unique(all_codes))\n",
        "    usage_percentage = (unique_codes / config.num_embeddings) * 100\n",
        "\n",
        "    return usage_percentage, unique_codes, all_codes\n",
        "\n",
        "# ============================================================================\n",
        "# Trainer with Codebook Monitoring\n",
        "# ============================================================================\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, config):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate_vqvae)\n",
        "        self.history = {'recon_loss': [], 'vq_loss': [], 'total_loss': [], 'perplexity': [],\n",
        "                       'mse': [], 'psnr': [], 'ssim': [], 'codebook_usage': []}\n",
        "        self.start_epoch = 0\n",
        "        self.best_usage = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch, filepath):\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'history': self.history\n",
        "        }\n",
        "        torch.save(checkpoint, filepath)\n",
        "        print(f\"âœ“ Checkpoint saved: {filepath}\")\n",
        "\n",
        "    def load_checkpoint(self, filepath):\n",
        "        if os.path.exists(filepath):\n",
        "            try:\n",
        "                checkpoint = torch.load(filepath, map_location=self.config.device)\n",
        "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                self.history = checkpoint['history']\n",
        "                self.start_epoch = checkpoint['epoch'] + 1\n",
        "                print(f\"âœ“ Checkpoint loaded, resuming from epoch {self.start_epoch}\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"âœ— Error loading checkpoint: {e}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "    def train_epoch(self, dataloader):\n",
        "        self.model.train()\n",
        "        epoch_recon_loss = epoch_vq_loss = epoch_perplexity = 0\n",
        "        pbar = tqdm(dataloader, desc=\"Training\")\n",
        "        for batch in pbar:\n",
        "            batch = batch.to(self.config.device)\n",
        "            recon, vq_loss, perplexity, _ = self.model(batch)\n",
        "            recon_loss = F.mse_loss(recon, batch)\n",
        "            loss = recon_loss + vq_loss\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            epoch_recon_loss += recon_loss.item()\n",
        "            epoch_vq_loss += vq_loss.item()\n",
        "            epoch_perplexity += perplexity.item()\n",
        "            pbar.set_postfix({'recon_loss': recon_loss.item(), 'vq_loss': vq_loss.item(), 'perplexity': perplexity.item()})\n",
        "        n = len(dataloader)\n",
        "        return {'recon_loss': epoch_recon_loss/n, 'vq_loss': epoch_vq_loss/n, 'perplexity': epoch_perplexity/n}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        all_original, all_recon = [], []\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(self.config.device)\n",
        "            recon, _, _, _ = self.model(batch)\n",
        "            all_original.append(batch)\n",
        "            all_recon.append(recon)\n",
        "        original = torch.cat(all_original, dim=0)\n",
        "        reconstructed = torch.cat(all_recon, dim=0)\n",
        "        return {'mse': calculate_mse(original, reconstructed),\n",
        "                'psnr': calculate_psnr(original, reconstructed),\n",
        "                'ssim': calculate_ssim(original[:64], reconstructed[:64])}\n",
        "\n",
        "    def train(self, train_loader, val_loader):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"PHASE 1: VQ-VAE TRAINING WITH CODEBOOK MONITORING\")\n",
        "        print(f\"Target: â‰¥{self.config.min_codebook_usage}% codebook utilization\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        for epoch in range(self.start_epoch, self.config.num_epochs_vqvae):\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs_vqvae}\")\n",
        "\n",
        "            train_metrics = self.train_epoch(train_loader)\n",
        "            val_metrics = self.evaluate(val_loader)\n",
        "\n",
        "            # Check codebook usage periodically\n",
        "            if (epoch + 1) % self.config.check_usage_every == 0:\n",
        "                usage_pct, unique_codes, _ = calculate_codebook_usage(self.model, train_loader, self.config.device)\n",
        "                self.history['codebook_usage'].append(usage_pct)\n",
        "\n",
        "                print(f\"\\n{'â”€'*60}\")\n",
        "                print(f\"ðŸ“Š CODEBOOK USAGE CHECK (Epoch {epoch+1})\")\n",
        "                print(f\"{'â”€'*60}\")\n",
        "                print(f\"Active codes: {unique_codes}/{self.config.num_embeddings}\")\n",
        "                print(f\"Usage: {usage_pct:.2f}%\")\n",
        "\n",
        "                if usage_pct > self.best_usage:\n",
        "                    self.best_usage = usage_pct\n",
        "                    print(f\"âœ“ New best usage: {usage_pct:.2f}%\")\n",
        "                    self.save_checkpoint(epoch, os.path.join(self.config.checkpoint_dir, 'vqvae_best_usage.pt'))\n",
        "\n",
        "                if usage_pct >= self.config.min_codebook_usage:\n",
        "                    print(f\"âœ“ TARGET REACHED! ({usage_pct:.2f}% â‰¥ {self.config.min_codebook_usage}%)\")\n",
        "                else:\n",
        "                    remaining = self.config.min_codebook_usage - usage_pct\n",
        "                    print(f\"âš  Need {remaining:.2f}% more to reach target\")\n",
        "                print(f\"{'â”€'*60}\\n\")\n",
        "\n",
        "            metrics = {**train_metrics, **val_metrics}\n",
        "            for key, value in metrics.items():\n",
        "                self.history[key].append(value)\n",
        "\n",
        "            print(f\"Recon: {metrics['recon_loss']:.4f}, VQ: {metrics['vq_loss']:.4f}, Perplexity: {metrics['perplexity']:.2f}\")\n",
        "            print(f\"MSE: {metrics['mse']:.4f}, PSNR: {metrics['psnr']:.2f}, SSIM: {metrics['ssim']:.4f}\")\n",
        "\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                self.save_checkpoint(epoch, os.path.join(self.config.checkpoint_dir, f'vqvae_epoch_{epoch+1}.pt'))\n",
        "\n",
        "        self.save_checkpoint(self.config.num_epochs_vqvae - 1, os.path.join(self.config.checkpoint_dir, 'vqvae_final.pt'))\n",
        "\n",
        "        # Final usage check\n",
        "        final_usage, final_unique, _ = calculate_codebook_usage(self.model, train_loader, self.config.device)\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"PHASE 1 COMPLETE\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Final codebook usage: {final_usage:.2f}% ({final_unique}/{self.config.num_embeddings})\")\n",
        "        print(f\"Best codebook usage: {self.best_usage:.2f}%\")\n",
        "\n",
        "        if final_usage >= self.config.min_codebook_usage:\n",
        "            print(f\"âœ“ SUCCESS: Codebook utilization target achieved!\")\n",
        "            return True, self.history\n",
        "        else:\n",
        "            print(f\"âš  WARNING: Codebook utilization below target ({final_usage:.2f}% < {self.config.min_codebook_usage}%)\")\n",
        "            print(f\"Consider: 1) Training longer, 2) Reducing num_embeddings, 3) Adjusting commitment_cost\")\n",
        "            return False, self.history\n",
        "\n",
        "# ============================================================================\n",
        "# Visualization\n",
        "# ============================================================================\n",
        "\n",
        "def plot_training_history(history, save_path):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes[0, 0].plot(history['recon_loss'])\n",
        "    axes[0, 0].set_title('Reconstruction Loss')\n",
        "    axes[0, 1].plot(history['vq_loss'])\n",
        "    axes[0, 1].set_title('VQ Loss')\n",
        "    axes[0, 2].plot(history['perplexity'])\n",
        "    axes[0, 2].set_title('Perplexity')\n",
        "    axes[1, 0].plot(history['mse'])\n",
        "    axes[1, 0].set_title('MSE')\n",
        "    axes[1, 1].plot(history['psnr'])\n",
        "    axes[1, 1].set_title('PSNR')\n",
        "    axes[1, 2].plot(history['ssim'])\n",
        "    axes[1, 2].set_title('SSIM')\n",
        "    for ax in axes.flat:\n",
        "        ax.grid(True)\n",
        "        ax.set_xlabel('Epoch')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_reconstructions(model, dataloader, device, save_path, num_images=8):\n",
        "    model.eval()\n",
        "    images = next(iter(dataloader))[:num_images].to(device)\n",
        "    with torch.no_grad():\n",
        "        reconstructions, _, _, _ = model(images)\n",
        "    images, reconstructions = (images + 1) / 2, (reconstructions + 1) / 2\n",
        "    comparison = torch.cat([images, reconstructions])\n",
        "    grid = make_grid(comparison, nrow=num_images, padding=2)\n",
        "    plt.figure(figsize=(16, 4))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.title('Top: Original, Bottom: Reconstructed')\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_codebook_usage(history, all_codes, save_path):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Usage over time\n",
        "    if len(history['codebook_usage']) > 0:\n",
        "        epochs = [i * config.check_usage_every for i in range(1, len(history['codebook_usage']) + 1)]\n",
        "        axes[0].plot(epochs, history['codebook_usage'], marker='o', linewidth=2)\n",
        "        axes[0].axhline(y=config.min_codebook_usage, color='r', linestyle='--', label=f'Target ({config.min_codebook_usage}%)')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Codebook Usage (%)')\n",
        "        axes[0].set_title('Codebook Utilization Over Time')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Histogram\n",
        "    axes[1].hist(all_codes, bins=config.num_embeddings, edgecolor='black')\n",
        "    axes[1].set_xlabel('Code Index')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    axes[1].set_title(f'Code Distribution (Final)')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def save_results_to_csv(config, results, save_path):\n",
        "    \"\"\"Save experiment config and results to CSV\"\"\"\n",
        "    import csv\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Combine config and results\n",
        "    row_data = {\n",
        "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        **config.to_dict(),\n",
        "        **results\n",
        "    }\n",
        "\n",
        "    # Check if file exists to determine if we need headers\n",
        "    file_exists = os.path.exists(save_path)\n",
        "\n",
        "    with open(save_path, 'a', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=row_data.keys())\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(row_data)\n",
        "\n",
        "    print(f\"Results appended to: {save_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Main\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*80)\n",
        "    print(\"VQ-VAE EMOJI GENERATION - PHASE 1: CODEBOOK TRAINING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Dataset\n",
        "    if not os.listdir(config.data_dir):\n",
        "        download_emoji_dataset()\n",
        "\n",
        "    dataset = EmojiDataset(config.data_dir, config.image_size)\n",
        "    train_size = int(0.9 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers, pin_memory=True)\n",
        "\n",
        "    print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
        "\n",
        "    # Train VQ-VAE\n",
        "    vqvae = VQVAE(config).to(config.device)\n",
        "    trainer = Trainer(vqvae, config)\n",
        "\n",
        "    if not trainer.load_checkpoint(os.path.join(config.checkpoint_dir, 'vqvae_final.pt')):\n",
        "        success, history = trainer.train(train_loader, val_loader)\n",
        "        plot_training_history(history, os.path.join(config.results_dir, 'training_history.png'))\n",
        "\n",
        "        # Final codebook analysis\n",
        "        usage_pct, unique_codes, all_codes = calculate_codebook_usage(vqvae, train_loader, config.device)\n",
        "        plot_codebook_usage(history, all_codes, os.path.join(config.results_dir, 'codebook_usage.png'))\n",
        "\n",
        "        if not success:\n",
        "            print(\"\\nâš  Phase 1 did not reach target codebook usage. Review suggestions above.\")\n",
        "            return\n",
        "\n",
        "    # Visualize results\n",
        "    plot_reconstructions(vqvae, val_loader, config.device, os.path.join(config.results_dir, 'reconstructions.png'))\n",
        "\n",
        "    # Save summary\n",
        "    usage_pct, unique_codes, _ = calculate_codebook_usage(vqvae, train_loader, config.device)\n",
        "    summary = {\n",
        "        'codebook_usage_percent': float(usage_pct),\n",
        "        'active_codes': int(unique_codes),\n",
        "        'total_codes': config.num_embeddings,\n",
        "        'final_perplexity': trainer.history['perplexity'][-1] if trainer.history['perplexity'] else None,\n",
        "        'target_achieved': usage_pct >= config.min_codebook_usage\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(config.results_dir, 'phase1_summary.json'), 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "\n",
        "    # Prepare results for CSV\n",
        "    results = {\n",
        "        'phase': 'phase1',\n",
        "        'codebook_usage_percent': float(usage_pct),\n",
        "        'active_codes': int(unique_codes),\n",
        "        'best_codebook_usage': trainer.best_usage,\n",
        "        'final_perplexity': trainer.history['perplexity'][-1] if trainer.history['perplexity'] else None,\n",
        "        'final_recon_loss': trainer.history['recon_loss'][-1] if trainer.history['recon_loss'] else None,\n",
        "        'final_vq_loss': trainer.history['vq_loss'][-1] if trainer.history['vq_loss'] else None,\n",
        "        'final_mse': trainer.history['mse'][-1] if trainer.history['mse'] else None,\n",
        "        'final_psnr': trainer.history['psnr'][-1] if trainer.history['psnr'] else None,\n",
        "        'final_ssim': trainer.history['ssim'][-1] if trainer.history['ssim'] else None,\n",
        "        'target_achieved': usage_pct >= config.min_codebook_usage,\n",
        "        'training_completed': True\n",
        "    }\n",
        "\n",
        "    # Save to CSV\n",
        "    csv_path = os.path.join(config.results_dir, 'experiment_results.csv')\n",
        "    save_results_to_csv(config, results, csv_path)\n",
        "\n",
        "    # Also save JSON for Phase 2 compatibility\n",
        "    with open(os.path.join(config.results_dir, 'phase1_summary.json'), 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nâœ“ Phase 1 complete. Results saved to {config.results_dir}\")\n",
        "    print(f\"âœ“ Ready for Phase 2: Prior training and generation\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "576b7be713b248d8af68dbb257f6ac07",
            "7760bc39170e4a578724d1a9766406b1",
            "7147be28ef8a4feca9a1f36162506804",
            "86a1f0e9f6f64425a69a60ae949822c7",
            "b3ad216bc9174627b39b224d08f201f8",
            "2a68a9c162504d079433b14e3a2c9486",
            "d7cde14fe3bd42229a49826000ccc342",
            "3a89491b9cc7431899758b2ce285aab4",
            "9c5ccbfb353342eea79f25585522f439",
            "a11ed5b605e54b8fa80718c491ce98af",
            "dba10b82b9424edfbd650294726440bb",
            "a3d263cd9b8646aea1c258cfa57f8ff1",
            "b7c6723b741a4202a49d3e54c68a2373",
            "353f4c230fcd4491bbed14a7042fcfc2",
            "dc5df4a030d542f8b91ed37006357395",
            "2fb221bdfd1f4c459ce7c6d5c2bada65",
            "4a6f3f5994e947d78e24d047ac97b095",
            "8ef33255cd59430296314c33959134bc",
            "05fbc5784b7c4ac2aceb0117e73bec1d",
            "ca3f9c49a9ec40678202491e6a9b444d",
            "5eac55ad9f4543feaeece364932efe2c",
            "6e5df2d2a61e4a71a49cf97a45d2b7b0",
            "208833f5c22b4298a7070363cfb3eeb3",
            "6c2ecd1799d9460f898f4ba34f905d70",
            "2027ce8946bb4d7c995dd2b9e9c05b46",
            "a0d8542fdecd4eb79ab74ce6bdecff6f",
            "5a3d220ca4984ba087317a19d2a0ffb1",
            "bb50bed2b3f248cead2ef1282c647b0f",
            "d43d85bea2f34e4da080003146f092e6",
            "c27457a3aba64afa9a09541740f31dbd",
            "132f300edfcf448cbf3f0d4457f0eb15",
            "7db862cdfbc44ec982c4c203ffa3c9f0",
            "5674198464e846e4b9135c5d31b36d9e"
          ]
        },
        "id": "8X4_G3n9xTR_",
        "outputId": "6c0c750b-1972-4956-985d-85a38805053f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "================================================================================\n",
            "VQ-VAE Emoji Generation Project - ENHANCED VERSION\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 1: Dataset Preparation\n",
            "================================================================================\n",
            "Downloading emoji dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "576b7be713b248d8af68dbb257f6ac07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "dataset_infos.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3d263cd9b8646aea1c258cfa57f8ff1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001-38cc4fa96c139e(â€¦):   0%|          | 0.00/139M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "208833f5c22b4298a7070363cfb3eeb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/2749 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded 2749 emojis\n",
            "Saving images to disk...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2749/2749 [01:07<00:00, 40.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 2749 emoji images to ./emoji_data\n",
            "Total dataset size: 2749 images\n",
            "Training set: 2474 images\n",
            "Validation set: 275 images\n",
            "\n",
            "================================================================================\n",
            "STEP 2: Architecture Verification\n",
            "================================================================================\n",
            "\n",
            "Verifying architecture...\n",
            "After encoder: torch.Size([1, 128, 16, 16])\n",
            "After pre_vq_conv: torch.Size([1, 64, 16, 16])\n",
            "After VQ: torch.Size([1, 64, 16, 16])\n",
            "Codes shape: torch.Size([256, 1])\n",
            "After decoder: torch.Size([1, 3, 64, 64])\n",
            "\n",
            "Expected latent spatial size: 16x16\n",
            "\n",
            "Architecture verified! Latent spatial size: 16x16\n",
            "\n",
            "================================================================================\n",
            "STEP 3: Two-Phase VQ-VAE Training\n",
            "================================================================================\n",
            "VQ-VAE parameters: 663,363\n",
            "No checkpoint found, starting Phase 1 training from scratch...\n",
            "Starting Phase 1: Training until codebook utilization >= 50.0%\n",
            "\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:20<00:00,  2.07s/it, recon_loss=0.207, vq_loss=0.0394, perplexity=1.96]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:21<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.2723, VQ Loss: 0.0511, Perplexity: 2.26\n",
            "MSE: 0.2025, PSNR: 12.96, SSIM: 0.3323\n",
            "Codebook Utilization: 2/512 (0.39%)\n",
            "\n",
            "Epoch 2/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:17<00:00,  1.98s/it, recon_loss=0.135, vq_loss=0.0664, perplexity=2.56]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:21<00:00,  1.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.1588, VQ Loss: 0.0558, Perplexity: 2.08\n",
            "MSE: 0.1457, PSNR: 14.39, SSIM: 0.4645\n",
            "Codebook Utilization: 3/512 (0.59%)\n",
            "\n",
            "Epoch 3/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:16<00:00,  1.95s/it, recon_loss=0.104, vq_loss=0.0776, perplexity=2.66]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.1220, VQ Loss: 0.0742, Perplexity: 2.63\n",
            "MSE: 0.1097, PSNR: 15.62, SSIM: 0.5249\n",
            "Codebook Utilization: 3/512 (0.59%)\n",
            "\n",
            "Epoch 4/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:23<00:00,  2.14s/it, recon_loss=0.115, vq_loss=0.0765, perplexity=2.76]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.1067, VQ Loss: 0.0764, Perplexity: 2.73\n",
            "MSE: 0.1004, PSNR: 16.01, SSIM: 0.5633\n",
            "Codebook Utilization: 3/512 (0.59%)\n",
            "\n",
            "Epoch 5/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:17<00:00,  2.00s/it, recon_loss=0.0847, vq_loss=0.0721, perplexity=3.2]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:20<00:00,  1.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0955, VQ Loss: 0.0764, Perplexity: 2.88\n",
            "MSE: 0.1056, PSNR: 15.78, SSIM: 0.5809\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "Checkpoint saved: ./checkpoints/vqvae_phase1_epoch5.pt\n",
            "\n",
            "Epoch 6/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:18<00:00,  2.02s/it, recon_loss=0.0968, vq_loss=0.0557, perplexity=3.53]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:21<00:00,  1.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0837, VQ Loss: 0.0647, Perplexity: 3.33\n",
            "MSE: 0.0822, PSNR: 16.87, SSIM: 0.6152\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "\n",
            "Epoch 7/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:18<00:00,  2.02s/it, recon_loss=0.0671, vq_loss=0.067, perplexity=3.59]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:21<00:00,  1.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0735, VQ Loss: 0.0603, Perplexity: 3.62\n",
            "MSE: 0.0750, PSNR: 17.27, SSIM: 0.6421\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "\n",
            "Epoch 8/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:19<00:00,  2.04s/it, recon_loss=0.0727, vq_loss=0.0919, perplexity=3.63]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0723, VQ Loss: 0.0706, Perplexity: 3.65\n",
            "MSE: 0.0750, PSNR: 17.27, SSIM: 0.6366\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "\n",
            "Epoch 9/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:18<00:00,  2.02s/it, recon_loss=0.0816, vq_loss=0.0762, perplexity=3.67]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:23<00:00,  1.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0731, VQ Loss: 0.0723, Perplexity: 3.66\n",
            "MSE: 0.0710, PSNR: 17.51, SSIM: 0.6595\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "\n",
            "Epoch 10/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:18<00:00,  2.02s/it, recon_loss=0.0643, vq_loss=0.0697, perplexity=3.7]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:20<00:00,  1.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0671, VQ Loss: 0.0668, Perplexity: 3.68\n",
            "MSE: 0.0665, PSNR: 17.79, SSIM: 0.6682\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "Checkpoint saved: ./checkpoints/vqvae_phase1_epoch10.pt\n",
            "\n",
            "Epoch 11/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:21<00:00,  2.08s/it, recon_loss=0.0558, vq_loss=0.0564, perplexity=3.63]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0651, VQ Loss: 0.0675, Perplexity: 3.69\n",
            "MSE: 0.0731, PSNR: 17.38, SSIM: 0.6604\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "\n",
            "Epoch 12/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:18<00:00,  2.00s/it, recon_loss=0.0595, vq_loss=0.0569, perplexity=3.69]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0631, VQ Loss: 0.0664, Perplexity: 3.68\n",
            "MSE: 0.0616, PSNR: 18.13, SSIM: 0.6762\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "\n",
            "Epoch 13/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:17<00:00,  1.98s/it, recon_loss=0.062, vq_loss=0.0759, perplexity=3.61]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0627, VQ Loss: 0.0698, Perplexity: 3.69\n",
            "MSE: 0.0676, PSNR: 17.72, SSIM: 0.6641\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "\n",
            "Epoch 14/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:17<00:00,  1.98s/it, recon_loss=0.059, vq_loss=0.0615, perplexity=3.71]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:20<00:00,  1.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0632, VQ Loss: 0.0744, Perplexity: 3.70\n",
            "MSE: 0.0689, PSNR: 17.64, SSIM: 0.6718\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "\n",
            "Epoch 15/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:23<00:00,  2.14s/it, recon_loss=0.0599, vq_loss=0.0631, perplexity=3.67]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:21<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0639, VQ Loss: 0.0781, Perplexity: 3.68\n",
            "MSE: 0.0683, PSNR: 17.68, SSIM: 0.6713\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "Checkpoint saved: ./checkpoints/vqvae_phase1_epoch15.pt\n",
            "\n",
            "Epoch 16/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:16<00:00,  1.97s/it, recon_loss=0.0678, vq_loss=0.0678, perplexity=3.77]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:20<00:00,  1.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0642, VQ Loss: 0.0729, Perplexity: 3.68\n",
            "MSE: 0.0621, PSNR: 18.09, SSIM: 0.6760\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "\n",
            "Epoch 17/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:29<00:00,  2.29s/it, recon_loss=0.0729, vq_loss=0.0689, perplexity=3.71]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0617, VQ Loss: 0.0655, Perplexity: 3.69\n",
            "MSE: 0.0659, PSNR: 17.83, SSIM: 0.6787\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "\n",
            "Epoch 18/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:19<00:00,  2.05s/it, recon_loss=0.0628, vq_loss=0.0698, perplexity=3.74]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0602, VQ Loss: 0.0654, Perplexity: 3.70\n",
            "MSE: 0.0630, PSNR: 18.03, SSIM: 0.6816\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "\n",
            "Epoch 19/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:17<00:00,  1.99s/it, recon_loss=0.0656, vq_loss=0.0697, perplexity=3.74]\n",
            "Calculating codebook utilization: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:22<00:00,  1.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recon Loss: 0.0639, VQ Loss: 0.0728, Perplexity: 3.66\n",
            "MSE: 0.0642, PSNR: 17.95, SSIM: 0.6750\n",
            "Codebook Utilization: 4/512 (0.78%)\n",
            "\n",
            "Epoch 20/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [00:39<00:43,  2.15s/it, recon_loss=0.0673, vq_loss=0.0793, perplexity=3.71]"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\"\"\"\n",
        "VQ-VAE Emoji Generation Project - ENHANCED VERSION\n",
        "Complete implementation with improved codebook utilization monitoring and all analysis from both codes\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "import seaborn as sns\n",
        "from scipy import linalg\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    # Paths\n",
        "    data_dir = './emoji_data'\n",
        "    checkpoint_dir = './checkpoints'\n",
        "    results_dir = './results'\n",
        "\n",
        "    # Data\n",
        "    image_size = 64\n",
        "    batch_size = 64  # Reduced from 128 for stability\n",
        "    num_workers = 2\n",
        "\n",
        "    # VQ-VAE Architecture - UPDATED FOR BETTER CODEBOOK USAGE\n",
        "    num_hiddens = 128\n",
        "    num_residual_hiddens = 32\n",
        "    num_residual_layers = 2\n",
        "    embedding_dim = 64\n",
        "    num_embeddings = 512\n",
        "    commitment_cost = 0.25\n",
        "\n",
        "    # Add EMA for codebook (optional but helps)\n",
        "    decay = 0.999\n",
        "\n",
        "    # Training\n",
        "    num_epochs_vqvae = 100\n",
        "    learning_rate_vqvae = 3e-4\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Prior (PixelCNN) - UPDATED\n",
        "    num_epochs_prior = 50\n",
        "    learning_rate_prior = 1e-4  # Reduced from 3e-4 for stability\n",
        "    pixelcnn_layers = 12  # Reduced from 15\n",
        "    pixelcnn_hidden = 64  # Reduced from 128\n",
        "    grad_clip = 1.0  # Add gradient clipping\n",
        "\n",
        "    # Generation\n",
        "    num_samples = 64\n",
        "\n",
        "    # Interpolation\n",
        "    num_interpolation_steps = 10\n",
        "\n",
        "    # Codebook utilization threshold\n",
        "    codebook_utilization_threshold = 0.5  # 50% minimum utilization\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(config.results_dir, exist_ok=True)\n",
        "os.makedirs(config.data_dir, exist_ok=True)\n",
        "print(f\"Using device: {config.device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Dataset Preparation\n",
        "# ============================================================================\n",
        "class EmojiDataset(Dataset):\n",
        "    \"\"\"Dataset class for emoji images\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, image_size=64, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.image_size = image_size\n",
        "        self.image_files = [f for f in os.listdir(data_dir)\n",
        "                           if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((image_size, image_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "def download_emoji_dataset():\n",
        "    \"\"\"Download emoji dataset from HuggingFace\"\"\"\n",
        "    print(\"Downloading emoji dataset...\")\n",
        "    try:\n",
        "        from datasets import load_dataset\n",
        "        dataset = load_dataset(\"valhalla/emoji-dataset\", split=\"train\")\n",
        "\n",
        "        print(f\"Downloaded {len(dataset)} emojis\")\n",
        "        print(\"Saving images to disk...\")\n",
        "\n",
        "        for idx, item in enumerate(tqdm(dataset)):\n",
        "            img = item['image']\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')\n",
        "            img.save(os.path.join(config.data_dir, f'emoji{idx:05d}.png'))\n",
        "\n",
        "        print(f\"Saved {len(dataset)} emoji images to {config.data_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading dataset: {e}\")\n",
        "        print(\"Please manually download emojis to the data directory\")\n",
        "\n",
        "# ============================================================================\n",
        "# VQ-VAE Model Components\n",
        "# ============================================================================\n",
        "class VectorQuantizerEMA(nn.Module):\n",
        "    \"\"\"Vector Quantization with Exponential Moving Average\"\"\"\n",
        "\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay=0.99, epsilon=1e-5):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.commitment_cost = commitment_cost\n",
        "        self.decay = decay\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Initialize embeddings correctly - shape should be [num_embeddings, embedding_dim]\n",
        "        embed = torch.randn(num_embeddings, embedding_dim)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(num_embeddings))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs: [B, C, H, W]\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input: [B, C, H, W] -> [BHW, C]\n",
        "        flat_input = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        flat_input = flat_input.view(-1, self.embedding_dim)\n",
        "\n",
        "        # Calculate distances - fixed dimensions\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) +\n",
        "                    torch.sum(self.embed**2, dim=1) -\n",
        "                    2 * torch.matmul(flat_input, self.embed.t()))\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings,\n",
        "                               device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize\n",
        "        quantized = torch.matmul(encodings, self.embed)\n",
        "\n",
        "        # Use EMA to update the embedding vectors\n",
        "        if self.training:\n",
        "            # Update cluster sizes\n",
        "            self.cluster_size.data.mul_(self.decay).add_(\n",
        "                encodings.sum(0), alpha=1 - self.decay\n",
        "            )\n",
        "\n",
        "            # Update embedding averages\n",
        "            dw = torch.matmul(encodings.t(), flat_input)\n",
        "            self.embed_avg.data.mul_(self.decay).add_(dw, alpha=1 - self.decay)\n",
        "\n",
        "            # Normalize embeddings\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = (\n",
        "                (self.cluster_size + self.epsilon) /\n",
        "                (n + self.num_embeddings * self.epsilon) * n\n",
        "            )\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), flat_input)\n",
        "        loss = self.commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight-through estimator\n",
        "        quantized = flat_input + (quantized - flat_input).detach()\n",
        "\n",
        "        # Reshape back\n",
        "        quantized = quantized.view(input_shape[0], input_shape[2],\n",
        "                                   input_shape[3], self.embedding_dim)\n",
        "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        # Perplexity\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        return quantized, loss, perplexity, encoding_indices\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block for encoder/decoder\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels, num_residual_hiddens, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_residual_hiddens),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(num_residual_hiddens, num_hiddens, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_hiddens)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    \"\"\"Stack of residual blocks\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            ResidualBlock(in_channels, num_hiddens, num_residual_hiddens)\n",
        "            for _ in range(num_residual_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder network\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, num_hiddens // 2, 4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(num_hiddens // 2, num_hiddens, 4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(num_hiddens, num_hiddens, 3, padding=1)\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        return self.residual_stack(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder network\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(embedding_dim, num_hiddens, 3, padding=1)\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "        self.conv_trans1 = nn.ConvTranspose2d(num_hiddens, num_hiddens // 2,\n",
        "                                              4, stride=2, padding=1)\n",
        "        self.conv_trans2 = nn.ConvTranspose2d(num_hiddens // 2, 3,\n",
        "                                              4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.residual_stack(x)\n",
        "        x = F.relu(self.conv_trans1(x))\n",
        "        return torch.tanh(self.conv_trans2(x))\n",
        "\n",
        "# ============================================================================\n",
        "# FIX 2: Debug the spatial size issue\n",
        "# ============================================================================\n",
        "# Add this function to check architecture:\n",
        "def verify_architecture():\n",
        "    \"\"\"Verify the VQ-VAE produces correct spatial sizes\"\"\"\n",
        "    print(\"\\nVerifying architecture...\")\n",
        "    model = VQVAE(config).to(config.device)\n",
        "\n",
        "    # Test with dummy input\n",
        "    dummy_input = torch.randn(1, 3, 64, 64).to(config.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Check encoder output\n",
        "        z = model.encoder(dummy_input)\n",
        "        print(f\"After encoder: {z.shape}\")\n",
        "\n",
        "        z = model.pre_vq_conv(z)\n",
        "        print(f\"After pre_vq_conv: {z.shape}\")\n",
        "\n",
        "        # Check VQ\n",
        "        quantized, _, _, codes = model.vq(z)\n",
        "        print(f\"After VQ: {quantized.shape}\")\n",
        "        print(f\"Codes shape: {codes.shape}\")\n",
        "\n",
        "        # Check decoder\n",
        "        recon = model.decoder(quantized)\n",
        "        print(f\"After decoder: {recon.shape}\")\n",
        "\n",
        "        # Calculate spatial size\n",
        "        spatial_size = z.shape[2]\n",
        "        print(f\"\\nExpected latent spatial size: {spatial_size}x{spatial_size}\")\n",
        "\n",
        "    return model, spatial_size\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    \"\"\"Complete VQ-VAE model with EMA\"\"\"\n",
        "\n",
        "    def __init__(self, config, use_ema=True):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(3, config.num_hiddens, config.num_residual_layers,\n",
        "                              config.num_residual_hiddens)\n",
        "        self.pre_vq_conv = nn.Conv2d(config.num_hiddens, config.embedding_dim, 1)\n",
        "\n",
        "        # Choose VQ method\n",
        "        if use_ema:\n",
        "            self.vq = VectorQuantizerEMA(\n",
        "                config.num_embeddings,\n",
        "                config.embedding_dim,\n",
        "                config.commitment_cost,\n",
        "                decay=config.decay\n",
        "            )\n",
        "        else:\n",
        "            self.vq = VectorQuantizerEMA(\n",
        "                config.num_embeddings,\n",
        "                config.embedding_dim,\n",
        "                config.commitment_cost\n",
        "            )\n",
        "\n",
        "        self.decoder = Decoder(config.embedding_dim, config.num_hiddens,\n",
        "                              config.num_residual_layers, config.num_residual_hiddens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        z = self.pre_vq_conv(z)\n",
        "        quantized, vq_loss, perplexity, encoding_indices = self.vq(z)\n",
        "        x_recon = self.decoder(quantized)\n",
        "        return x_recon, vq_loss, perplexity, encoding_indices\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Encode to discrete codes\"\"\"\n",
        "        z = self.encoder(x)\n",
        "        z = self.pre_vq_conv(z)\n",
        "        _, _, _, encoding_indices = self.vq(z)\n",
        "        # Reshape to [B, H*W]\n",
        "        B = x.shape[0]\n",
        "        encoding_indices = encoding_indices.view(B, -1)\n",
        "        return encoding_indices\n",
        "\n",
        "    def decode_codes(self, encoding_indices, spatial_size):\n",
        "        \"\"\"Decode from discrete codes\"\"\"\n",
        "        # Reshape codes to spatial layout\n",
        "        codes = encoding_indices.view(-1, spatial_size, spatial_size)\n",
        "\n",
        "        # Get quantized vectors - FIXED for both versions\n",
        "        if hasattr(self.vq, 'embed'):  # EMA version\n",
        "            quantized = F.embedding(codes, self.vq.embed)  # Remove .t()\n",
        "        else:  # Original version\n",
        "            quantized = self.vq.embeddings(codes)\n",
        "\n",
        "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        # Decode\n",
        "        return self.decoder(quantized)\n",
        "\n",
        "# ============================================================================\n",
        "# PixelCNN Prior\n",
        "# ============================================================================\n",
        "class MaskedConv2d(nn.Conv2d):\n",
        "    \"\"\"Masked convolution for autoregressive modeling\"\"\"\n",
        "\n",
        "    def __init__(self, mask_type, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.register_buffer('mask', torch.zeros_like(self.weight))\n",
        "        self.create_mask(mask_type)\n",
        "\n",
        "    def create_mask(self, mask_type):\n",
        "        k = self.kernel_size[0]\n",
        "        self.mask[:, :, :k//2, :] = 1\n",
        "        self.mask[:, :, k//2, :k//2] = 1\n",
        "        if mask_type == 'B':\n",
        "            self.mask[:, :, k//2, k//2] = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.weight.data *= self.mask\n",
        "        return super().forward(x)\n",
        "\n",
        "class PixelCNNResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block for PixelCNN\"\"\"\n",
        "\n",
        "    def __init__(self, h):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MaskedConv2d('B', h, h, 1),\n",
        "            nn.BatchNorm2d(h),\n",
        "            nn.ReLU(),\n",
        "            MaskedConv2d('B', h, h, 1),\n",
        "            nn.BatchNorm2d(h)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv(x)\n",
        "\n",
        "class PixelCNN(nn.Module):\n",
        "    \"\"\"PixelCNN prior for discrete codes with stability improvements\"\"\"\n",
        "\n",
        "    def __init__(self, num_embeddings, num_layers=12, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "\n",
        "        # Input layer\n",
        "        self.input_conv = MaskedConv2d('A', num_embeddings, hidden_dim, 7, padding=3)\n",
        "\n",
        "        # Residual blocks\n",
        "        self.residual_blocks = nn.ModuleList([\n",
        "            PixelCNNResidualBlock(hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layers\n",
        "        self.output = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MaskedConv2d('B', hidden_dim, hidden_dim, 1),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_dim, num_embeddings, 1)\n",
        "        )\n",
        "\n",
        "        # Initialize weights properly\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights with smaller values for stability\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, MaskedConv2d)):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=0.1)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, H, W] with values in [0, num_embeddings)\n",
        "        # Convert to one-hot: [B, num_embeddings, H, W]\n",
        "        x_onehot = F.one_hot(x, self.num_embeddings).float()\n",
        "        x_onehot = x_onehot.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        x = self.input_conv(x_onehot)\n",
        "        for block in self.residual_blocks:\n",
        "            x = block(x)\n",
        "        return self.output(x)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, batch_size, spatial_size, device, temperature=1.0):\n",
        "        \"\"\"Generate samples autoregressively\"\"\"\n",
        "        samples = torch.zeros(batch_size, spatial_size, spatial_size,\n",
        "                            dtype=torch.long, device=device)\n",
        "\n",
        "        for i in range(spatial_size):\n",
        "            for j in range(spatial_size):\n",
        "                logits = self(samples)\n",
        "                probs = F.softmax(logits[:, :, i, j] / temperature, dim=1)\n",
        "                samples[:, i, j] = torch.multinomial(probs, 1).squeeze(-1)\n",
        "\n",
        "        return samples\n",
        "\n",
        "# ============================================================================\n",
        "# Metrics\n",
        "# ============================================================================\n",
        "def calculate_mse(original, reconstructed):\n",
        "    \"\"\"Calculate Mean Squared Error\"\"\"\n",
        "    mse = F.mse_loss(reconstructed, original)\n",
        "    return mse.item()\n",
        "\n",
        "def calculate_psnr(original, reconstructed, max_val=2.0):\n",
        "    \"\"\"Calculate Peak Signal-to-Noise Ratio\"\"\"\n",
        "    mse = F.mse_loss(reconstructed, original)\n",
        "    psnr = 10 * torch.log10(max_val**2 / mse)\n",
        "    return psnr.item()\n",
        "\n",
        "def calculate_ssim(original, reconstructed):\n",
        "    \"\"\"Calculate Structural Similarity Index\"\"\"\n",
        "    # Convert to numpy and normalize to [0, 1]\n",
        "    orig_np = (original.detach().cpu().numpy() + 1) / 2\n",
        "    recon_np = (reconstructed.detach().cpu().numpy() + 1) / 2\n",
        "\n",
        "    ssim_scores = []\n",
        "    for i in range(orig_np.shape[0]):\n",
        "        score = ssim(orig_np[i].transpose(1, 2, 0),\n",
        "                    recon_np[i].transpose(1, 2, 0),\n",
        "                    multichannel=True, data_range=1.0, channel_axis=2)\n",
        "        ssim_scores.append(score)\n",
        "\n",
        "    return np.mean(ssim_scores)\n",
        "\n",
        "def calculate_fid(real_features, fake_features):\n",
        "    \"\"\"Calculate FrÃ©chet Inception Distance\"\"\"\n",
        "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
        "    mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
        "\n",
        "    ssdiff = np.sum((mu1 - mu2)**2)\n",
        "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2*covmean)\n",
        "    return fid\n",
        "\n",
        "def get_inception_features(images, model, device):\n",
        "    \"\"\"Extract features using a pretrained model\"\"\"\n",
        "    from torchvision.models import inception_v3\n",
        "\n",
        "    if model is None:\n",
        "        model = inception_v3(pretrained=True, transform_input=False)\n",
        "        model.fc = nn.Identity()\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Resize images to 299x299 for Inception\n",
        "        images_resized = F.interpolate(images, size=(299, 299), mode='bilinear')\n",
        "        features = model(images_resized)\n",
        "\n",
        "    return features.cpu().numpy(), model\n",
        "\n",
        "# ============================================================================\n",
        "# Training Functions\n",
        "# ============================================================================\n",
        "class Trainer:\n",
        "    \"\"\"Trainer class for VQ-VAE\"\"\"\n",
        "\n",
        "    def __init__(self, model, config):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate_vqvae)\n",
        "        self.history = {\n",
        "            'recon_loss': [], 'vq_loss': [], 'total_loss': [],\n",
        "            'perplexity': [], 'mse': [], 'psnr': [], 'ssim': []\n",
        "        }\n",
        "        self.start_epoch = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch, filepath):\n",
        "        \"\"\"Save training checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'history': self.history\n",
        "        }\n",
        "        torch.save(checkpoint, filepath)\n",
        "        print(f\"Checkpoint saved: {filepath}\")\n",
        "\n",
        "    def load_checkpoint(self, filepath):\n",
        "        \"\"\"Load training checkpoint\"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            try:\n",
        "                # First try with weights_only=True (safe)\n",
        "                checkpoint = torch.load(filepath, map_location=self.config.device, weights_only=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Safe loading failed: {e}\")\n",
        "                print(\"Attempting unsafe load (only use if you trust the checkpoint source)...\")\n",
        "                # Fallback to unsafe load\n",
        "                checkpoint = torch.load(filepath, map_location=self.config.device, weights_only=False)\n",
        "\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.history = checkpoint['history']\n",
        "            self.start_epoch = checkpoint['epoch'] + 1\n",
        "            print(f\"Checkpoint loaded: {filepath}, resuming from epoch {self.start_epoch}\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def train_epoch(self, dataloader):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        epoch_recon_loss = 0\n",
        "        epoch_vq_loss = 0\n",
        "        epoch_perplexity = 0\n",
        "\n",
        "        pbar = tqdm(dataloader, desc=\"Training\")\n",
        "        for batch in pbar:\n",
        "            batch = batch.to(self.config.device)\n",
        "\n",
        "            # Forward pass\n",
        "            recon, vq_loss, perplexity, _ = self.model(batch)\n",
        "            recon_loss = F.mse_loss(recon, batch)\n",
        "            loss = recon_loss + vq_loss\n",
        "\n",
        "            # Backward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            epoch_recon_loss += recon_loss.item()\n",
        "            epoch_vq_loss += vq_loss.item()\n",
        "            epoch_perplexity += perplexity.item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'recon_loss': recon_loss.item(),\n",
        "                'vq_loss': vq_loss.item(),\n",
        "                'perplexity': perplexity.item()\n",
        "            })\n",
        "\n",
        "        n_batches = len(dataloader)\n",
        "        return {\n",
        "            'recon_loss': epoch_recon_loss / n_batches,\n",
        "            'vq_loss': epoch_vq_loss / n_batches,\n",
        "            'perplexity': epoch_perplexity / n_batches\n",
        "        }\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, dataloader):\n",
        "        \"\"\"Evaluate model\"\"\"\n",
        "        self.model.eval()\n",
        "        all_original = []\n",
        "        all_recon = []\n",
        "\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(self.config.device)\n",
        "            recon, _, _, _ = self.model(batch)\n",
        "            all_original.append(batch)\n",
        "            all_recon.append(recon)\n",
        "\n",
        "        original = torch.cat(all_original, dim=0)\n",
        "        reconstructed = torch.cat(all_recon, dim=0)\n",
        "\n",
        "        mse = calculate_mse(original, reconstructed)\n",
        "        psnr = calculate_psnr(original, reconstructed)\n",
        "        ssim_score = calculate_ssim(original[:64], reconstructed[:64])  # Sample for speed\n",
        "\n",
        "        return {'mse': mse, 'psnr': psnr, 'ssim': ssim_score}\n",
        "\n",
        "    def get_codebook_utilization(self, dataloader):\n",
        "        \"\"\"Calculate codebook utilization percentage\"\"\"\n",
        "        self.model.eval()\n",
        "        all_codes = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=\"Calculating codebook utilization\"):\n",
        "                batch = batch.to(self.config.device)\n",
        "                codes = self.model.encode(batch)\n",
        "                all_codes.append(codes.cpu())\n",
        "\n",
        "        all_codes = torch.cat(all_codes, dim=0).numpy().flatten()\n",
        "        unique_codes = len(np.unique(all_codes))\n",
        "        utilization_rate = unique_codes / self.config.num_embeddings\n",
        "\n",
        "        return utilization_rate, unique_codes\n",
        "\n",
        "    def train_phase1_codebook(self, train_loader, val_loader, target_utilization=0.5):\n",
        "        \"\"\"Phase 1: Train until codebook utilization reaches target\"\"\"\n",
        "        print(f\"Starting Phase 1: Training until codebook utilization >= {target_utilization*100}%\")\n",
        "\n",
        "        for epoch in range(self.start_epoch, self.config.num_epochs_vqvae):\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs_vqvae}\")\n",
        "\n",
        "            # Train\n",
        "            train_metrics = self.train_epoch(train_loader)\n",
        "\n",
        "            # Evaluate\n",
        "            val_metrics = self.evaluate(val_loader)\n",
        "\n",
        "            # Check codebook utilization\n",
        "            utilization_rate, unique_codes = self.get_codebook_utilization(train_loader)\n",
        "\n",
        "            # Combine metrics\n",
        "            metrics = {**train_metrics, **val_metrics}\n",
        "            metrics['utilization_rate'] = utilization_rate\n",
        "            metrics['unique_codes'] = unique_codes\n",
        "\n",
        "            for key, value in metrics.items():\n",
        "                if key in self.history:\n",
        "                    self.history[key].append(value)\n",
        "\n",
        "            # Print metrics\n",
        "            print(f\"Recon Loss: {metrics['recon_loss']:.4f}, \"\n",
        "                  f\"VQ Loss: {metrics['vq_loss']:.4f}, \"\n",
        "                  f\"Perplexity: {metrics['perplexity']:.2f}\")\n",
        "            print(f\"MSE: {metrics['mse']:.4f}, \"\n",
        "                  f\"PSNR: {metrics['psnr']:.2f}, \"\n",
        "                  f\"SSIM: {metrics['ssim']:.4f}\")\n",
        "            print(f\"Codebook Utilization: {unique_codes}/{self.config.num_embeddings} \"\n",
        "                  f\"({utilization_rate*100:.2f}%)\")\n",
        "\n",
        "            # Save checkpoint every 5 epochs\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                checkpoint_path = os.path.join(\n",
        "                    self.config.checkpoint_dir,\n",
        "                    f'vqvae_phase1_epoch{epoch+1}.pt'\n",
        "                )\n",
        "                self.save_checkpoint(epoch, checkpoint_path)\n",
        "\n",
        "            # Check if we've reached target utilization\n",
        "            if utilization_rate >= target_utilization:\n",
        "                print(f\"\\nðŸŽ‰ Target codebook utilization reached! \"\n",
        "                      f\"{utilization_rate*100:.2f}% >= {target_utilization*100}%\")\n",
        "                checkpoint_path = os.path.join(\n",
        "                    self.config.checkpoint_dir,\n",
        "                    f'vqvae_phase1_complete_epoch{epoch+1}.pt'\n",
        "                )\n",
        "                self.save_checkpoint(epoch, checkpoint_path)\n",
        "                return epoch + 1, metrics\n",
        "\n",
        "        print(f\"\\nâš ï¸  Target utilization not reached after {self.config.num_epochs_vqvae} epochs\")\n",
        "        return self.config.num_epochs_vqvae, metrics\n",
        "\n",
        "    def train_phase2_final(self, train_loader, val_loader, start_epoch):\n",
        "        \"\"\"Phase 2: Continue training for final quality\"\"\"\n",
        "        print(f\"\\nStarting Phase 2: Final training from epoch {start_epoch}\")\n",
        "\n",
        "        for epoch in range(start_epoch, self.config.num_epochs_vqvae):\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs_vqvae}\")\n",
        "\n",
        "            # Train\n",
        "            train_metrics = self.train_epoch(train_loader)\n",
        "\n",
        "            # Evaluate\n",
        "            val_metrics = self.evaluate(val_loader)\n",
        "\n",
        "            # Combine metrics\n",
        "            metrics = {**train_metrics, **val_metrics}\n",
        "            for key, value in metrics.items():\n",
        "                self.history[key].append(value)\n",
        "\n",
        "            # Print metrics\n",
        "            print(f\"Recon Loss: {metrics['recon_loss']:.4f}, \"\n",
        "                  f\"VQ Loss: {metrics['vq_loss']:.4f}, \"\n",
        "                  f\"Perplexity: {metrics['perplexity']:.2f}\")\n",
        "            print(f\"MSE: {metrics['mse']:.4f}, \"\n",
        "                  f\"PSNR: {metrics['psnr']:.2f}, \"\n",
        "                  f\"SSIM: {metrics['ssim']:.4f}\")\n",
        "\n",
        "            # Save checkpoint every 5 epochs\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                checkpoint_path = os.path.join(\n",
        "                    self.config.checkpoint_dir,\n",
        "                    f'vqvae_phase2_epoch{epoch+1}.pt'\n",
        "                )\n",
        "                self.save_checkpoint(epoch, checkpoint_path)\n",
        "\n",
        "        # Save final model\n",
        "        final_path = os.path.join(self.config.checkpoint_dir, 'vqvae_final.pt')\n",
        "        self.save_checkpoint(self.config.num_epochs_vqvae - 1, final_path)\n",
        "\n",
        "        return self.history\n",
        "\n",
        "# ============================================================================\n",
        "# Enhanced Analysis Functions (From Code 2)\n",
        "# ============================================================================\n",
        "def analyze_codebook_tsne(model, dataloader, device, save_path, num_samples=1000):\n",
        "    \"\"\"Enhanced t-SNE analysis of codebook usage\"\"\"\n",
        "    model.eval()\n",
        "    all_codes = []\n",
        "\n",
        "    print(\"Extracting codes for t-SNE analysis...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = batch.to(device)\n",
        "            codes = model.encode(batch)\n",
        "            all_codes.append(codes.cpu())\n",
        "            if len(all_codes) * config.batch_size >= num_samples:\n",
        "                break\n",
        "\n",
        "    all_codes = torch.cat(all_codes, dim=0).numpy()[:num_samples]\n",
        "\n",
        "    # Apply t-SNE if we have more than 2 dimensions\n",
        "    if all_codes.shape[1] > 2:\n",
        "        print(\"Applying t-SNE...\")\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "        codes_2d = tsne.fit_transform(all_codes)\n",
        "    else:\n",
        "        codes_2d = all_codes\n",
        "\n",
        "    # Plot t-SNE\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(codes_2d[:, 0], codes_2d[:, 1], alpha=0.6, s=10)\n",
        "    plt.title('t-SNE Visualization of Latent Codes')\n",
        "    plt.xlabel('t-SNE Component 1')\n",
        "    plt.ylabel('t-SNE Component 2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"t-SNE visualization saved to {save_path}\")\n",
        "\n",
        "    return codes_2d\n",
        "\n",
        "def analyze_codebook_clustering(model, dataloader, device, save_path, n_clusters=10, num_samples=1000):\n",
        "    \"\"\"Enhanced clustering analysis\"\"\"\n",
        "    model.eval()\n",
        "    all_codes = []\n",
        "    all_images = []\n",
        "\n",
        "    print(\"Extracting codes and images for clustering...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = batch.to(device)\n",
        "            codes = model.encode(batch)\n",
        "            all_codes.append(codes.cpu())\n",
        "            all_images.append(batch.cpu())\n",
        "            if len(all_codes) * config.batch_size >= num_samples:\n",
        "                break\n",
        "\n",
        "    all_codes = torch.cat(all_codes, dim=0).numpy()[:num_samples]\n",
        "    all_images = torch.cat(all_images, dim=0)[:num_samples]\n",
        "\n",
        "    # Apply t-SNE for visualization\n",
        "    if all_codes.shape[1] > 2:\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        codes_2d = tsne.fit_transform(all_codes)\n",
        "    else:\n",
        "        codes_2d = all_codes\n",
        "\n",
        "    # K-means clustering\n",
        "    print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(all_codes)\n",
        "\n",
        "    # Plot clustering results\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    scatter = plt.scatter(codes_2d[:, 0], codes_2d[:, 1], c=cluster_labels,\n",
        "                         alpha=0.6, cmap='tab10', s=20)\n",
        "    plt.title('K-means Clustering of Latent Codes')\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Show cluster sizes\n",
        "    plt.subplot(1, 2, 2)\n",
        "    cluster_sizes = np.bincount(cluster_labels, minlength=n_clusters)\n",
        "    plt.bar(range(n_clusters), cluster_sizes)\n",
        "    plt.title('Cluster Sizes Distribution')\n",
        "    plt.xlabel('Cluster ID')\n",
        "    plt.ylabel('Number of Samples')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Clustering analysis saved to {save_path}\")\n",
        "\n",
        "    return cluster_labels, kmeans, codes_2d\n",
        "\n",
        "def enhanced_codebook_analysis(model, dataloader, device, save_dir):\n",
        "    \"\"\"Comprehensive codebook analysis combining both approaches\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Basic codebook usage (from Code 1)\n",
        "    codebook_stats = visualize_codebook_usage(\n",
        "        model, dataloader, device,\n",
        "        os.path.join(save_dir, 'codebook_usage.png')\n",
        "    )\n",
        "\n",
        "    # 2. t-SNE analysis (from Code 2)\n",
        "    codes_2d = analyze_codebook_tsne(\n",
        "        model, dataloader, device,\n",
        "        os.path.join(save_dir, 'tsne_codes.png')\n",
        "    )\n",
        "\n",
        "    # 3. Clustering analysis (from Code 2)\n",
        "    cluster_labels, kmeans, codes_2d_cluster = analyze_codebook_clustering(\n",
        "        model, dataloader, device,\n",
        "        os.path.join(save_dir, 'clustering_analysis.png')\n",
        "    )\n",
        "\n",
        "    # 4. Combined analysis plot\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Code usage histogram\n",
        "    plt.subplot(1, 3, 1)\n",
        "    code_counts = codebook_stats['code_counts']\n",
        "    plt.hist(code_counts[code_counts > 0], bins=50, edgecolor='black', alpha=0.7)\n",
        "    plt.title(f'Code Usage Distribution\\n(Active: {codebook_stats[\"active_codes\"]}/{codebook_stats[\"total_codes\"]})')\n",
        "    plt.xlabel('Usage Count')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # t-SNE visualization\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.scatter(codes_2d[:, 0], codes_2d[:, 1], alpha=0.6, s=10)\n",
        "    plt.title('Latent Codes t-SNE')\n",
        "    plt.xlabel('t-SNE 1')\n",
        "    plt.ylabel('t-SNE 2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Clustering visualization\n",
        "    plt.subplot(1, 3, 3)\n",
        "    scatter = plt.scatter(codes_2d_cluster[:, 0], codes_2d_cluster[:, 1],\n",
        "                         c=cluster_labels, alpha=0.6, cmap='tab10', s=10)\n",
        "    plt.title('K-means Clustering')\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'comprehensive_codebook_analysis.png'),\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Comprehensive codebook analysis saved to {save_dir}\")\n",
        "\n",
        "    return {\n",
        "        'codebook_stats': codebook_stats,\n",
        "        'clustering': {\n",
        "            'labels': cluster_labels,\n",
        "            'kmeans': kmeans,\n",
        "            'codes_2d': codes_2d_cluster\n",
        "        },\n",
        "        'tsne': codes_2d\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# Visualization Functions (Keep your existing ones but ensure they're correct)\n",
        "# ============================================================================\n",
        "def plot_training_history(history, save_path):\n",
        "    \"\"\"Plot training history\"\"\"\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "\n",
        "    # Plot losses\n",
        "    axes[0, 0].plot(history['recon_loss'], label='Reconstruction Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Reconstruction Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    axes[0, 1].plot(history['vq_loss'], label='VQ Loss', color='orange')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].set_title('VQ Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    axes[0, 2].plot(history['perplexity'], label='Perplexity', color='green')\n",
        "    axes[0, 2].set_xlabel('Epoch')\n",
        "    axes[0, 2].set_ylabel('Perplexity')\n",
        "    axes[0, 2].set_title('Codebook Perplexity')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True)\n",
        "\n",
        "    # Plot utilization rate if available\n",
        "    if 'utilization_rate' in history:\n",
        "        axes[0, 3].plot([x * 100 for x in history['utilization_rate']], label='Utilization Rate', color='purple')\n",
        "        axes[0, 3].set_xlabel('Epoch')\n",
        "        axes[0, 3].set_ylabel('Utilization (%)')\n",
        "        axes[0, 3].set_title('Codebook Utilization Rate')\n",
        "        axes[0, 3].legend()\n",
        "        axes[0, 3].grid(True)\n",
        "\n",
        "    # Plot metrics\n",
        "    axes[1, 0].plot(history['mse'], label='MSE', color='red')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('MSE')\n",
        "    axes[1, 0].set_title('Mean Squared Error')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "\n",
        "    axes[1, 1].plot(history['psnr'], label='PSNR', color='purple')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('PSNR (dB)')\n",
        "    axes[1, 1].set_title('Peak Signal-to-Noise Ratio')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "\n",
        "    axes[1, 2].plot(history['ssim'], label='SSIM', color='brown')\n",
        "    axes[1, 2].set_xlabel('Epoch')\n",
        "    axes[1, 2].set_ylabel('SSIM')\n",
        "    axes[1, 2].set_title('Structural Similarity Index')\n",
        "    axes[1, 2].legend()\n",
        "    axes[1, 2].grid(True)\n",
        "\n",
        "    # Hide unused subplot\n",
        "    axes[1, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Training history plot saved to {save_path}\")\n",
        "\n",
        "def plot_reconstructions(model, dataloader, device, save_path, num_images=8):\n",
        "    \"\"\"Plot original vs reconstructed images\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get a batch\n",
        "    images = next(iter(dataloader))[:num_images].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        reconstructions, _, _, _ = model(images)\n",
        "\n",
        "    # Denormalize\n",
        "    images = (images + 1) / 2\n",
        "    reconstructions = (reconstructions + 1) / 2\n",
        "\n",
        "    # Create grid\n",
        "    comparison = torch.cat([images, reconstructions])\n",
        "    grid = make_grid(comparison, nrow=num_images, padding=2)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(16, 4))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Top: Original, Bottom: Reconstructed')\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Reconstruction comparison saved to {save_path}\")\n",
        "\n",
        "def plot_generated_samples(samples, save_path, title=\"Generated Samples\"):\n",
        "    \"\"\"Plot generated emoji samples\"\"\"\n",
        "    # Denormalize\n",
        "    samples = (samples + 1) / 2\n",
        "\n",
        "    # Create grid\n",
        "    grid = make_grid(samples, nrow=8, padding=2)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Generated samples saved to {save_path}\")\n",
        "\n",
        "def visualize_codebook_usage(model, dataloader, device, save_path):\n",
        "    \"\"\"Analyze and visualize codebook usage\"\"\"\n",
        "    model.eval()\n",
        "    all_codes = []\n",
        "\n",
        "    print(\"Analyzing codebook usage...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = batch.to(device)\n",
        "            codes = model.encode(batch)\n",
        "            all_codes.append(codes.cpu())\n",
        "\n",
        "    all_codes = torch.cat(all_codes, dim=0).numpy().flatten()\n",
        "\n",
        "    # Plot histogram\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Histogram\n",
        "    axes[0].hist(all_codes, bins=config.num_embeddings, edgecolor='black')\n",
        "    axes[0].set_xlabel('Code Index')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].set_title('Codebook Usage Distribution')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Usage statistics\n",
        "    unique_codes = len(np.unique(all_codes))\n",
        "    usage_rate = unique_codes / config.num_embeddings * 100\n",
        "\n",
        "    code_counts = np.bincount(all_codes, minlength=config.num_embeddings)\n",
        "    axes[1].bar(range(len(code_counts)), sorted(code_counts, reverse=True))\n",
        "    axes[1].set_xlabel('Code Rank')\n",
        "    axes[1].set_ylabel('Usage Count')\n",
        "    axes[1].set_title(f'Sorted Code Usage (Active: {unique_codes}/{config.num_embeddings} = {usage_rate:.1f}%)')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Codebook usage plot saved to {save_path}\")\n",
        "\n",
        "    return {\n",
        "        'total_codes': config.num_embeddings,\n",
        "        'active_codes': unique_codes,\n",
        "        'usage_rate': usage_rate,\n",
        "        'code_counts': code_counts\n",
        "    }\n",
        "\n",
        "def visualize_latent_space(model, dataloader, device, save_path):\n",
        "    \"\"\"Visualize latent space using t-SNE\"\"\"\n",
        "    model.eval()\n",
        "    all_latents = []\n",
        "    all_images = []\n",
        "\n",
        "    print(\"Extracting latent representations...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = batch.to(device)\n",
        "            z = model.encoder(batch)\n",
        "            z = model.pre_vq_conv(z)\n",
        "            # Pool spatial dimensions\n",
        "            z_pooled = z.mean(dim=[2, 3])\n",
        "            all_latents.append(z_pooled.cpu())\n",
        "            all_images.append(batch.cpu())\n",
        "\n",
        "    latents = torch.cat(all_latents, dim=0).numpy()\n",
        "    images = torch.cat(all_images, dim=0)\n",
        "\n",
        "    # Apply t-SNE\n",
        "    print(\"Applying t-SNE...\")\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "    latents_2d = tsne.fit_transform(latents[:1000])  # Use subset for speed\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "    scatter = ax.scatter(latents_2d[:, 0], latents_2d[:, 1],\n",
        "                        c=range(len(latents_2d)), cmap='viridis',\n",
        "                        alpha=0.6, s=20)\n",
        "    ax.set_title('t-SNE Visualization of Latent Space')\n",
        "    ax.set_xlabel('t-SNE Component 1')\n",
        "    ax.set_ylabel('t-SNE Component 2')\n",
        "    plt.colorbar(scatter, ax=ax, label='Sample Index')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Latent space visualization saved to {save_path}\")\n",
        "\n",
        "    return latents_2d\n",
        "\n",
        "def visualize_latent_interpolation(model, dataloader, device, save_path, num_steps=10):\n",
        "    \"\"\"Interpolate between two emojis in latent space\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get two random images\n",
        "    images = next(iter(dataloader))[:2].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode to latent\n",
        "        z1 = model.encoder(images[0:1])\n",
        "        z1 = model.pre_vq_conv(z1)\n",
        "\n",
        "        z2 = model.encoder(images[1:2])\n",
        "        z2 = model.pre_vq_conv(z2)\n",
        "\n",
        "        # Interpolate\n",
        "        interpolations = []\n",
        "        for alpha in np.linspace(0, 1, num_steps):\n",
        "            z_interp = (1 - alpha) * z1 + alpha * z2\n",
        "\n",
        "            # Quantize\n",
        "            quantized, _, _, _ = model.vq(z_interp)\n",
        "\n",
        "            # Decode\n",
        "            recon = model.decoder(quantized)\n",
        "            interpolations.append(recon)\n",
        "\n",
        "        interpolations = torch.cat(interpolations, dim=0)\n",
        "\n",
        "    # Denormalize\n",
        "    interpolations = (interpolations + 1) / 2\n",
        "    images = (images + 1) / 2\n",
        "\n",
        "    # Create grid\n",
        "    grid = make_grid(interpolations, nrow=num_steps, padding=2)\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(15, 6))\n",
        "\n",
        "    # Show start and end\n",
        "    axes[0].imshow(make_grid(images, nrow=2, padding=2).permute(1, 2, 0).cpu().numpy())\n",
        "    axes[0].set_title('Start and End Emojis')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Show interpolation\n",
        "    axes[1].imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "    axes[1].set_title(f'Latent Space Interpolation ({num_steps} steps)')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Latent interpolation saved to {save_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Prior Trainer (Keep your existing PriorTrainer)\n",
        "# ============================================================================\n",
        "class PriorTrainer:\n",
        "    \"\"\"Trainer for PixelCNN prior with stability improvements\"\"\"\n",
        "\n",
        "    def __init__(self, prior, vqvae, config):\n",
        "        self.prior = prior\n",
        "        self.vqvae = vqvae\n",
        "        self.config = config\n",
        "        self.optimizer = torch.optim.Adam(prior.parameters(), lr=config.learning_rate_prior)\n",
        "        self.history = {'loss': []}\n",
        "        self.start_epoch = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch, filepath):\n",
        "        \"\"\"Save training checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.prior.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'history': self.history\n",
        "        }\n",
        "        torch.save(checkpoint, filepath)\n",
        "        print(f\"Checkpoint saved: {filepath}\")\n",
        "\n",
        "    def load_checkpoint(self, filepath):\n",
        "        \"\"\"Load training checkpoint\"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            try:\n",
        "                checkpoint = torch.load(filepath, map_location=self.config.device, weights_only=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Safe loading failed: {e}\")\n",
        "                print(\"Attempting unsafe load...\")\n",
        "                checkpoint = torch.load(filepath, map_location=self.config.device, weights_only=False)\n",
        "\n",
        "            self.prior.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.history = checkpoint['history']\n",
        "            self.start_epoch = checkpoint['epoch'] + 1\n",
        "            print(f\"Checkpoint loaded: {filepath}, resuming from epoch {self.start_epoch}\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def train_epoch(self, dataloader):\n",
        "        \"\"\"Train for one epoch with gradient clipping\"\"\"\n",
        "        self.prior.train()\n",
        "        self.vqvae.eval()\n",
        "        epoch_loss = 0\n",
        "        num_valid_batches = 0\n",
        "\n",
        "        pbar = tqdm(dataloader, desc=\"Training Prior\")\n",
        "        for batch in pbar:\n",
        "            batch = batch.to(self.config.device)\n",
        "\n",
        "            # Get discrete codes\n",
        "            with torch.no_grad():\n",
        "                codes = self.vqvae.encode(batch)\n",
        "                spatial_size = int(np.sqrt(codes.shape[1]))\n",
        "                codes = codes.view(-1, spatial_size, spatial_size)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = self.prior(codes)\n",
        "            loss = F.cross_entropy(logits, codes)\n",
        "\n",
        "            # Check for NaN\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"Warning: NaN loss detected, skipping batch\")\n",
        "                continue\n",
        "\n",
        "            # Backward pass with gradient clipping\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.prior.parameters(), self.config.grad_clip)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            num_valid_batches += 1\n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        if num_valid_batches == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        return epoch_loss / num_valid_batches\n",
        "\n",
        "    def train(self, train_loader):\n",
        "        \"\"\"Full training loop\"\"\"\n",
        "        print(f\"Starting prior training from epoch {self.start_epoch}\")\n",
        "\n",
        "        for epoch in range(self.start_epoch, self.config.num_epochs_prior):\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs_prior}\")\n",
        "\n",
        "            loss = self.train_epoch(train_loader)\n",
        "\n",
        "            if loss == float('inf'):\n",
        "                print(\"Training failed due to NaN losses. Stopping.\")\n",
        "                break\n",
        "\n",
        "            self.history['loss'].append(loss)\n",
        "\n",
        "            print(f\"Loss: {loss:.4f}\")\n",
        "\n",
        "            # Save checkpoint every 5 epochs\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                checkpoint_path = os.path.join(\n",
        "                    self.config.checkpoint_dir,\n",
        "                    f'prior_epoch{epoch+1}.pt'\n",
        "                )\n",
        "                self.save_checkpoint(epoch, checkpoint_path)\n",
        "\n",
        "        # Save final model\n",
        "        final_path = os.path.join(self.config.checkpoint_dir, 'prior_final.pt')\n",
        "        self.save_checkpoint(self.config.num_epochs_prior - 1, final_path)\n",
        "\n",
        "        return self.history\n",
        "\n",
        "# ============================================================================\n",
        "# Main Training and Evaluation Pipeline - ENHANCED\n",
        "# ============================================================================\n",
        "def main():\n",
        "    \"\"\"Enhanced main function with two-phase training\"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"VQ-VAE Emoji Generation Project - ENHANCED VERSION\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 1: Download and prepare dataset\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 1: Dataset Preparation\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if not os.listdir(config.data_dir):\n",
        "        download_emoji_dataset()\n",
        "    else:\n",
        "        print(f\"Dataset already exists in {config.data_dir}\")\n",
        "\n",
        "    # Create dataset and dataloaders\n",
        "    dataset = EmojiDataset(config.data_dir, config.image_size)\n",
        "    print(f\"Total dataset size: {len(dataset)} images\")\n",
        "\n",
        "    # Split into train and validation\n",
        "    train_size = int(0.9 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=config.batch_size,\n",
        "        shuffle=True, num_workers=config.num_workers, pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=config.batch_size,\n",
        "        shuffle=False, num_workers=config.num_workers, pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {len(train_dataset)} images\")\n",
        "    print(f\"Validation set: {len(val_dataset)} images\")\n",
        "\n",
        "    # Step 2: Architecture Verification\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 2: Architecture Verification\")\n",
        "    print(\"=\" * 80)\n",
        "    model_test, expected_spatial_size = verify_architecture()\n",
        "    print(f\"\\nArchitecture verified! Latent spatial size: {expected_spatial_size}x{expected_spatial_size}\")\n",
        "    del model_test  # Free memory\n",
        "\n",
        "    # Step 3: Two-Phase VQ-VAE Training\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 3: Two-Phase VQ-VAE Training\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    vqvae = VQVAE(config, use_ema=True).to(config.device)\n",
        "    print(f\"VQ-VAE parameters: {sum(p.numel() for p in vqvae.parameters()):,}\")\n",
        "\n",
        "    trainer = Trainer(vqvae, config)\n",
        "\n",
        "    # Try to load checkpoint\n",
        "    checkpoint_path = os.path.join(config.checkpoint_dir, 'vqvae_final.pt')\n",
        "    if trainer.load_checkpoint(checkpoint_path):\n",
        "        print(\"Found existing checkpoint, checking codebook utilization...\")\n",
        "        utilization_rate, unique_codes = trainer.get_codebook_utilization(train_loader)\n",
        "        print(f\"Current codebook utilization: {unique_codes}/{config.num_embeddings} ({utilization_rate*100:.2f}%)\")\n",
        "\n",
        "        if utilization_rate >= config.codebook_utilization_threshold:\n",
        "            print(\"Codebook utilization already meets threshold! Proceeding to Phase 2 analysis.\")\n",
        "            phase1_complete_epoch = config.num_epochs_vqvae\n",
        "        else:\n",
        "            print(\"Codebook utilization below threshold. Starting Phase 1 training...\")\n",
        "            phase1_complete_epoch, phase1_metrics = trainer.train_phase1_codebook(\n",
        "                train_loader, val_loader, config.codebook_utilization_threshold\n",
        "            )\n",
        "    else:\n",
        "        print(\"No checkpoint found, starting Phase 1 training from scratch...\")\n",
        "        phase1_complete_epoch, phase1_metrics = trainer.train_phase1_codebook(\n",
        "            train_loader, val_loader, config.codebook_utilization_threshold\n",
        "        )\n",
        "\n",
        "    # Continue with Phase 2 if we haven't reached the end\n",
        "    if phase1_complete_epoch < config.num_epochs_vqvae:\n",
        "        history = trainer.train_phase2_final(train_loader, val_loader, phase1_complete_epoch)\n",
        "    else:\n",
        "        history = trainer.history\n",
        "        print(\"VQ-VAE training complete!\")\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(\n",
        "        history,\n",
        "        os.path.join(config.results_dir, 'training_history.png')\n",
        "    )\n",
        "\n",
        "    # Step 4: Enhanced VQ-VAE Analysis\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 4: Enhanced VQ-VAE Analysis\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Plot reconstructions\n",
        "    plot_reconstructions(\n",
        "        vqvae, val_loader, config.device,\n",
        "        os.path.join(config.results_dir, 'reconstructions.png')\n",
        "    )\n",
        "\n",
        "    # Enhanced codebook analysis (combining both approaches)\n",
        "    codebook_analysis_results = enhanced_codebook_analysis(\n",
        "        vqvae, train_loader, config.device,\n",
        "        os.path.join(config.results_dir, 'codebook_analysis')\n",
        "    )\n",
        "\n",
        "    # Visualize latent space\n",
        "    visualize_latent_space(\n",
        "        vqvae, train_loader, config.device,\n",
        "        os.path.join(config.results_dir, 'latent_space_tsne.png')\n",
        "    )\n",
        "\n",
        "    # Latent interpolation\n",
        "    visualize_latent_interpolation(\n",
        "        vqvae, val_loader, config.device,\n",
        "        os.path.join(config.results_dir, 'latent_interpolation.png'),\n",
        "        num_steps=config.num_interpolation_steps\n",
        "    )\n",
        "\n",
        "    # Step 5: Train Prior\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 5: Prior Training (PixelCNN)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Calculate spatial size\n",
        "    sample_batch = next(iter(train_loader))[:1].to(config.device)\n",
        "    with torch.no_grad():\n",
        "        codes = vqvae.encode(sample_batch)\n",
        "        spatial_size = int(np.sqrt(codes.shape[1]))\n",
        "    print(f\"Latent spatial size: {spatial_size}x{spatial_size}\")\n",
        "\n",
        "    prior = PixelCNN(\n",
        "        config.num_embeddings,\n",
        "        config.pixelcnn_layers,\n",
        "        config.pixelcnn_hidden\n",
        "    ).to(config.device)\n",
        "    print(f\"PixelCNN parameters: {sum(p.numel() for p in prior.parameters()):,}\")\n",
        "\n",
        "    prior_trainer = PriorTrainer(prior, vqvae, config)\n",
        "\n",
        "    # Try to load checkpoint\n",
        "    prior_checkpoint_path = os.path.join(config.checkpoint_dir, 'prior_final.pt')\n",
        "    if not prior_trainer.load_checkpoint(prior_checkpoint_path):\n",
        "        print(\"No prior checkpoint found, training from scratch...\")\n",
        "\n",
        "    # Train\n",
        "    if prior_trainer.start_epoch < config.num_epochs_prior:\n",
        "        prior_history = prior_trainer.train(train_loader)\n",
        "\n",
        "        # Plot prior training history\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(prior_history['loss'], label='Cross-Entropy Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Prior Training History')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(\n",
        "            os.path.join(config.results_dir, 'prior_training_history.png'),\n",
        "            dpi=300, bbox_inches='tight'\n",
        "        )\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(\"Prior training already complete!\")\n",
        "\n",
        "    # Step 6: Generate Novel Emojis\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 6: Novel Emoji Generation\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"Generating {config.num_samples} novel emojis...\")\n",
        "    prior.eval()\n",
        "    vqvae.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Sample from prior\n",
        "        sampled_codes = prior.sample(\n",
        "            config.num_samples, spatial_size, config.device, temperature=1.0\n",
        "        )\n",
        "\n",
        "        # Decode to images\n",
        "        generated_images = vqvae.decode_codes(sampled_codes, spatial_size)\n",
        "\n",
        "    # Plot generated samples\n",
        "    plot_generated_samples(\n",
        "        generated_images,\n",
        "        os.path.join(config.results_dir, 'generated_emojis.png'),\n",
        "        title=\"Generated Novel Emojis\"\n",
        "    )\n",
        "\n",
        "    # Step 7: Calculate FID\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 7: Calculating FID Score\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Extracting features from real images...\")\n",
        "    real_images = []\n",
        "    for batch in tqdm(val_loader):\n",
        "        real_images.append(batch)\n",
        "        if len(real_images) * config.batch_size >= config.num_samples:\n",
        "            break\n",
        "    real_images = torch.cat(real_images, dim=0)[:config.num_samples].to(config.device)\n",
        "\n",
        "    real_features, inception_model = get_inception_features(real_images, None, config.device)\n",
        "    print(\"Extracting features from generated images...\")\n",
        "    fake_features, _ = get_inception_features(generated_images, inception_model, config.device)\n",
        "\n",
        "    fid_score = calculate_fid(real_features, fake_features)\n",
        "    print(f\"\\nFID Score: {fid_score:.2f}\")\n",
        "\n",
        "    # Step 8: Generate Comprehensive Report\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 8: Generating Final Report\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Get final utilization rate\n",
        "    final_utilization_rate, final_unique_codes = trainer.get_codebook_utilization(train_loader)\n",
        "\n",
        "    report = {\n",
        "        'dataset_size': len(dataset),\n",
        "        'train_size': len(train_dataset),\n",
        "        'val_size': len(val_dataset),\n",
        "        'vqvae_params': sum(p.numel() for p in vqvae.parameters()),\n",
        "        'prior_params': sum(p.numel() for p in prior.parameters()),\n",
        "        'final_metrics': {\n",
        "            'mse': history['mse'][-1] if history['mse'] else None,\n",
        "            'psnr': history['psnr'][-1] if history['psnr'] else None,\n",
        "            'ssim': history['ssim'][-1] if history['ssim'] else None,\n",
        "            'fid': fid_score,\n",
        "            'perplexity': history['perplexity'][-1] if history['perplexity'] else None\n",
        "        },\n",
        "        'codebook_stats': {\n",
        "            'total_codes': config.num_embeddings,\n",
        "            'active_codes': int(final_unique_codes),\n",
        "            'usage_rate': float(final_utilization_rate * 100),\n",
        "            'target_utilization': config.codebook_utilization_threshold * 100,\n",
        "            'utilization_achieved': final_utilization_rate >= config.codebook_utilization_threshold\n",
        "        },\n",
        "        'training_info': {\n",
        "            'phase1_completed_at_epoch': phase1_complete_epoch,\n",
        "            'total_epochs_trained': len(history['recon_loss']) if history['recon_loss'] else 0,\n",
        "            'final_recon_loss': history['recon_loss'][-1] if history['recon_loss'] else None,\n",
        "            'final_vq_loss': history['vq_loss'][-1] if history['vq_loss'] else None\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save report\n",
        "    report_path = os.path.join(config.results_dir, 'final_report.json')\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nReconstruction Quality:\")\n",
        "    print(f\"  MSE: {report['final_metrics']['mse']:.4f}\")\n",
        "    print(f\"  PSNR: {report['final_metrics']['psnr']:.2f} dB\")\n",
        "    print(f\"  SSIM: {report['final_metrics']['ssim']:.4f}\")\n",
        "    print(f\"\\nGeneration Quality:\")\n",
        "    print(f\"  FID: {report['final_metrics']['fid']:.2f}\")\n",
        "    print(f\"\\nCodebook Analysis:\")\n",
        "    print(f\"  Perplexity: {report['final_metrics']['perplexity']:.2f}\")\n",
        "    print(f\"  Active codes: {report['codebook_stats']['active_codes']}/{report['codebook_stats']['total_codes']}\")\n",
        "    print(f\"  Usage rate: {report['codebook_stats']['usage_rate']:.2f}%\")\n",
        "    print(f\"  Target utilization: {report['codebook_stats']['target_utilization']}%\")\n",
        "    print(f\"  Utilization achieved: {'âœ… YES' if report['codebook_stats']['utilization_achieved'] else 'âŒ NO'}\")\n",
        "    print(f\"\\nTraining Info:\")\n",
        "    print(f\"  Phase 1 completed at epoch: {report['training_info']['phase1_completed_at_epoch']}\")\n",
        "    print(f\"  Total epochs trained: {report['training_info']['total_epochs_trained']}\")\n",
        "    print(f\"\\nAll results saved to: {config.results_dir}\")\n",
        "    print(f\"Report saved to: {report_path}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PROJECT COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# Run the complete pipeline\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTn2sbiXMpJu",
        "outputId": "3a43a451-21fd-47ef-c94b-0ee928d79ee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mresults\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMMLRpPJNKin"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Display all PNG images\n",
        "results_dir = '/content/results/'\n",
        "image_files = [f for f in os.listdir(results_dir) if f.endswith('.png')]\n",
        "\n",
        "print(\"Displaying saved plots:\")\n",
        "for img_file in image_files:\n",
        "    img_path = os.path.join(results_dir, img_file)\n",
        "    img = mpimg.imread(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.title(img_file)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Print the contents of experiment_results.csv\n",
        "csv_path = os.path.join(results_dir, 'experiment_results.csv')\n",
        "\n",
        "if os.path.exists(csv_path):\n",
        "    print(\"\\nContents of experiment_results.csv:\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    for index, row in df.iterrows():\n",
        "        print(','.join(map(str, row.tolist())))\n",
        "else:\n",
        "    print(f\"\\n{csv_path} not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d07ca2e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "# Define source and destination paths\n",
        "source_checkpoints = '/content/checkpoints'\n",
        "source_results = '/content/results'\n",
        "destination_base = '/content/drive/MyDrive'\n",
        "destination_folder_name = config.experiment_name\n",
        "destination_path = os.path.join(destination_base, destination_folder_name)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "print(f\"Creating destination folder: {destination_path}\")\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "print(f\"Destination folder created or already exists.\")\n",
        "\n",
        "# Transfer checkpoints folder\n",
        "destination_checkpoints = os.path.join(destination_path, os.path.basename(source_checkpoints))\n",
        "print(f\"\\nTransferring checkpoints from {source_checkpoints} to {destination_checkpoints}...\")\n",
        "if os.path.exists(source_checkpoints):\n",
        "    if os.path.exists(destination_checkpoints):\n",
        "        print(f\"Removing existing destination checkpoints folder: {destination_checkpoints}\")\n",
        "        shutil.rmtree(destination_checkpoints)\n",
        "        print(f\"Removed existing destination checkpoints folder: {destination_checkpoints}\")\n",
        "    try:\n",
        "        shutil.copytree(source_checkpoints, destination_checkpoints)\n",
        "        print(\"Checkpoints transfer complete.\")\n",
        "        # Verify contents\n",
        "        source_files = os.listdir(source_checkpoints)\n",
        "        destination_files = os.listdir(destination_checkpoints)\n",
        "        print(f\"Source files in checkpoints: {source_files}\")\n",
        "        print(f\"Destination files in checkpoints: {destination_files}\")\n",
        "        if set(source_files) == set(destination_files) and len(source_files) > 0:\n",
        "            print(\"Checkpoints contents successfully verified.\")\n",
        "        elif len(source_files) == 0:\n",
        "             print(\"Source checkpoints folder is empty.\")\n",
        "        else:\n",
        "             print(\"Warning: Checkpoints contents may not have been fully copied.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during checkpoints transfer: {e}\")\n",
        "else:\n",
        "    print(f\"Source checkpoints folder not found: {source_checkpoints}\")\n",
        "\n",
        "\n",
        "# Transfer results folder\n",
        "destination_results = os.path.join(destination_path, os.path.basename(source_results))\n",
        "print(f\"\\nTransferring results from {source_results} to {destination_results}...\")\n",
        "if os.path.exists(source_results):\n",
        "    if os.path.exists(destination_results):\n",
        "        print(f\"Removing existing destination results folder: {destination_results}\")\n",
        "        shutil.rmtree(destination_results)\n",
        "        print(f\"Removed existing destination results folder: {destination_results}\")\n",
        "    try:\n",
        "        shutil.copytree(source_results, destination_results)\n",
        "        print(\"Results transfer complete.\")\n",
        "        # Verify contents\n",
        "        source_files = os.listdir(source_results)\n",
        "        destination_files = os.listdir(destination_results)\n",
        "        print(f\"Source files in results: {source_files}\")\n",
        "        print(f\"Destination files in results: {destination_files}\")\n",
        "        if set(source_files) == set(destination_files) and len(source_files) > 0:\n",
        "            print(\"Results contents successfully verified.\")\n",
        "        elif len(source_files) == 0:\n",
        "             print(\"Source results folder is empty.\")\n",
        "        else:\n",
        "             print(\"Warning: Results contents may not have been fully copied.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during results transfer: {e}\")\n",
        "else:\n",
        "    print(f\"Source results folder not found: {source_results}\")\n",
        "\n",
        "print(\"\\nTransfer process finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eni0xc9lROGF"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05fbc5784b7c4ac2aceb0117e73bec1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "132f300edfcf448cbf3f0d4457f0eb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2027ce8946bb4d7c995dd2b9e9c05b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c27457a3aba64afa9a09541740f31dbd",
            "max": 2749,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_132f300edfcf448cbf3f0d4457f0eb15",
            "value": 2749
          }
        },
        "208833f5c22b4298a7070363cfb3eeb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c2ecd1799d9460f898f4ba34f905d70",
              "IPY_MODEL_2027ce8946bb4d7c995dd2b9e9c05b46",
              "IPY_MODEL_a0d8542fdecd4eb79ab74ce6bdecff6f"
            ],
            "layout": "IPY_MODEL_5a3d220ca4984ba087317a19d2a0ffb1"
          }
        },
        "2a68a9c162504d079433b14e3a2c9486": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fb221bdfd1f4c459ce7c6d5c2bada65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353f4c230fcd4491bbed14a7042fcfc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05fbc5784b7c4ac2aceb0117e73bec1d",
            "max": 138595810,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca3f9c49a9ec40678202491e6a9b444d",
            "value": 138595810
          }
        },
        "3a89491b9cc7431899758b2ce285aab4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a6f3f5994e947d78e24d047ac97b095": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5674198464e846e4b9135c5d31b36d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "576b7be713b248d8af68dbb257f6ac07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7760bc39170e4a578724d1a9766406b1",
              "IPY_MODEL_7147be28ef8a4feca9a1f36162506804",
              "IPY_MODEL_86a1f0e9f6f64425a69a60ae949822c7"
            ],
            "layout": "IPY_MODEL_b3ad216bc9174627b39b224d08f201f8"
          }
        },
        "5a3d220ca4984ba087317a19d2a0ffb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eac55ad9f4543feaeece364932efe2c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c2ecd1799d9460f898f4ba34f905d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb50bed2b3f248cead2ef1282c647b0f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d43d85bea2f34e4da080003146f092e6",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "6e5df2d2a61e4a71a49cf97a45d2b7b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7147be28ef8a4feca9a1f36162506804": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a89491b9cc7431899758b2ce285aab4",
            "max": 720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c5ccbfb353342eea79f25585522f439",
            "value": 720
          }
        },
        "7760bc39170e4a578724d1a9766406b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a68a9c162504d079433b14e3a2c9486",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d7cde14fe3bd42229a49826000ccc342",
            "value": "dataset_infos.json:â€‡100%"
          }
        },
        "7db862cdfbc44ec982c4c203ffa3c9f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86a1f0e9f6f64425a69a60ae949822c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a11ed5b605e54b8fa80718c491ce98af",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dba10b82b9424edfbd650294726440bb",
            "value": "â€‡720/720â€‡[00:00&lt;00:00,â€‡36.1kB/s]"
          }
        },
        "8ef33255cd59430296314c33959134bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c5ccbfb353342eea79f25585522f439": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0d8542fdecd4eb79ab74ce6bdecff6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7db862cdfbc44ec982c4c203ffa3c9f0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5674198464e846e4b9135c5d31b36d9e",
            "value": "â€‡2749/2749â€‡[00:00&lt;00:00,â€‡3644.31â€‡examples/s]"
          }
        },
        "a11ed5b605e54b8fa80718c491ce98af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3d263cd9b8646aea1c258cfa57f8ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7c6723b741a4202a49d3e54c68a2373",
              "IPY_MODEL_353f4c230fcd4491bbed14a7042fcfc2",
              "IPY_MODEL_dc5df4a030d542f8b91ed37006357395"
            ],
            "layout": "IPY_MODEL_2fb221bdfd1f4c459ce7c6d5c2bada65"
          }
        },
        "b3ad216bc9174627b39b224d08f201f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c6723b741a4202a49d3e54c68a2373": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a6f3f5994e947d78e24d047ac97b095",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8ef33255cd59430296314c33959134bc",
            "value": "data/train-00000-of-00001-38cc4fa96c139e(â€¦):â€‡100%"
          }
        },
        "bb50bed2b3f248cead2ef1282c647b0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27457a3aba64afa9a09541740f31dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca3f9c49a9ec40678202491e6a9b444d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d43d85bea2f34e4da080003146f092e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7cde14fe3bd42229a49826000ccc342": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dba10b82b9424edfbd650294726440bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc5df4a030d542f8b91ed37006357395": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5eac55ad9f4543feaeece364932efe2c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6e5df2d2a61e4a71a49cf97a45d2b7b0",
            "value": "â€‡139M/139Mâ€‡[00:03&lt;00:00,â€‡74.6MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNpSu0EWJFMo",
        "outputId": "75bd901c-1552-483b-d406-9c4063a7ae6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing old incompatible checkpoints...\n",
            "Removing old resutls...\n"
          ]
        }
      ],
      "source": [
        "# Delete old incompatible checkpoints and start fresh\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "checkpoint_dir = '/content/checkpoints'\n",
        "results_dir = '/content/results'\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"Removing old incompatible checkpoints...\")\n",
        "    shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "if os.path.exists(results_dir):\n",
        "    print(\"Removing old resutls...\")\n",
        "    shutil.rmtree(results_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5l146FW4VC_A"
      },
      "outputs": [],
      "source": [
        "! mkdir checkpoints results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hpN9iT_UaYB",
        "outputId": "2981e79c-a571-4ca7-f3e6-ffa3aa6f0964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Codebook_Training_Phase_1.ipynb  PriorTraining_PixelCNN_Phase_2.ipynb\n",
            "phase_1_manual_hp_tuning\t results_interpolation_v1\n",
            "phase_2_manual_hp_tuning\t v1.ipynb\n"
          ]
        }
      ],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (2.8.0)\n",
            "Requirement already satisfied: torchvision in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (0.23.0)\n",
            "Requirement already satisfied: numpy in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (2.3.3)\n",
            "Requirement already satisfied: matplotlib in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (3.10.6)\n",
            "Requirement already satisfied: tqdm in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (4.67.1)\n",
            "Requirement already satisfied: Pillow in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (11.3.0)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting seaborn\n",
            "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting scikit-image\n",
            "  Downloading scikit_image-0.25.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (78.1.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Collecting scipy>=1.8.0 (from scikit-learn)\n",
            "  Downloading scipy-1.16.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pandas>=1.2 (from seaborn)\n",
            "  Downloading pandas-2.3.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "Collecting imageio!=2.35.0,>=2.33 (from scikit-image)\n",
            "  Using cached imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
            "  Downloading tifffile-2025.10.16-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting lazy-loader>=0.4 (from scikit-image)\n",
            "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/nira/miniconda3/envs/posit_nn/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading scikit_learn-1.7.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "Downloading scikit_image-0.25.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached imageio-2.37.0-py3-none-any.whl (315 kB)\n",
            "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
            "Downloading pandas-2.3.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading scipy-1.16.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading tifffile-2025.10.16-py3-none-any.whl (231 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Installing collected packages: pytz, tzdata, tifffile, threadpoolctl, scipy, lazy-loader, joblib, imageio, scikit-learn, scikit-image, pandas, seaborn\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/12\u001b[0m [seaborn]1/12\u001b[0m [seaborn]mage]\n",
            "\u001b[1A\u001b[2KSuccessfully installed imageio-2.37.0 joblib-1.5.2 lazy-loader-0.4 pandas-2.3.3 pytz-2025.2 scikit-image-0.25.2 scikit-learn-1.7.2 scipy-1.16.2 seaborn-0.13.2 threadpoolctl-3.6.0 tifffile-2025.10.16 tzdata-2025.2\n"
          ]
        }
      ],
      "source": [
        "! pip install torch torchvision numpy matplotlib tqdm Pillow scikit-learn seaborn scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "uQen0Pmky7y-",
        "outputId": "81be5376-76a3-4ac8-9768-bc108ce1ccbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "VQ-VAE EMOJI GENERATION - PHASE 1: CODEBOOK TRAINING\n",
            "================================================================================\n",
            "Downloading emoji dataset...\n",
            "Error downloading dataset: No module named 'datasets'\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 602\u001b[39m\n\u001b[32m    599\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Ready for Phase 2: Prior training and generation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 536\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    534\u001b[39m val_size = \u001b[38;5;28mlen\u001b[39m(dataset) - train_size\n\u001b[32m    535\u001b[39m train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m train_loader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=config.num_workers, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    539\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/posit_nn/lib/python3.13/site-packages/torch/utils/data/dataloader.py:388\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    390\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/posit_nn/lib/python3.13/site-packages/torch/utils/data/sampler.py:156\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "VQ-VAE Emoji Generation - Phase 1: Codebook Training\n",
        "Focus on achieving good codebook utilization (50-60%) before proceeding\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration Management\n",
        "# ============================================================================\n",
        "\n",
        "# Define all configurations in JSON format\n",
        "EXPERIMENT_CONFIGS = {\n",
        "    # Paths\n",
        "    \"data_dir\": \"./emoji_data\",\n",
        "    \"checkpoint_dir\": \"./checkpoints\",\n",
        "    \"results_dir\": \"./results\",\n",
        "\n",
        "    # Data\n",
        "    \"image_size\": 64,\n",
        "    \"batch_size\": 64,\n",
        "    \"num_workers\": 2,\n",
        "\n",
        "    # VQ-VAE Architecture\n",
        "    \"num_hiddens\": 128,\n",
        "    \"num_residual_hiddens\": 32,\n",
        "    \"num_residual_layers\": 2,\n",
        "    \"embedding_dim\": 64,\n",
        "    \"num_embeddings\": 256,\n",
        "    \"commitment_cost\": 0.01,\n",
        "    \"decay\": 0.95,\n",
        "\n",
        "    # Training\n",
        "    \"num_epochs_vqvae\": 100,\n",
        "    \"learning_rate_vqvae\": 3e-4,\n",
        "\n",
        "    # Codebook monitoring\n",
        "    \"min_codebook_usage\": 50.0,\n",
        "    \"check_usage_every\": 5,\n",
        "\n",
        "    # Experiment metadata\n",
        "    \"experiment_name\": \"commitment_cost_10\",\n",
        "    \"notes\": \"Decreasing commitment cost to 10\"\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class that loads from JSON dict\"\"\"\n",
        "    def __init__(self, config_dict):\n",
        "        for key, value in config_dict.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "        # Add device (not in JSON as it's system-dependent)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Convert config back to dictionary for CSV export\"\"\"\n",
        "        return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}\n",
        "\n",
        "config = Config(EXPERIMENT_CONFIGS)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Dataset\n",
        "# ============================================================================\n",
        "\n",
        "class EmojiDataset(Dataset):\n",
        "    def __init__(self, data_dir, image_size=64, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.image_size = image_size\n",
        "        self.image_files = [f for f in os.listdir(data_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((image_size, image_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        return self.transform(image)\n",
        "\n",
        "def download_emoji_dataset():\n",
        "    print(\"Downloading emoji dataset...\")\n",
        "    try:\n",
        "        from datasets import load_dataset\n",
        "        dataset = load_dataset(\"valhalla/emoji-dataset\", split=\"train\")\n",
        "\n",
        "        print(f\"Downloaded {len(dataset)} emojis\")\n",
        "        print(\"Saving images to disk...\")\n",
        "\n",
        "        for idx, item in enumerate(tqdm(dataset)):\n",
        "            img = item['image']\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')\n",
        "            img.save(os.path.join(config.data_dir, f'emoji_{idx:05d}.png'))\n",
        "\n",
        "        print(f\"Saved {len(dataset)} emoji images to {config.data_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading dataset: {e}\")\n",
        "\n",
        "# ============================================================================\n",
        "# VQ-VAE Components\n",
        "# ============================================================================\n",
        "\n",
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay=0.99, epsilon=1e-5):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.commitment_cost = commitment_cost\n",
        "        self.decay = decay\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        embed = torch.randn(num_embeddings, embedding_dim)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(num_embeddings))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        input_shape = inputs.shape\n",
        "        flat_input = inputs.permute(0, 2, 3, 1).contiguous().view(-1, self.embedding_dim)\n",
        "\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self.embed**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self.embed.t()))\n",
        "\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        quantized = torch.matmul(encodings, self.embed)\n",
        "\n",
        "        if self.training:\n",
        "            self.cluster_size.data.mul_(self.decay).add_(encodings.sum(0), alpha=1 - self.decay)\n",
        "            dw = torch.matmul(encodings.t(), flat_input)\n",
        "            self.embed_avg.data.mul_(self.decay).add_(dw, alpha=1 - self.decay)\n",
        "\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = ((self.cluster_size + self.epsilon) / (n + self.num_embeddings * self.epsilon) * n)\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), flat_input)\n",
        "        loss = self.commitment_cost * e_latent_loss\n",
        "        quantized = flat_input + (quantized - flat_input).detach()\n",
        "\n",
        "        quantized = quantized.view(input_shape[0], input_shape[2], input_shape[3], self.embedding_dim)\n",
        "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        return quantized, loss, perplexity, encoding_indices\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels, num_residual_hiddens, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_residual_hiddens),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(num_residual_hiddens, num_hiddens, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_hiddens)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            ResidualBlock(in_channels, num_hiddens, num_residual_hiddens)\n",
        "            for _ in range(num_residual_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, num_hiddens // 2, 4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(num_hiddens // 2, num_hiddens, 4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(num_hiddens, num_hiddens, 3, padding=1)\n",
        "        self.residual_stack = ResidualStack(num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        return self.residual_stack(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(embedding_dim, num_hiddens, 3, padding=1)\n",
        "        self.residual_stack = ResidualStack(num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
        "        self.conv_trans1 = nn.ConvTranspose2d(num_hiddens, num_hiddens // 2, 4, stride=2, padding=1)\n",
        "        self.conv_trans2 = nn.ConvTranspose2d(num_hiddens // 2, 3, 4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.residual_stack(x)\n",
        "        x = F.relu(self.conv_trans1(x))\n",
        "        return torch.tanh(self.conv_trans2(x))\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(3, config.num_hiddens, config.num_residual_layers, config.num_residual_hiddens)\n",
        "        self.pre_vq_conv = nn.Conv2d(config.num_hiddens, config.embedding_dim, 1)\n",
        "        self.vq = VectorQuantizerEMA(config.num_embeddings, config.embedding_dim, config.commitment_cost, decay=config.decay)\n",
        "        self.decoder = Decoder(config.embedding_dim, config.num_hiddens, config.num_residual_layers, config.num_residual_hiddens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        z = self.pre_vq_conv(z)\n",
        "        quantized, vq_loss, perplexity, encoding_indices = self.vq(z)\n",
        "        x_recon = self.decoder(quantized)\n",
        "        return x_recon, vq_loss, perplexity, encoding_indices\n",
        "\n",
        "    def encode(self, x):\n",
        "        z = self.encoder(x)\n",
        "        z = self.pre_vq_conv(z)\n",
        "        _, _, _, encoding_indices = self.vq(z)\n",
        "        B = x.shape[0]\n",
        "        return encoding_indices.view(B, -1)\n",
        "\n",
        "# ============================================================================\n",
        "# Metrics\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_mse(original, reconstructed):\n",
        "    return F.mse_loss(reconstructed, original).item()\n",
        "\n",
        "def calculate_psnr(original, reconstructed, max_val=2.0):\n",
        "    mse = F.mse_loss(reconstructed, original)\n",
        "    return (10 * torch.log10(max_val**2 / mse)).item()\n",
        "\n",
        "def calculate_ssim(original, reconstructed):\n",
        "    orig_np = (original.detach().cpu().numpy() + 1) / 2\n",
        "    recon_np = (reconstructed.detach().cpu().numpy() + 1) / 2\n",
        "    ssim_scores = [ssim(orig_np[i].transpose(1, 2, 0), recon_np[i].transpose(1, 2, 0),\n",
        "                       channel_axis=2, data_range=1.0) for i in range(orig_np.shape[0])]\n",
        "    return np.mean(ssim_scores)\n",
        "\n",
        "def calculate_codebook_usage(model, dataloader, device):\n",
        "    \"\"\"Calculate codebook utilization percentage\"\"\"\n",
        "    model.eval()\n",
        "    all_codes = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(device)\n",
        "            codes = model.encode(batch)\n",
        "            all_codes.append(codes.cpu())\n",
        "\n",
        "    all_codes = torch.cat(all_codes, dim=0).numpy().flatten()\n",
        "    unique_codes = len(np.unique(all_codes))\n",
        "    usage_percentage = (unique_codes / config.num_embeddings) * 100\n",
        "\n",
        "    return usage_percentage, unique_codes, all_codes\n",
        "\n",
        "# ============================================================================\n",
        "# Trainer with Codebook Monitoring\n",
        "# ============================================================================\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, config):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate_vqvae)\n",
        "        self.history = {'recon_loss': [], 'vq_loss': [], 'total_loss': [], 'perplexity': [],\n",
        "                       'mse': [], 'psnr': [], 'ssim': [], 'codebook_usage': []}\n",
        "        self.start_epoch = 0\n",
        "        self.best_usage = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch, filepath):\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'history': self.history\n",
        "        }\n",
        "        torch.save(checkpoint, filepath)\n",
        "        print(f\"âœ“ Checkpoint saved: {filepath}\")\n",
        "\n",
        "    def load_checkpoint(self, filepath):\n",
        "        if os.path.exists(filepath):\n",
        "            try:\n",
        "                checkpoint = torch.load(filepath, map_location=self.config.device)\n",
        "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                self.history = checkpoint['history']\n",
        "                self.start_epoch = checkpoint['epoch'] + 1\n",
        "                print(f\"âœ“ Checkpoint loaded, resuming from epoch {self.start_epoch}\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"âœ— Error loading checkpoint: {e}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "    def train_epoch(self, dataloader):\n",
        "        self.model.train()\n",
        "        epoch_recon_loss = epoch_vq_loss = epoch_perplexity = 0\n",
        "        pbar = tqdm(dataloader, desc=\"Training\")\n",
        "        for batch in pbar:\n",
        "            batch = batch.to(self.config.device)\n",
        "            recon, vq_loss, perplexity, _ = self.model(batch)\n",
        "            recon_loss = F.mse_loss(recon, batch)\n",
        "            loss = recon_loss + vq_loss\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            epoch_recon_loss += recon_loss.item()\n",
        "            epoch_vq_loss += vq_loss.item()\n",
        "            epoch_perplexity += perplexity.item()\n",
        "            pbar.set_postfix({'recon_loss': recon_loss.item(), 'vq_loss': vq_loss.item(), 'perplexity': perplexity.item()})\n",
        "        n = len(dataloader)\n",
        "        return {'recon_loss': epoch_recon_loss/n, 'vq_loss': epoch_vq_loss/n, 'perplexity': epoch_perplexity/n}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        all_original, all_recon = [], []\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(self.config.device)\n",
        "            recon, _, _, _ = self.model(batch)\n",
        "            all_original.append(batch)\n",
        "            all_recon.append(recon)\n",
        "        original = torch.cat(all_original, dim=0)\n",
        "        reconstructed = torch.cat(all_recon, dim=0)\n",
        "        return {'mse': calculate_mse(original, reconstructed),\n",
        "                'psnr': calculate_psnr(original, reconstructed),\n",
        "                'ssim': calculate_ssim(original[:64], reconstructed[:64])}\n",
        "\n",
        "    def train(self, train_loader, val_loader):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"PHASE 1: VQ-VAE TRAINING WITH CODEBOOK MONITORING\")\n",
        "        print(f\"Target: â‰¥{self.config.min_codebook_usage}% codebook utilization\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        for epoch in range(self.start_epoch, self.config.num_epochs_vqvae):\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs_vqvae}\")\n",
        "\n",
        "            train_metrics = self.train_epoch(train_loader)\n",
        "            val_metrics = self.evaluate(val_loader)\n",
        "\n",
        "            # Check codebook usage periodically\n",
        "            if (epoch + 1) % self.config.check_usage_every == 0:\n",
        "                usage_pct, unique_codes, _ = calculate_codebook_usage(self.model, train_loader, self.config.device)\n",
        "                self.history['codebook_usage'].append(usage_pct)\n",
        "\n",
        "                print(f\"\\n{'â”€'*60}\")\n",
        "                print(f\"ðŸ“Š CODEBOOK USAGE CHECK (Epoch {epoch+1})\")\n",
        "                print(f\"{'â”€'*60}\")\n",
        "                print(f\"Active codes: {unique_codes}/{self.config.num_embeddings}\")\n",
        "                print(f\"Usage: {usage_pct:.2f}%\")\n",
        "\n",
        "                if usage_pct > self.best_usage:\n",
        "                    self.best_usage = usage_pct\n",
        "                    print(f\"âœ“ New best usage: {usage_pct:.2f}%\")\n",
        "                    self.save_checkpoint(epoch, os.path.join(self.config.checkpoint_dir, 'vqvae_best_usage.pt'))\n",
        "\n",
        "                if usage_pct >= self.config.min_codebook_usage:\n",
        "                    print(f\"âœ“ TARGET REACHED! ({usage_pct:.2f}% â‰¥ {self.config.min_codebook_usage}%)\")\n",
        "                else:\n",
        "                    remaining = self.config.min_codebook_usage - usage_pct\n",
        "                    print(f\"âš  Need {remaining:.2f}% more to reach target\")\n",
        "                print(f\"{'â”€'*60}\\n\")\n",
        "\n",
        "            metrics = {**train_metrics, **val_metrics}\n",
        "            for key, value in metrics.items():\n",
        "                self.history[key].append(value)\n",
        "\n",
        "            print(f\"Recon: {metrics['recon_loss']:.4f}, VQ: {metrics['vq_loss']:.4f}, Perplexity: {metrics['perplexity']:.2f}\")\n",
        "            print(f\"MSE: {metrics['mse']:.4f}, PSNR: {metrics['psnr']:.2f}, SSIM: {metrics['ssim']:.4f}\")\n",
        "\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                self.save_checkpoint(epoch, os.path.join(self.config.checkpoint_dir, f'vqvae_epoch_{epoch+1}.pt'))\n",
        "\n",
        "        self.save_checkpoint(self.config.num_epochs_vqvae - 1, os.path.join(self.config.checkpoint_dir, 'vqvae_final.pt'))\n",
        "\n",
        "        # Final usage check\n",
        "        final_usage, final_unique, _ = calculate_codebook_usage(self.model, train_loader, self.config.device)\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"PHASE 1 COMPLETE\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Final codebook usage: {final_usage:.2f}% ({final_unique}/{self.config.num_embeddings})\")\n",
        "        print(f\"Best codebook usage: {self.best_usage:.2f}%\")\n",
        "\n",
        "        if final_usage >= self.config.min_codebook_usage:\n",
        "            print(f\"âœ“ SUCCESS: Codebook utilization target achieved!\")\n",
        "            return True, self.history\n",
        "        else:\n",
        "            print(f\"âš  WARNING: Codebook utilization below target ({final_usage:.2f}% < {self.config.min_codebook_usage}%)\")\n",
        "            print(f\"Consider: 1) Training longer, 2) Reducing num_embeddings, 3) Adjusting commitment_cost\")\n",
        "            return False, self.history\n",
        "\n",
        "# ============================================================================\n",
        "# Visualization\n",
        "# ============================================================================\n",
        "\n",
        "def plot_training_history(history, save_path):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes[0, 0].plot(history['recon_loss'])\n",
        "    axes[0, 0].set_title('Reconstruction Loss')\n",
        "    axes[0, 1].plot(history['vq_loss'])\n",
        "    axes[0, 1].set_title('VQ Loss')\n",
        "    axes[0, 2].plot(history['perplexity'])\n",
        "    axes[0, 2].set_title('Perplexity')\n",
        "    axes[1, 0].plot(history['mse'])\n",
        "    axes[1, 0].set_title('MSE')\n",
        "    axes[1, 1].plot(history['psnr'])\n",
        "    axes[1, 1].set_title('PSNR')\n",
        "    axes[1, 2].plot(history['ssim'])\n",
        "    axes[1, 2].set_title('SSIM')\n",
        "    for ax in axes.flat:\n",
        "        ax.grid(True)\n",
        "        ax.set_xlabel('Epoch')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_reconstructions(model, dataloader, device, save_path, num_images=8):\n",
        "    model.eval()\n",
        "    images = next(iter(dataloader))[:num_images].to(device)\n",
        "    with torch.no_grad():\n",
        "        reconstructions, _, _, _ = model(images)\n",
        "    images, reconstructions = (images + 1) / 2, (reconstructions + 1) / 2\n",
        "    comparison = torch.cat([images, reconstructions])\n",
        "    grid = make_grid(comparison, nrow=num_images, padding=2)\n",
        "    plt.figure(figsize=(16, 4))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.title('Top: Original, Bottom: Reconstructed')\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_codebook_usage(history, all_codes, save_path):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Usage over time\n",
        "    if len(history['codebook_usage']) > 0:\n",
        "        epochs = [i * config.check_usage_every for i in range(1, len(history['codebook_usage']) + 1)]\n",
        "        axes[0].plot(epochs, history['codebook_usage'], marker='o', linewidth=2)\n",
        "        axes[0].axhline(y=config.min_codebook_usage, color='r', linestyle='--', label=f'Target ({config.min_codebook_usage}%)')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Codebook Usage (%)')\n",
        "        axes[0].set_title('Codebook Utilization Over Time')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Histogram\n",
        "    axes[1].hist(all_codes, bins=config.num_embeddings, edgecolor='black')\n",
        "    axes[1].set_xlabel('Code Index')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    axes[1].set_title(f'Code Distribution (Final)')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def save_results_to_csv(config, results, save_path):\n",
        "    \"\"\"Save experiment config and results to CSV\"\"\"\n",
        "    import csv\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Combine config and results\n",
        "    row_data = {\n",
        "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        **config.to_dict(),\n",
        "        **results\n",
        "    }\n",
        "\n",
        "    # Check if file exists to determine if we need headers\n",
        "    file_exists = os.path.exists(save_path)\n",
        "\n",
        "    with open(save_path, 'a', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=row_data.keys())\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(row_data)\n",
        "\n",
        "    print(f\"Results appended to: {save_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Main\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*80)\n",
        "    print(\"VQ-VAE EMOJI GENERATION - PHASE 1: CODEBOOK TRAINING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Dataset\n",
        "    if not os.listdir(config.data_dir):\n",
        "        download_emoji_dataset()\n",
        "\n",
        "    dataset = EmojiDataset(config.data_dir, config.image_size)\n",
        "    train_size = int(0.9 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers, pin_memory=True)\n",
        "\n",
        "    print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
        "\n",
        "    # Train VQ-VAE\n",
        "    vqvae = VQVAE(config).to(config.device)\n",
        "    trainer = Trainer(vqvae, config)\n",
        "\n",
        "    if not trainer.load_checkpoint(os.path.join(config.checkpoint_dir, 'vqvae_final.pt')):\n",
        "        success, history = trainer.train(train_loader, val_loader)\n",
        "        plot_training_history(history, os.path.join(config.results_dir, 'training_history.png'))\n",
        "\n",
        "        # Final codebook analysis\n",
        "        usage_pct, unique_codes, all_codes = calculate_codebook_usage(vqvae, train_loader, config.device)\n",
        "        plot_codebook_usage(history, all_codes, os.path.join(config.results_dir, 'codebook_usage.png'))\n",
        "\n",
        "        if not success:\n",
        "            print(\"\\nâš  Phase 1 did not reach target codebook usage. Review suggestions above.\")\n",
        "            return\n",
        "\n",
        "    # Visualize results\n",
        "    plot_reconstructions(vqvae, val_loader, config.device, os.path.join(config.results_dir, 'reconstructions.png'))\n",
        "\n",
        "    # Save summary\n",
        "    usage_pct, unique_codes, _ = calculate_codebook_usage(vqvae, train_loader, config.device)\n",
        "    summary = {\n",
        "        'codebook_usage_percent': float(usage_pct),\n",
        "        'active_codes': int(unique_codes),\n",
        "        'total_codes': config.num_embeddings,\n",
        "        'final_perplexity': trainer.history['perplexity'][-1] if trainer.history['perplexity'] else None,\n",
        "        'target_achieved': usage_pct >= config.min_codebook_usage\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(config.results_dir, 'phase1_summary.json'), 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "\n",
        "    # Prepare results for CSV\n",
        "    results = {\n",
        "        'phase': 'phase1',\n",
        "        'codebook_usage_percent': float(usage_pct),\n",
        "        'active_codes': int(unique_codes),\n",
        "        'best_codebook_usage': trainer.best_usage,\n",
        "        'final_perplexity': trainer.history['perplexity'][-1] if trainer.history['perplexity'] else None,\n",
        "        'final_recon_loss': trainer.history['recon_loss'][-1] if trainer.history['recon_loss'] else None,\n",
        "        'final_vq_loss': trainer.history['vq_loss'][-1] if trainer.history['vq_loss'] else None,\n",
        "        'final_mse': trainer.history['mse'][-1] if trainer.history['mse'] else None,\n",
        "        'final_psnr': trainer.history['psnr'][-1] if trainer.history['psnr'] else None,\n",
        "        'final_ssim': trainer.history['ssim'][-1] if trainer.history['ssim'] else None,\n",
        "        'target_achieved': usage_pct >= config.min_codebook_usage,\n",
        "        'training_completed': True\n",
        "    }\n",
        "\n",
        "    # Save to CSV\n",
        "    csv_path = os.path.join(config.results_dir, 'experiment_results.csv')\n",
        "    save_results_to_csv(config, results, csv_path)\n",
        "\n",
        "    # Also save JSON for Phase 2 compatibility\n",
        "    with open(os.path.join(config.results_dir, 'phase1_summary.json'), 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nâœ“ Phase 1 complete. Results saved to {config.results_dir}\")\n",
        "    print(f\"âœ“ Ready for Phase 2: Prior training and generation\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "576b7be713b248d8af68dbb257f6ac07",
            "7760bc39170e4a578724d1a9766406b1",
            "7147be28ef8a4feca9a1f36162506804",
            "86a1f0e9f6f64425a69a60ae949822c7",
            "b3ad216bc9174627b39b224d08f201f8",
            "2a68a9c162504d079433b14e3a2c9486",
            "d7cde14fe3bd42229a49826000ccc342",
            "3a89491b9cc7431899758b2ce285aab4",
            "9c5ccbfb353342eea79f25585522f439",
            "a11ed5b605e54b8fa80718c491ce98af",
            "dba10b82b9424edfbd650294726440bb",
            "a3d263cd9b8646aea1c258cfa57f8ff1",
            "b7c6723b741a4202a49d3e54c68a2373",
            "353f4c230fcd4491bbed14a7042fcfc2",
            "dc5df4a030d542f8b91ed37006357395",
            "2fb221bdfd1f4c459ce7c6d5c2bada65",
            "4a6f3f5994e947d78e24d047ac97b095",
            "8ef33255cd59430296314c33959134bc",
            "05fbc5784b7c4ac2aceb0117e73bec1d",
            "ca3f9c49a9ec40678202491e6a9b444d",
            "5eac55ad9f4543feaeece364932efe2c",
            "6e5df2d2a61e4a71a49cf97a45d2b7b0",
            "208833f5c22b4298a7070363cfb3eeb3",
            "6c2ecd1799d9460f898f4ba34f905d70",
            "2027ce8946bb4d7c995dd2b9e9c05b46",
            "a0d8542fdecd4eb79ab74ce6bdecff6f",
            "5a3d220ca4984ba087317a19d2a0ffb1",
            "bb50bed2b3f248cead2ef1282c647b0f",
            "d43d85bea2f34e4da080003146f092e6",
            "c27457a3aba64afa9a09541740f31dbd",
            "132f300edfcf448cbf3f0d4457f0eb15",
            "7db862cdfbc44ec982c4c203ffa3c9f0",
            "5674198464e846e4b9135c5d31b36d9e"
          ]
        },
        "id": "8X4_G3n9xTR_",
        "outputId": "6c0c750b-1972-4956-985d-85a38805053f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "================================================================================\n",
            "VQ-VAE Emoji Generation Project - ENHANCED VERSION\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 1: Dataset Preparation\n",
            "================================================================================\n",
            "Downloading emoji dataset...\n",
            "Error downloading dataset: No module named 'datasets'\n",
            "Please manually download emojis to the data directory\n",
            "Total dataset size: 0 images\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1583\u001b[39m\n\u001b[32m   1579\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m   1580\u001b[39m \u001b[38;5;66;03m# Run the complete pipeline\u001b[39;00m\n\u001b[32m   1581\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m   1582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1583\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1329\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1324\u001b[39m val_size = \u001b[38;5;28mlen\u001b[39m(dataset) - train_size\n\u001b[32m   1325\u001b[39m train_dataset, val_dataset = torch.utils.data.random_split(\n\u001b[32m   1326\u001b[39m     dataset, [train_size, val_size]\n\u001b[32m   1327\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m train_loader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m   1332\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1333\u001b[39m val_loader = DataLoader(\n\u001b[32m   1334\u001b[39m     val_dataset, batch_size=config.batch_size,\n\u001b[32m   1335\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=config.num_workers, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1336\u001b[39m )\n\u001b[32m   1338\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m images\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/posit_nn/lib/python3.13/site-packages/torch/utils/data/dataloader.py:388\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    390\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/posit_nn/lib/python3.13/site-packages/torch/utils/data/sampler.py:156\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "\"\"\"\n",
        "VQ-VAE Emoji Generation Project - ENHANCED VERSION\n",
        "Complete implementation with improved codebook utilization monitoring and all analysis from both codes\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "import seaborn as sns\n",
        "from scipy import linalg\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    # Paths\n",
        "    data_dir = './emoji_data'\n",
        "    checkpoint_dir = './checkpoints'\n",
        "    results_dir = './results'\n",
        "\n",
        "    # Data\n",
        "    image_size = 64\n",
        "    batch_size = 64  # Reduced from 128 for stability\n",
        "    num_workers = 2\n",
        "\n",
        "    # VQ-VAE Architecture - UPDATED FOR BETTER CODEBOOK USAGE\n",
        "    num_hiddens = 128\n",
        "    num_residual_hiddens = 32\n",
        "    num_residual_layers = 2\n",
        "    embedding_dim = 64\n",
        "    num_embeddings = 512\n",
        "    commitment_cost = 0.25\n",
        "\n",
        "    # Add EMA for codebook (optional but helps)\n",
        "    decay = 0.999\n",
        "\n",
        "    # Training\n",
        "    num_epochs_vqvae = 100\n",
        "    learning_rate_vqvae = 3e-4\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Prior (PixelCNN) - UPDATED\n",
        "    num_epochs_prior = 50\n",
        "    learning_rate_prior = 1e-4  # Reduced from 3e-4 for stability\n",
        "    pixelcnn_layers = 12  # Reduced from 15\n",
        "    pixelcnn_hidden = 64  # Reduced from 128\n",
        "    grad_clip = 1.0  # Add gradient clipping\n",
        "\n",
        "    # Generation\n",
        "    num_samples = 64\n",
        "\n",
        "    # Interpolation\n",
        "    num_interpolation_steps = 10\n",
        "\n",
        "    # Codebook utilization threshold\n",
        "    codebook_utilization_threshold = 0.5  # 50% minimum utilization\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(config.results_dir, exist_ok=True)\n",
        "os.makedirs(config.data_dir, exist_ok=True)\n",
        "print(f\"Using device: {config.device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Dataset Preparation\n",
        "# ============================================================================\n",
        "class EmojiDataset(Dataset):\n",
        "    \"\"\"Dataset class for emoji images\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, image_size=64, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.image_size = image_size\n",
        "        self.image_files = [f for f in os.listdir(data_dir)\n",
        "                           if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((image_size, image_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "def download_emoji_dataset():\n",
        "    \"\"\"Download emoji dataset from HuggingFace\"\"\"\n",
        "    print(\"Downloading emoji dataset...\")\n",
        "    try:\n",
        "        from datasets import load_dataset\n",
        "        dataset = load_dataset(\"valhalla/emoji-dataset\", split=\"train\")\n",
        "\n",
        "        print(f\"Downloaded {len(dataset)} emojis\")\n",
        "        print(\"Saving images to disk...\")\n",
        "\n",
        "        for idx, item in enumerate(tqdm(dataset)):\n",
        "            img = item['image']\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')\n",
        "            img.save(os.path.join(config.data_dir, f'emoji{idx:05d}.png'))\n",
        "\n",
        "        print(f\"Saved {len(dataset)} emoji images to {config.data_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading dataset: {e}\")\n",
        "        print(\"Please manually download emojis to the data directory\")\n",
        "\n",
        "# ============================================================================\n",
        "# VQ-VAE Model Components\n",
        "# ============================================================================\n",
        "class VectorQuantizerEMA(nn.Module):\n",
        "    \"\"\"Vector Quantization with Exponential Moving Average\"\"\"\n",
        "\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay=0.99, epsilon=1e-5):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.commitment_cost = commitment_cost\n",
        "        self.decay = decay\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Initialize embeddings correctly - shape should be [num_embeddings, embedding_dim]\n",
        "        embed = torch.randn(num_embeddings, embedding_dim)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(num_embeddings))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs: [B, C, H, W]\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input: [B, C, H, W] -> [BHW, C]\n",
        "        flat_input = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        flat_input = flat_input.view(-1, self.embedding_dim)\n",
        "\n",
        "        # Calculate distances - fixed dimensions\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) +\n",
        "                    torch.sum(self.embed**2, dim=1) -\n",
        "                    2 * torch.matmul(flat_input, self.embed.t()))\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings,\n",
        "                               device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize\n",
        "        quantized = torch.matmul(encodings, self.embed)\n",
        "\n",
        "        # Use EMA to update the embedding vectors\n",
        "        if self.training:\n",
        "            # Update cluster sizes\n",
        "            self.cluster_size.data.mul_(self.decay).add_(\n",
        "                encodings.sum(0), alpha=1 - self.decay\n",
        "            )\n",
        "\n",
        "            # Update embedding averages\n",
        "            dw = torch.matmul(encodings.t(), flat_input)\n",
        "            self.embed_avg.data.mul_(self.decay).add_(dw, alpha=1 - self.decay)\n",
        "\n",
        "            # Normalize embeddings\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = (\n",
        "                (self.cluster_size + self.epsilon) /\n",
        "                (n + self.num_embeddings * self.epsilon) * n\n",
        "            )\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), flat_input)\n",
        "        loss = self.commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight-through estimator\n",
        "        quantized = flat_input + (quantized - flat_input).detach()\n",
        "\n",
        "        # Reshape back\n",
        "        quantized = quantized.view(input_shape[0], input_shape[2],\n",
        "                                   input_shape[3], self.embedding_dim)\n",
        "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        # Perplexity\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        return quantized, loss, perplexity, encoding_indices\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block for encoder/decoder\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels, num_residual_hiddens, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_residual_hiddens),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(num_residual_hiddens, num_hiddens, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_hiddens)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    \"\"\"Stack of residual blocks\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            ResidualBlock(in_channels, num_hiddens, num_residual_hiddens)\n",
        "            for _ in range(num_residual_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder network\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, num_hiddens // 2, 4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(num_hiddens // 2, num_hiddens, 4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(num_hiddens, num_hiddens, 3, padding=1)\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        return self.residual_stack(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder network\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(embedding_dim, num_hiddens, 3, padding=1)\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "        self.conv_trans1 = nn.ConvTranspose2d(num_hiddens, num_hiddens // 2,\n",
        "                                              4, stride=2, padding=1)\n",
        "        self.conv_trans2 = nn.ConvTranspose2d(num_hiddens // 2, 3,\n",
        "                                              4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.residual_stack(x)\n",
        "        x = F.relu(self.conv_trans1(x))\n",
        "        return torch.tanh(self.conv_trans2(x))\n",
        "\n",
        "# ============================================================================\n",
        "# FIX 2: Debug the spatial size issue\n",
        "# ============================================================================\n",
        "# Add this function to check architecture:\n",
        "def verify_architecture():\n",
        "    \"\"\"Verify the VQ-VAE produces correct spatial sizes\"\"\"\n",
        "    print(\"\\nVerifying architecture...\")\n",
        "    model = VQVAE(config).to(config.device)\n",
        "\n",
        "    # Test with dummy input\n",
        "    dummy_input = torch.randn(1, 3, 64, 64).to(config.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Check encoder output\n",
        "        z = model.encoder(dummy_input)\n",
        "        print(f\"After encoder: {z.shape}\")\n",
        "\n",
        "        z = model.pre_vq_conv(z)\n",
        "        print(f\"After pre_vq_conv: {z.shape}\")\n",
        "\n",
        "        # Check VQ\n",
        "        quantized, _, _, codes = model.vq(z)\n",
        "        print(f\"After VQ: {quantized.shape}\")\n",
        "        print(f\"Codes shape: {codes.shape}\")\n",
        "\n",
        "        # Check decoder\n",
        "        recon = model.decoder(quantized)\n",
        "        print(f\"After decoder: {recon.shape}\")\n",
        "\n",
        "        # Calculate spatial size\n",
        "        spatial_size = z.shape[2]\n",
        "        print(f\"\\nExpected latent spatial size: {spatial_size}x{spatial_size}\")\n",
        "\n",
        "    return model, spatial_size\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    \"\"\"Complete VQ-VAE model with EMA\"\"\"\n",
        "\n",
        "    def __init__(self, config, use_ema=True):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(3, config.num_hiddens, config.num_residual_layers,\n",
        "                              config.num_residual_hiddens)\n",
        "        self.pre_vq_conv = nn.Conv2d(config.num_hiddens, config.embedding_dim, 1)\n",
        "\n",
        "        # Choose VQ method\n",
        "        if use_ema:\n",
        "            self.vq = VectorQuantizerEMA(\n",
        "                config.num_embeddings,\n",
        "                config.embedding_dim,\n",
        "                config.commitment_cost,\n",
        "                decay=config.decay\n",
        "            )\n",
        "        else:\n",
        "            self.vq = VectorQuantizerEMA(\n",
        "                config.num_embeddings,\n",
        "                config.embedding_dim,\n",
        "                config.commitment_cost\n",
        "            )\n",
        "\n",
        "        self.decoder = Decoder(config.embedding_dim, config.num_hiddens,\n",
        "                              config.num_residual_layers, config.num_residual_hiddens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        z = self.pre_vq_conv(z)\n",
        "        quantized, vq_loss, perplexity, encoding_indices = self.vq(z)\n",
        "        x_recon = self.decoder(quantized)\n",
        "        return x_recon, vq_loss, perplexity, encoding_indices\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Encode to discrete codes\"\"\"\n",
        "        z = self.encoder(x)\n",
        "        z = self.pre_vq_conv(z)\n",
        "        _, _, _, encoding_indices = self.vq(z)\n",
        "        # Reshape to [B, H*W]\n",
        "        B = x.shape[0]\n",
        "        encoding_indices = encoding_indices.view(B, -1)\n",
        "        return encoding_indices\n",
        "\n",
        "    def decode_codes(self, encoding_indices, spatial_size):\n",
        "        \"\"\"Decode from discrete codes\"\"\"\n",
        "        # Reshape codes to spatial layout\n",
        "        codes = encoding_indices.view(-1, spatial_size, spatial_size)\n",
        "\n",
        "        # Get quantized vectors - FIXED for both versions\n",
        "        if hasattr(self.vq, 'embed'):  # EMA version\n",
        "            quantized = F.embedding(codes, self.vq.embed)  # Remove .t()\n",
        "        else:  # Original version\n",
        "            quantized = self.vq.embeddings(codes)\n",
        "\n",
        "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        # Decode\n",
        "        return self.decoder(quantized)\n",
        "\n",
        "# ============================================================================\n",
        "# PixelCNN Prior\n",
        "# ============================================================================\n",
        "class MaskedConv2d(nn.Conv2d):\n",
        "    \"\"\"Masked convolution for autoregressive modeling\"\"\"\n",
        "\n",
        "    def __init__(self, mask_type, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.register_buffer('mask', torch.zeros_like(self.weight))\n",
        "        self.create_mask(mask_type)\n",
        "\n",
        "    def create_mask(self, mask_type):\n",
        "        k = self.kernel_size[0]\n",
        "        self.mask[:, :, :k//2, :] = 1\n",
        "        self.mask[:, :, k//2, :k//2] = 1\n",
        "        if mask_type == 'B':\n",
        "            self.mask[:, :, k//2, k//2] = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.weight.data *= self.mask\n",
        "        return super().forward(x)\n",
        "\n",
        "class PixelCNNResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block for PixelCNN\"\"\"\n",
        "\n",
        "    def __init__(self, h):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MaskedConv2d('B', h, h, 1),\n",
        "            nn.BatchNorm2d(h),\n",
        "            nn.ReLU(),\n",
        "            MaskedConv2d('B', h, h, 1),\n",
        "            nn.BatchNorm2d(h)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv(x)\n",
        "\n",
        "class PixelCNN(nn.Module):\n",
        "    \"\"\"PixelCNN prior for discrete codes with stability improvements\"\"\"\n",
        "\n",
        "    def __init__(self, num_embeddings, num_layers=12, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "\n",
        "        # Input layer\n",
        "        self.input_conv = MaskedConv2d('A', num_embeddings, hidden_dim, 7, padding=3)\n",
        "\n",
        "        # Residual blocks\n",
        "        self.residual_blocks = nn.ModuleList([\n",
        "            PixelCNNResidualBlock(hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layers\n",
        "        self.output = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            MaskedConv2d('B', hidden_dim, hidden_dim, 1),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_dim, num_embeddings, 1)\n",
        "        )\n",
        "\n",
        "        # Initialize weights properly\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights with smaller values for stability\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, MaskedConv2d)):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=0.1)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, H, W] with values in [0, num_embeddings)\n",
        "        # Convert to one-hot: [B, num_embeddings, H, W]\n",
        "        x_onehot = F.one_hot(x, self.num_embeddings).float()\n",
        "        x_onehot = x_onehot.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        x = self.input_conv(x_onehot)\n",
        "        for block in self.residual_blocks:\n",
        "            x = block(x)\n",
        "        return self.output(x)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, batch_size, spatial_size, device, temperature=1.0):\n",
        "        \"\"\"Generate samples autoregressively\"\"\"\n",
        "        samples = torch.zeros(batch_size, spatial_size, spatial_size,\n",
        "                            dtype=torch.long, device=device)\n",
        "\n",
        "        for i in range(spatial_size):\n",
        "            for j in range(spatial_size):\n",
        "                logits = self(samples)\n",
        "                probs = F.softmax(logits[:, :, i, j] / temperature, dim=1)\n",
        "                samples[:, i, j] = torch.multinomial(probs, 1).squeeze(-1)\n",
        "\n",
        "        return samples\n",
        "\n",
        "# ============================================================================\n",
        "# Metrics\n",
        "# ============================================================================\n",
        "def calculate_mse(original, reconstructed):\n",
        "    \"\"\"Calculate Mean Squared Error\"\"\"\n",
        "    mse = F.mse_loss(reconstructed, original)\n",
        "    return mse.item()\n",
        "\n",
        "def calculate_psnr(original, reconstructed, max_val=2.0):\n",
        "    \"\"\"Calculate Peak Signal-to-Noise Ratio\"\"\"\n",
        "    mse = F.mse_loss(reconstructed, original)\n",
        "    psnr = 10 * torch.log10(max_val**2 / mse)\n",
        "    return psnr.item()\n",
        "\n",
        "def calculate_ssim(original, reconstructed):\n",
        "    \"\"\"Calculate Structural Similarity Index\"\"\"\n",
        "    # Convert to numpy and normalize to [0, 1]\n",
        "    orig_np = (original.detach().cpu().numpy() + 1) / 2\n",
        "    recon_np = (reconstructed.detach().cpu().numpy() + 1) / 2\n",
        "\n",
        "    ssim_scores = []\n",
        "    for i in range(orig_np.shape[0]):\n",
        "        score = ssim(orig_np[i].transpose(1, 2, 0),\n",
        "                    recon_np[i].transpose(1, 2, 0),\n",
        "                    multichannel=True, data_range=1.0, channel_axis=2)\n",
        "        ssim_scores.append(score)\n",
        "\n",
        "    return np.mean(ssim_scores)\n",
        "\n",
        "def calculate_fid(real_features, fake_features):\n",
        "    \"\"\"Calculate FrÃ©chet Inception Distance\"\"\"\n",
        "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
        "    mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
        "\n",
        "    ssdiff = np.sum((mu1 - mu2)**2)\n",
        "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2*covmean)\n",
        "    return fid\n",
        "\n",
        "def get_inception_features(images, model, device):\n",
        "    \"\"\"Extract features using a pretrained model\"\"\"\n",
        "    from torchvision.models import inception_v3\n",
        "\n",
        "    if model is None:\n",
        "        model = inception_v3(pretrained=True, transform_input=False)\n",
        "        model.fc = nn.Identity()\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Resize images to 299x299 for Inception\n",
        "        images_resized = F.interpolate(images, size=(299, 299), mode='bilinear')\n",
        "        features = model(images_resized)\n",
        "\n",
        "    return features.cpu().numpy(), model\n",
        "\n",
        "# ============================================================================\n",
        "# Training Functions\n",
        "# ============================================================================\n",
        "class Trainer:\n",
        "    \"\"\"Trainer class for VQ-VAE\"\"\"\n",
        "\n",
        "    def __init__(self, model, config):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate_vqvae)\n",
        "        self.history = {\n",
        "            'recon_loss': [], 'vq_loss': [], 'total_loss': [],\n",
        "            'perplexity': [], 'mse': [], 'psnr': [], 'ssim': []\n",
        "        }\n",
        "        self.start_epoch = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch, filepath):\n",
        "        \"\"\"Save training checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'history': self.history\n",
        "        }\n",
        "        torch.save(checkpoint, filepath)\n",
        "        print(f\"Checkpoint saved: {filepath}\")\n",
        "\n",
        "    def load_checkpoint(self, filepath):\n",
        "        \"\"\"Load training checkpoint\"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            try:\n",
        "                # First try with weights_only=True (safe)\n",
        "                checkpoint = torch.load(filepath, map_location=self.config.device, weights_only=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Safe loading failed: {e}\")\n",
        "                print(\"Attempting unsafe load (only use if you trust the checkpoint source)...\")\n",
        "                # Fallback to unsafe load\n",
        "                checkpoint = torch.load(filepath, map_location=self.config.device, weights_only=False)\n",
        "\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.history = checkpoint['history']\n",
        "            self.start_epoch = checkpoint['epoch'] + 1\n",
        "            print(f\"Checkpoint loaded: {filepath}, resuming from epoch {self.start_epoch}\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def train_epoch(self, dataloader):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        epoch_recon_loss = 0\n",
        "        epoch_vq_loss = 0\n",
        "        epoch_perplexity = 0\n",
        "\n",
        "        pbar = tqdm(dataloader, desc=\"Training\")\n",
        "        for batch in pbar:\n",
        "            batch = batch.to(self.config.device)\n",
        "\n",
        "            # Forward pass\n",
        "            recon, vq_loss, perplexity, _ = self.model(batch)\n",
        "            recon_loss = F.mse_loss(recon, batch)\n",
        "            loss = recon_loss + vq_loss\n",
        "\n",
        "            # Backward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            epoch_recon_loss += recon_loss.item()\n",
        "            epoch_vq_loss += vq_loss.item()\n",
        "            epoch_perplexity += perplexity.item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'recon_loss': recon_loss.item(),\n",
        "                'vq_loss': vq_loss.item(),\n",
        "                'perplexity': perplexity.item()\n",
        "            })\n",
        "\n",
        "        n_batches = len(dataloader)\n",
        "        return {\n",
        "            'recon_loss': epoch_recon_loss / n_batches,\n",
        "            'vq_loss': epoch_vq_loss / n_batches,\n",
        "            'perplexity': epoch_perplexity / n_batches\n",
        "        }\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, dataloader):\n",
        "        \"\"\"Evaluate model\"\"\"\n",
        "        self.model.eval()\n",
        "        all_original = []\n",
        "        all_recon = []\n",
        "\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(self.config.device)\n",
        "            recon, _, _, _ = self.model(batch)\n",
        "            all_original.append(batch)\n",
        "            all_recon.append(recon)\n",
        "\n",
        "        original = torch.cat(all_original, dim=0)\n",
        "        reconstructed = torch.cat(all_recon, dim=0)\n",
        "\n",
        "        mse = calculate_mse(original, reconstructed)\n",
        "        psnr = calculate_psnr(original, reconstructed)\n",
        "        ssim_score = calculate_ssim(original[:64], reconstructed[:64])  # Sample for speed\n",
        "\n",
        "        return {'mse': mse, 'psnr': psnr, 'ssim': ssim_score}\n",
        "\n",
        "    def get_codebook_utilization(self, dataloader):\n",
        "        \"\"\"Calculate codebook utilization percentage\"\"\"\n",
        "        self.model.eval()\n",
        "        all_codes = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=\"Calculating codebook utilization\"):\n",
        "                batch = batch.to(self.config.device)\n",
        "                codes = self.model.encode(batch)\n",
        "                all_codes.append(codes.cpu())\n",
        "\n",
        "        all_codes = torch.cat(all_codes, dim=0).numpy().flatten()\n",
        "        unique_codes = len(np.unique(all_codes))\n",
        "        utilization_rate = unique_codes / self.config.num_embeddings\n",
        "\n",
        "        return utilization_rate, unique_codes\n",
        "\n",
        "    def train_phase1_codebook(self, train_loader, val_loader, target_utilization=0.5):\n",
        "        \"\"\"Phase 1: Train until codebook utilization reaches target\"\"\"\n",
        "        print(f\"Starting Phase 1: Training until codebook utilization >= {target_utilization*100}%\")\n",
        "\n",
        "        for epoch in range(self.start_epoch, self.config.num_epochs_vqvae):\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs_vqvae}\")\n",
        "\n",
        "            # Train\n",
        "            train_metrics = self.train_epoch(train_loader)\n",
        "\n",
        "            # Evaluate\n",
        "            val_metrics = self.evaluate(val_loader)\n",
        "\n",
        "            # Check codebook utilization\n",
        "            utilization_rate, unique_codes = self.get_codebook_utilization(train_loader)\n",
        "\n",
        "            # Combine metrics\n",
        "            metrics = {**train_metrics, **val_metrics}\n",
        "            metrics['utilization_rate'] = utilization_rate\n",
        "            metrics['unique_codes'] = unique_codes\n",
        "\n",
        "            for key, value in metrics.items():\n",
        "                if key in self.history:\n",
        "                    self.history[key].append(value)\n",
        "\n",
        "            # Print metrics\n",
        "            print(f\"Recon Loss: {metrics['recon_loss']:.4f}, \"\n",
        "                  f\"VQ Loss: {metrics['vq_loss']:.4f}, \"\n",
        "                  f\"Perplexity: {metrics['perplexity']:.2f}\")\n",
        "            print(f\"MSE: {metrics['mse']:.4f}, \"\n",
        "                  f\"PSNR: {metrics['psnr']:.2f}, \"\n",
        "                  f\"SSIM: {metrics['ssim']:.4f}\")\n",
        "            print(f\"Codebook Utilization: {unique_codes}/{self.config.num_embeddings} \"\n",
        "                  f\"({utilization_rate*100:.2f}%)\")\n",
        "\n",
        "            # Save checkpoint every 5 epochs\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                checkpoint_path = os.path.join(\n",
        "                    self.config.checkpoint_dir,\n",
        "                    f'vqvae_phase1_epoch{epoch+1}.pt'\n",
        "                )\n",
        "                self.save_checkpoint(epoch, checkpoint_path)\n",
        "\n",
        "            # Check if we've reached target utilization\n",
        "            if utilization_rate >= target_utilization:\n",
        "                print(f\"\\nðŸŽ‰ Target codebook utilization reached! \"\n",
        "                      f\"{utilization_rate*100:.2f}% >= {target_utilization*100}%\")\n",
        "                checkpoint_path = os.path.join(\n",
        "                    self.config.checkpoint_dir,\n",
        "                    f'vqvae_phase1_complete_epoch{epoch+1}.pt'\n",
        "                )\n",
        "                self.save_checkpoint(epoch, checkpoint_path)\n",
        "                return epoch + 1, metrics\n",
        "\n",
        "        print(f\"\\nâš ï¸  Target utilization not reached after {self.config.num_epochs_vqvae} epochs\")\n",
        "        return self.config.num_epochs_vqvae, metrics\n",
        "\n",
        "    def train_phase2_final(self, train_loader, val_loader, start_epoch):\n",
        "        \"\"\"Phase 2: Continue training for final quality\"\"\"\n",
        "        print(f\"\\nStarting Phase 2: Final training from epoch {start_epoch}\")\n",
        "\n",
        "        for epoch in range(start_epoch, self.config.num_epochs_vqvae):\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs_vqvae}\")\n",
        "\n",
        "            # Train\n",
        "            train_metrics = self.train_epoch(train_loader)\n",
        "\n",
        "            # Evaluate\n",
        "            val_metrics = self.evaluate(val_loader)\n",
        "\n",
        "            # Combine metrics\n",
        "            metrics = {**train_metrics, **val_metrics}\n",
        "            for key, value in metrics.items():\n",
        "                self.history[key].append(value)\n",
        "\n",
        "            # Print metrics\n",
        "            print(f\"Recon Loss: {metrics['recon_loss']:.4f}, \"\n",
        "                  f\"VQ Loss: {metrics['vq_loss']:.4f}, \"\n",
        "                  f\"Perplexity: {metrics['perplexity']:.2f}\")\n",
        "            print(f\"MSE: {metrics['mse']:.4f}, \"\n",
        "                  f\"PSNR: {metrics['psnr']:.2f}, \"\n",
        "                  f\"SSIM: {metrics['ssim']:.4f}\")\n",
        "\n",
        "            # Save checkpoint every 5 epochs\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                checkpoint_path = os.path.join(\n",
        "                    self.config.checkpoint_dir,\n",
        "                    f'vqvae_phase2_epoch{epoch+1}.pt'\n",
        "                )\n",
        "                self.save_checkpoint(epoch, checkpoint_path)\n",
        "\n",
        "        # Save final model\n",
        "        final_path = os.path.join(self.config.checkpoint_dir, 'vqvae_final.pt')\n",
        "        self.save_checkpoint(self.config.num_epochs_vqvae - 1, final_path)\n",
        "\n",
        "        return self.history\n",
        "\n",
        "# ============================================================================\n",
        "# Enhanced Analysis Functions (From Code 2)\n",
        "# ============================================================================\n",
        "def analyze_codebook_tsne(model, dataloader, device, save_path, num_samples=1000):\n",
        "    \"\"\"Enhanced t-SNE analysis of codebook usage\"\"\"\n",
        "    model.eval()\n",
        "    all_codes = []\n",
        "\n",
        "    print(\"Extracting codes for t-SNE analysis...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = batch.to(device)\n",
        "            codes = model.encode(batch)\n",
        "            all_codes.append(codes.cpu())\n",
        "            if len(all_codes) * config.batch_size >= num_samples:\n",
        "                break\n",
        "\n",
        "    all_codes = torch.cat(all_codes, dim=0).numpy()[:num_samples]\n",
        "\n",
        "    # Apply t-SNE if we have more than 2 dimensions\n",
        "    if all_codes.shape[1] > 2:\n",
        "        print(\"Applying t-SNE...\")\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "        codes_2d = tsne.fit_transform(all_codes)\n",
        "    else:\n",
        "        codes_2d = all_codes\n",
        "\n",
        "    # Plot t-SNE\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(codes_2d[:, 0], codes_2d[:, 1], alpha=0.6, s=10)\n",
        "    plt.title('t-SNE Visualization of Latent Codes')\n",
        "    plt.xlabel('t-SNE Component 1')\n",
        "    plt.ylabel('t-SNE Component 2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"t-SNE visualization saved to {save_path}\")\n",
        "\n",
        "    return codes_2d\n",
        "\n",
        "def analyze_codebook_clustering(model, dataloader, device, save_path, n_clusters=10, num_samples=1000):\n",
        "    \"\"\"Enhanced clustering analysis\"\"\"\n",
        "    model.eval()\n",
        "    all_codes = []\n",
        "    all_images = []\n",
        "\n",
        "    print(\"Extracting codes and images for clustering...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = batch.to(device)\n",
        "            codes = model.encode(batch)\n",
        "            all_codes.append(codes.cpu())\n",
        "            all_images.append(batch.cpu())\n",
        "            if len(all_codes) * config.batch_size >= num_samples:\n",
        "                break\n",
        "\n",
        "    all_codes = torch.cat(all_codes, dim=0).numpy()[:num_samples]\n",
        "    all_images = torch.cat(all_images, dim=0)[:num_samples]\n",
        "\n",
        "    # Apply t-SNE for visualization\n",
        "    if all_codes.shape[1] > 2:\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        codes_2d = tsne.fit_transform(all_codes)\n",
        "    else:\n",
        "        codes_2d = all_codes\n",
        "\n",
        "    # K-means clustering\n",
        "    print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(all_codes)\n",
        "\n",
        "    # Plot clustering results\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    scatter = plt.scatter(codes_2d[:, 0], codes_2d[:, 1], c=cluster_labels,\n",
        "                         alpha=0.6, cmap='tab10', s=20)\n",
        "    plt.title('K-means Clustering of Latent Codes')\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Show cluster sizes\n",
        "    plt.subplot(1, 2, 2)\n",
        "    cluster_sizes = np.bincount(cluster_labels, minlength=n_clusters)\n",
        "    plt.bar(range(n_clusters), cluster_sizes)\n",
        "    plt.title('Cluster Sizes Distribution')\n",
        "    plt.xlabel('Cluster ID')\n",
        "    plt.ylabel('Number of Samples')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Clustering analysis saved to {save_path}\")\n",
        "\n",
        "    return cluster_labels, kmeans, codes_2d\n",
        "\n",
        "def enhanced_codebook_analysis(model, dataloader, device, save_dir):\n",
        "    \"\"\"Comprehensive codebook analysis combining both approaches\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Basic codebook usage (from Code 1)\n",
        "    codebook_stats = visualize_codebook_usage(\n",
        "        model, dataloader, device,\n",
        "        os.path.join(save_dir, 'codebook_usage.png')\n",
        "    )\n",
        "\n",
        "    # 2. t-SNE analysis (from Code 2)\n",
        "    codes_2d = analyze_codebook_tsne(\n",
        "        model, dataloader, device,\n",
        "        os.path.join(save_dir, 'tsne_codes.png')\n",
        "    )\n",
        "\n",
        "    # 3. Clustering analysis (from Code 2)\n",
        "    cluster_labels, kmeans, codes_2d_cluster = analyze_codebook_clustering(\n",
        "        model, dataloader, device,\n",
        "        os.path.join(save_dir, 'clustering_analysis.png')\n",
        "    )\n",
        "\n",
        "    # 4. Combined analysis plot\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Code usage histogram\n",
        "    plt.subplot(1, 3, 1)\n",
        "    code_counts = codebook_stats['code_counts']\n",
        "    plt.hist(code_counts[code_counts > 0], bins=50, edgecolor='black', alpha=0.7)\n",
        "    plt.title(f'Code Usage Distribution\\n(Active: {codebook_stats[\"active_codes\"]}/{codebook_stats[\"total_codes\"]})')\n",
        "    plt.xlabel('Usage Count')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # t-SNE visualization\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.scatter(codes_2d[:, 0], codes_2d[:, 1], alpha=0.6, s=10)\n",
        "    plt.title('Latent Codes t-SNE')\n",
        "    plt.xlabel('t-SNE 1')\n",
        "    plt.ylabel('t-SNE 2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Clustering visualization\n",
        "    plt.subplot(1, 3, 3)\n",
        "    scatter = plt.scatter(codes_2d_cluster[:, 0], codes_2d_cluster[:, 1],\n",
        "                         c=cluster_labels, alpha=0.6, cmap='tab10', s=10)\n",
        "    plt.title('K-means Clustering')\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'comprehensive_codebook_analysis.png'),\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Comprehensive codebook analysis saved to {save_dir}\")\n",
        "\n",
        "    return {\n",
        "        'codebook_stats': codebook_stats,\n",
        "        'clustering': {\n",
        "            'labels': cluster_labels,\n",
        "            'kmeans': kmeans,\n",
        "            'codes_2d': codes_2d_cluster\n",
        "        },\n",
        "        'tsne': codes_2d\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# Visualization Functions (Keep your existing ones but ensure they're correct)\n",
        "# ============================================================================\n",
        "def plot_training_history(history, save_path):\n",
        "    \"\"\"Plot training history\"\"\"\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "\n",
        "    # Plot losses\n",
        "    axes[0, 0].plot(history['recon_loss'], label='Reconstruction Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Reconstruction Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    axes[0, 1].plot(history['vq_loss'], label='VQ Loss', color='orange')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].set_title('VQ Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    axes[0, 2].plot(history['perplexity'], label='Perplexity', color='green')\n",
        "    axes[0, 2].set_xlabel('Epoch')\n",
        "    axes[0, 2].set_ylabel('Perplexity')\n",
        "    axes[0, 2].set_title('Codebook Perplexity')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True)\n",
        "\n",
        "    # Plot utilization rate if available\n",
        "    if 'utilization_rate' in history:\n",
        "        axes[0, 3].plot([x * 100 for x in history['utilization_rate']], label='Utilization Rate', color='purple')\n",
        "        axes[0, 3].set_xlabel('Epoch')\n",
        "        axes[0, 3].set_ylabel('Utilization (%)')\n",
        "        axes[0, 3].set_title('Codebook Utilization Rate')\n",
        "        axes[0, 3].legend()\n",
        "        axes[0, 3].grid(True)\n",
        "\n",
        "    # Plot metrics\n",
        "    axes[1, 0].plot(history['mse'], label='MSE', color='red')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('MSE')\n",
        "    axes[1, 0].set_title('Mean Squared Error')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "\n",
        "    axes[1, 1].plot(history['psnr'], label='PSNR', color='purple')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('PSNR (dB)')\n",
        "    axes[1, 1].set_title('Peak Signal-to-Noise Ratio')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "\n",
        "    axes[1, 2].plot(history['ssim'], label='SSIM', color='brown')\n",
        "    axes[1, 2].set_xlabel('Epoch')\n",
        "    axes[1, 2].set_ylabel('SSIM')\n",
        "    axes[1, 2].set_title('Structural Similarity Index')\n",
        "    axes[1, 2].legend()\n",
        "    axes[1, 2].grid(True)\n",
        "\n",
        "    # Hide unused subplot\n",
        "    axes[1, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Training history plot saved to {save_path}\")\n",
        "\n",
        "def plot_reconstructions(model, dataloader, device, save_path, num_images=8):\n",
        "    \"\"\"Plot original vs reconstructed images\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get a batch\n",
        "    images = next(iter(dataloader))[:num_images].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        reconstructions, _, _, _ = model(images)\n",
        "\n",
        "    # Denormalize\n",
        "    images = (images + 1) / 2\n",
        "    reconstructions = (reconstructions + 1) / 2\n",
        "\n",
        "    # Create grid\n",
        "    comparison = torch.cat([images, reconstructions])\n",
        "    grid = make_grid(comparison, nrow=num_images, padding=2)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(16, 4))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Top: Original, Bottom: Reconstructed')\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Reconstruction comparison saved to {save_path}\")\n",
        "\n",
        "def plot_generated_samples(samples, save_path, title=\"Generated Samples\"):\n",
        "    \"\"\"Plot generated emoji samples\"\"\"\n",
        "    # Denormalize\n",
        "    samples = (samples + 1) / 2\n",
        "\n",
        "    # Create grid\n",
        "    grid = make_grid(samples, nrow=8, padding=2)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Generated samples saved to {save_path}\")\n",
        "\n",
        "def visualize_codebook_usage(model, dataloader, device, save_path):\n",
        "    \"\"\"Analyze and visualize codebook usage\"\"\"\n",
        "    model.eval()\n",
        "    all_codes = []\n",
        "\n",
        "    print(\"Analyzing codebook usage...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = batch.to(device)\n",
        "            codes = model.encode(batch)\n",
        "            all_codes.append(codes.cpu())\n",
        "\n",
        "    all_codes = torch.cat(all_codes, dim=0).numpy().flatten()\n",
        "\n",
        "    # Plot histogram\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Histogram\n",
        "    axes[0].hist(all_codes, bins=config.num_embeddings, edgecolor='black')\n",
        "    axes[0].set_xlabel('Code Index')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].set_title('Codebook Usage Distribution')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Usage statistics\n",
        "    unique_codes = len(np.unique(all_codes))\n",
        "    usage_rate = unique_codes / config.num_embeddings * 100\n",
        "\n",
        "    code_counts = np.bincount(all_codes, minlength=config.num_embeddings)\n",
        "    axes[1].bar(range(len(code_counts)), sorted(code_counts, reverse=True))\n",
        "    axes[1].set_xlabel('Code Rank')\n",
        "    axes[1].set_ylabel('Usage Count')\n",
        "    axes[1].set_title(f'Sorted Code Usage (Active: {unique_codes}/{config.num_embeddings} = {usage_rate:.1f}%)')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Codebook usage plot saved to {save_path}\")\n",
        "\n",
        "    return {\n",
        "        'total_codes': config.num_embeddings,\n",
        "        'active_codes': unique_codes,\n",
        "        'usage_rate': usage_rate,\n",
        "        'code_counts': code_counts\n",
        "    }\n",
        "\n",
        "def visualize_latent_space(model, dataloader, device, save_path):\n",
        "    \"\"\"Visualize latent space using t-SNE\"\"\"\n",
        "    model.eval()\n",
        "    all_latents = []\n",
        "    all_images = []\n",
        "\n",
        "    print(\"Extracting latent representations...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = batch.to(device)\n",
        "            z = model.encoder(batch)\n",
        "            z = model.pre_vq_conv(z)\n",
        "            # Pool spatial dimensions\n",
        "            z_pooled = z.mean(dim=[2, 3])\n",
        "            all_latents.append(z_pooled.cpu())\n",
        "            all_images.append(batch.cpu())\n",
        "\n",
        "    latents = torch.cat(all_latents, dim=0).numpy()\n",
        "    images = torch.cat(all_images, dim=0)\n",
        "\n",
        "    # Apply t-SNE\n",
        "    print(\"Applying t-SNE...\")\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "    latents_2d = tsne.fit_transform(latents[:1000])  # Use subset for speed\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "    scatter = ax.scatter(latents_2d[:, 0], latents_2d[:, 1],\n",
        "                        c=range(len(latents_2d)), cmap='viridis',\n",
        "                        alpha=0.6, s=20)\n",
        "    ax.set_title('t-SNE Visualization of Latent Space')\n",
        "    ax.set_xlabel('t-SNE Component 1')\n",
        "    ax.set_ylabel('t-SNE Component 2')\n",
        "    plt.colorbar(scatter, ax=ax, label='Sample Index')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Latent space visualization saved to {save_path}\")\n",
        "\n",
        "    return latents_2d\n",
        "\n",
        "def visualize_latent_interpolation(model, dataloader, device, save_path, num_steps=10):\n",
        "    \"\"\"Interpolate between two emojis in latent space\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get two random images\n",
        "    images = next(iter(dataloader))[:2].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode to latent\n",
        "        z1 = model.encoder(images[0:1])\n",
        "        z1 = model.pre_vq_conv(z1)\n",
        "\n",
        "        z2 = model.encoder(images[1:2])\n",
        "        z2 = model.pre_vq_conv(z2)\n",
        "\n",
        "        # Interpolate\n",
        "        interpolations = []\n",
        "        for alpha in np.linspace(0, 1, num_steps):\n",
        "            z_interp = (1 - alpha) * z1 + alpha * z2\n",
        "\n",
        "            # Quantize\n",
        "            quantized, _, _, _ = model.vq(z_interp)\n",
        "\n",
        "            # Decode\n",
        "            recon = model.decoder(quantized)\n",
        "            interpolations.append(recon)\n",
        "\n",
        "        interpolations = torch.cat(interpolations, dim=0)\n",
        "\n",
        "    # Denormalize\n",
        "    interpolations = (interpolations + 1) / 2\n",
        "    images = (images + 1) / 2\n",
        "\n",
        "    # Create grid\n",
        "    grid = make_grid(interpolations, nrow=num_steps, padding=2)\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(15, 6))\n",
        "\n",
        "    # Show start and end\n",
        "    axes[0].imshow(make_grid(images, nrow=2, padding=2).permute(1, 2, 0).cpu().numpy())\n",
        "    axes[0].set_title('Start and End Emojis')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Show interpolation\n",
        "    axes[1].imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "    axes[1].set_title(f'Latent Space Interpolation ({num_steps} steps)')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Latent interpolation saved to {save_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Prior Trainer (Keep your existing PriorTrainer)\n",
        "# ============================================================================\n",
        "class PriorTrainer:\n",
        "    \"\"\"Trainer for PixelCNN prior with stability improvements\"\"\"\n",
        "\n",
        "    def __init__(self, prior, vqvae, config):\n",
        "        self.prior = prior\n",
        "        self.vqvae = vqvae\n",
        "        self.config = config\n",
        "        self.optimizer = torch.optim.Adam(prior.parameters(), lr=config.learning_rate_prior)\n",
        "        self.history = {'loss': []}\n",
        "        self.start_epoch = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch, filepath):\n",
        "        \"\"\"Save training checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.prior.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'history': self.history\n",
        "        }\n",
        "        torch.save(checkpoint, filepath)\n",
        "        print(f\"Checkpoint saved: {filepath}\")\n",
        "\n",
        "    def load_checkpoint(self, filepath):\n",
        "        \"\"\"Load training checkpoint\"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            try:\n",
        "                checkpoint = torch.load(filepath, map_location=self.config.device, weights_only=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Safe loading failed: {e}\")\n",
        "                print(\"Attempting unsafe load...\")\n",
        "                checkpoint = torch.load(filepath, map_location=self.config.device, weights_only=False)\n",
        "\n",
        "            self.prior.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.history = checkpoint['history']\n",
        "            self.start_epoch = checkpoint['epoch'] + 1\n",
        "            print(f\"Checkpoint loaded: {filepath}, resuming from epoch {self.start_epoch}\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def train_epoch(self, dataloader):\n",
        "        \"\"\"Train for one epoch with gradient clipping\"\"\"\n",
        "        self.prior.train()\n",
        "        self.vqvae.eval()\n",
        "        epoch_loss = 0\n",
        "        num_valid_batches = 0\n",
        "\n",
        "        pbar = tqdm(dataloader, desc=\"Training Prior\")\n",
        "        for batch in pbar:\n",
        "            batch = batch.to(self.config.device)\n",
        "\n",
        "            # Get discrete codes\n",
        "            with torch.no_grad():\n",
        "                codes = self.vqvae.encode(batch)\n",
        "                spatial_size = int(np.sqrt(codes.shape[1]))\n",
        "                codes = codes.view(-1, spatial_size, spatial_size)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = self.prior(codes)\n",
        "            loss = F.cross_entropy(logits, codes)\n",
        "\n",
        "            # Check for NaN\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"Warning: NaN loss detected, skipping batch\")\n",
        "                continue\n",
        "\n",
        "            # Backward pass with gradient clipping\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.prior.parameters(), self.config.grad_clip)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            num_valid_batches += 1\n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        if num_valid_batches == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        return epoch_loss / num_valid_batches\n",
        "\n",
        "    def train(self, train_loader):\n",
        "        \"\"\"Full training loop\"\"\"\n",
        "        print(f\"Starting prior training from epoch {self.start_epoch}\")\n",
        "\n",
        "        for epoch in range(self.start_epoch, self.config.num_epochs_prior):\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs_prior}\")\n",
        "\n",
        "            loss = self.train_epoch(train_loader)\n",
        "\n",
        "            if loss == float('inf'):\n",
        "                print(\"Training failed due to NaN losses. Stopping.\")\n",
        "                break\n",
        "\n",
        "            self.history['loss'].append(loss)\n",
        "\n",
        "            print(f\"Loss: {loss:.4f}\")\n",
        "\n",
        "            # Save checkpoint every 5 epochs\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                checkpoint_path = os.path.join(\n",
        "                    self.config.checkpoint_dir,\n",
        "                    f'prior_epoch{epoch+1}.pt'\n",
        "                )\n",
        "                self.save_checkpoint(epoch, checkpoint_path)\n",
        "\n",
        "        # Save final model\n",
        "        final_path = os.path.join(self.config.checkpoint_dir, 'prior_final.pt')\n",
        "        self.save_checkpoint(self.config.num_epochs_prior - 1, final_path)\n",
        "\n",
        "        return self.history\n",
        "\n",
        "# ============================================================================\n",
        "# Main Training and Evaluation Pipeline - ENHANCED\n",
        "# ============================================================================\n",
        "def main():\n",
        "    \"\"\"Enhanced main function with two-phase training\"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"VQ-VAE Emoji Generation Project - ENHANCED VERSION\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 1: Download and prepare dataset\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 1: Dataset Preparation\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if not os.listdir(config.data_dir):\n",
        "        download_emoji_dataset()\n",
        "    else:\n",
        "        print(f\"Dataset already exists in {config.data_dir}\")\n",
        "\n",
        "    # Create dataset and dataloaders\n",
        "    dataset = EmojiDataset(config.data_dir, config.image_size)\n",
        "    print(f\"Total dataset size: {len(dataset)} images\")\n",
        "\n",
        "    # Split into train and validation\n",
        "    train_size = int(0.9 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=config.batch_size,\n",
        "        shuffle=True, num_workers=config.num_workers, pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=config.batch_size,\n",
        "        shuffle=False, num_workers=config.num_workers, pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {len(train_dataset)} images\")\n",
        "    print(f\"Validation set: {len(val_dataset)} images\")\n",
        "\n",
        "    # Step 2: Architecture Verification\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 2: Architecture Verification\")\n",
        "    print(\"=\" * 80)\n",
        "    model_test, expected_spatial_size = verify_architecture()\n",
        "    print(f\"\\nArchitecture verified! Latent spatial size: {expected_spatial_size}x{expected_spatial_size}\")\n",
        "    del model_test  # Free memory\n",
        "\n",
        "    # Step 3: Two-Phase VQ-VAE Training\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 3: Two-Phase VQ-VAE Training\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    vqvae = VQVAE(config, use_ema=True).to(config.device)\n",
        "    print(f\"VQ-VAE parameters: {sum(p.numel() for p in vqvae.parameters()):,}\")\n",
        "\n",
        "    trainer = Trainer(vqvae, config)\n",
        "\n",
        "    # Try to load checkpoint\n",
        "    checkpoint_path = os.path.join(config.checkpoint_dir, 'vqvae_final.pt')\n",
        "    if trainer.load_checkpoint(checkpoint_path):\n",
        "        print(\"Found existing checkpoint, checking codebook utilization...\")\n",
        "        utilization_rate, unique_codes = trainer.get_codebook_utilization(train_loader)\n",
        "        print(f\"Current codebook utilization: {unique_codes}/{config.num_embeddings} ({utilization_rate*100:.2f}%)\")\n",
        "\n",
        "        if utilization_rate >= config.codebook_utilization_threshold:\n",
        "            print(\"Codebook utilization already meets threshold! Proceeding to Phase 2 analysis.\")\n",
        "            phase1_complete_epoch = config.num_epochs_vqvae\n",
        "        else:\n",
        "            print(\"Codebook utilization below threshold. Starting Phase 1 training...\")\n",
        "            phase1_complete_epoch, phase1_metrics = trainer.train_phase1_codebook(\n",
        "                train_loader, val_loader, config.codebook_utilization_threshold\n",
        "            )\n",
        "    else:\n",
        "        print(\"No checkpoint found, starting Phase 1 training from scratch...\")\n",
        "        phase1_complete_epoch, phase1_metrics = trainer.train_phase1_codebook(\n",
        "            train_loader, val_loader, config.codebook_utilization_threshold\n",
        "        )\n",
        "\n",
        "    # Continue with Phase 2 if we haven't reached the end\n",
        "    if phase1_complete_epoch < config.num_epochs_vqvae:\n",
        "        history = trainer.train_phase2_final(train_loader, val_loader, phase1_complete_epoch)\n",
        "    else:\n",
        "        history = trainer.history\n",
        "        print(\"VQ-VAE training complete!\")\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(\n",
        "        history,\n",
        "        os.path.join(config.results_dir, 'training_history.png')\n",
        "    )\n",
        "\n",
        "    # Step 4: Enhanced VQ-VAE Analysis\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 4: Enhanced VQ-VAE Analysis\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Plot reconstructions\n",
        "    plot_reconstructions(\n",
        "        vqvae, val_loader, config.device,\n",
        "        os.path.join(config.results_dir, 'reconstructions.png')\n",
        "    )\n",
        "\n",
        "    # Enhanced codebook analysis (combining both approaches)\n",
        "    codebook_analysis_results = enhanced_codebook_analysis(\n",
        "        vqvae, train_loader, config.device,\n",
        "        os.path.join(config.results_dir, 'codebook_analysis')\n",
        "    )\n",
        "\n",
        "    # Visualize latent space\n",
        "    visualize_latent_space(\n",
        "        vqvae, train_loader, config.device,\n",
        "        os.path.join(config.results_dir, 'latent_space_tsne.png')\n",
        "    )\n",
        "\n",
        "    # Latent interpolation\n",
        "    visualize_latent_interpolation(\n",
        "        vqvae, val_loader, config.device,\n",
        "        os.path.join(config.results_dir, 'latent_interpolation.png'),\n",
        "        num_steps=config.num_interpolation_steps\n",
        "    )\n",
        "\n",
        "    # Step 5: Train Prior\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 5: Prior Training (PixelCNN)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Calculate spatial size\n",
        "    sample_batch = next(iter(train_loader))[:1].to(config.device)\n",
        "    with torch.no_grad():\n",
        "        codes = vqvae.encode(sample_batch)\n",
        "        spatial_size = int(np.sqrt(codes.shape[1]))\n",
        "    print(f\"Latent spatial size: {spatial_size}x{spatial_size}\")\n",
        "\n",
        "    prior = PixelCNN(\n",
        "        config.num_embeddings,\n",
        "        config.pixelcnn_layers,\n",
        "        config.pixelcnn_hidden\n",
        "    ).to(config.device)\n",
        "    print(f\"PixelCNN parameters: {sum(p.numel() for p in prior.parameters()):,}\")\n",
        "\n",
        "    prior_trainer = PriorTrainer(prior, vqvae, config)\n",
        "\n",
        "    # Try to load checkpoint\n",
        "    prior_checkpoint_path = os.path.join(config.checkpoint_dir, 'prior_final.pt')\n",
        "    if not prior_trainer.load_checkpoint(prior_checkpoint_path):\n",
        "        print(\"No prior checkpoint found, training from scratch...\")\n",
        "\n",
        "    # Train\n",
        "    if prior_trainer.start_epoch < config.num_epochs_prior:\n",
        "        prior_history = prior_trainer.train(train_loader)\n",
        "\n",
        "        # Plot prior training history\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(prior_history['loss'], label='Cross-Entropy Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Prior Training History')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(\n",
        "            os.path.join(config.results_dir, 'prior_training_history.png'),\n",
        "            dpi=300, bbox_inches='tight'\n",
        "        )\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(\"Prior training already complete!\")\n",
        "\n",
        "    # Step 6: Generate Novel Emojis\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 6: Novel Emoji Generation\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"Generating {config.num_samples} novel emojis...\")\n",
        "    prior.eval()\n",
        "    vqvae.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Sample from prior\n",
        "        sampled_codes = prior.sample(\n",
        "            config.num_samples, spatial_size, config.device, temperature=1.0\n",
        "        )\n",
        "\n",
        "        # Decode to images\n",
        "        generated_images = vqvae.decode_codes(sampled_codes, spatial_size)\n",
        "\n",
        "    # Plot generated samples\n",
        "    plot_generated_samples(\n",
        "        generated_images,\n",
        "        os.path.join(config.results_dir, 'generated_emojis.png'),\n",
        "        title=\"Generated Novel Emojis\"\n",
        "    )\n",
        "\n",
        "    # Step 7: Calculate FID\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 7: Calculating FID Score\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Extracting features from real images...\")\n",
        "    real_images = []\n",
        "    for batch in tqdm(val_loader):\n",
        "        real_images.append(batch)\n",
        "        if len(real_images) * config.batch_size >= config.num_samples:\n",
        "            break\n",
        "    real_images = torch.cat(real_images, dim=0)[:config.num_samples].to(config.device)\n",
        "\n",
        "    real_features, inception_model = get_inception_features(real_images, None, config.device)\n",
        "    print(\"Extracting features from generated images...\")\n",
        "    fake_features, _ = get_inception_features(generated_images, inception_model, config.device)\n",
        "\n",
        "    fid_score = calculate_fid(real_features, fake_features)\n",
        "    print(f\"\\nFID Score: {fid_score:.2f}\")\n",
        "\n",
        "    # Step 8: Generate Comprehensive Report\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 8: Generating Final Report\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Get final utilization rate\n",
        "    final_utilization_rate, final_unique_codes = trainer.get_codebook_utilization(train_loader)\n",
        "\n",
        "    report = {\n",
        "        'dataset_size': len(dataset),\n",
        "        'train_size': len(train_dataset),\n",
        "        'val_size': len(val_dataset),\n",
        "        'vqvae_params': sum(p.numel() for p in vqvae.parameters()),\n",
        "        'prior_params': sum(p.numel() for p in prior.parameters()),\n",
        "        'final_metrics': {\n",
        "            'mse': history['mse'][-1] if history['mse'] else None,\n",
        "            'psnr': history['psnr'][-1] if history['psnr'] else None,\n",
        "            'ssim': history['ssim'][-1] if history['ssim'] else None,\n",
        "            'fid': fid_score,\n",
        "            'perplexity': history['perplexity'][-1] if history['perplexity'] else None\n",
        "        },\n",
        "        'codebook_stats': {\n",
        "            'total_codes': config.num_embeddings,\n",
        "            'active_codes': int(final_unique_codes),\n",
        "            'usage_rate': float(final_utilization_rate * 100),\n",
        "            'target_utilization': config.codebook_utilization_threshold * 100,\n",
        "            'utilization_achieved': final_utilization_rate >= config.codebook_utilization_threshold\n",
        "        },\n",
        "        'training_info': {\n",
        "            'phase1_completed_at_epoch': phase1_complete_epoch,\n",
        "            'total_epochs_trained': len(history['recon_loss']) if history['recon_loss'] else 0,\n",
        "            'final_recon_loss': history['recon_loss'][-1] if history['recon_loss'] else None,\n",
        "            'final_vq_loss': history['vq_loss'][-1] if history['vq_loss'] else None\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save report\n",
        "    report_path = os.path.join(config.results_dir, 'final_report.json')\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nReconstruction Quality:\")\n",
        "    print(f\"  MSE: {report['final_metrics']['mse']:.4f}\")\n",
        "    print(f\"  PSNR: {report['final_metrics']['psnr']:.2f} dB\")\n",
        "    print(f\"  SSIM: {report['final_metrics']['ssim']:.4f}\")\n",
        "    print(f\"\\nGeneration Quality:\")\n",
        "    print(f\"  FID: {report['final_metrics']['fid']:.2f}\")\n",
        "    print(f\"\\nCodebook Analysis:\")\n",
        "    print(f\"  Perplexity: {report['final_metrics']['perplexity']:.2f}\")\n",
        "    print(f\"  Active codes: {report['codebook_stats']['active_codes']}/{report['codebook_stats']['total_codes']}\")\n",
        "    print(f\"  Usage rate: {report['codebook_stats']['usage_rate']:.2f}%\")\n",
        "    print(f\"  Target utilization: {report['codebook_stats']['target_utilization']}%\")\n",
        "    print(f\"  Utilization achieved: {'âœ… YES' if report['codebook_stats']['utilization_achieved'] else 'âŒ NO'}\")\n",
        "    print(f\"\\nTraining Info:\")\n",
        "    print(f\"  Phase 1 completed at epoch: {report['training_info']['phase1_completed_at_epoch']}\")\n",
        "    print(f\"  Total epochs trained: {report['training_info']['total_epochs_trained']}\")\n",
        "    print(f\"\\nAll results saved to: {config.results_dir}\")\n",
        "    print(f\"Report saved to: {report_path}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PROJECT COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# Run the complete pipeline\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTn2sbiXMpJu",
        "outputId": "3a43a451-21fd-47ef-c94b-0ee928d79ee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mresults\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMMLRpPJNKin"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Display all PNG images\n",
        "results_dir = '/content/results/'\n",
        "image_files = [f for f in os.listdir(results_dir) if f.endswith('.png')]\n",
        "\n",
        "print(\"Displaying saved plots:\")\n",
        "for img_file in image_files:\n",
        "    img_path = os.path.join(results_dir, img_file)\n",
        "    img = mpimg.imread(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.title(img_file)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Print the contents of experiment_results.csv\n",
        "csv_path = os.path.join(results_dir, 'experiment_results.csv')\n",
        "\n",
        "if os.path.exists(csv_path):\n",
        "    print(\"\\nContents of experiment_results.csv:\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    for index, row in df.iterrows():\n",
        "        print(','.join(map(str, row.tolist())))\n",
        "else:\n",
        "    print(f\"\\n{csv_path} not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d07ca2e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "# Define source and destination paths\n",
        "source_checkpoints = '/content/checkpoints'\n",
        "source_results = '/content/results'\n",
        "destination_base = '/content/drive/MyDrive'\n",
        "destination_folder_name = config.experiment_name\n",
        "destination_path = os.path.join(destination_base, destination_folder_name)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "print(f\"Creating destination folder: {destination_path}\")\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "print(f\"Destination folder created or already exists.\")\n",
        "\n",
        "# Transfer checkpoints folder\n",
        "destination_checkpoints = os.path.join(destination_path, os.path.basename(source_checkpoints))\n",
        "print(f\"\\nTransferring checkpoints from {source_checkpoints} to {destination_checkpoints}...\")\n",
        "if os.path.exists(source_checkpoints):\n",
        "    if os.path.exists(destination_checkpoints):\n",
        "        print(f\"Removing existing destination checkpoints folder: {destination_checkpoints}\")\n",
        "        shutil.rmtree(destination_checkpoints)\n",
        "        print(f\"Removed existing destination checkpoints folder: {destination_checkpoints}\")\n",
        "    try:\n",
        "        shutil.copytree(source_checkpoints, destination_checkpoints)\n",
        "        print(\"Checkpoints transfer complete.\")\n",
        "        # Verify contents\n",
        "        source_files = os.listdir(source_checkpoints)\n",
        "        destination_files = os.listdir(destination_checkpoints)\n",
        "        print(f\"Source files in checkpoints: {source_files}\")\n",
        "        print(f\"Destination files in checkpoints: {destination_files}\")\n",
        "        if set(source_files) == set(destination_files) and len(source_files) > 0:\n",
        "            print(\"Checkpoints contents successfully verified.\")\n",
        "        elif len(source_files) == 0:\n",
        "             print(\"Source checkpoints folder is empty.\")\n",
        "        else:\n",
        "             print(\"Warning: Checkpoints contents may not have been fully copied.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during checkpoints transfer: {e}\")\n",
        "else:\n",
        "    print(f\"Source checkpoints folder not found: {source_checkpoints}\")\n",
        "\n",
        "\n",
        "# Transfer results folder\n",
        "destination_results = os.path.join(destination_path, os.path.basename(source_results))\n",
        "print(f\"\\nTransferring results from {source_results} to {destination_results}...\")\n",
        "if os.path.exists(source_results):\n",
        "    if os.path.exists(destination_results):\n",
        "        print(f\"Removing existing destination results folder: {destination_results}\")\n",
        "        shutil.rmtree(destination_results)\n",
        "        print(f\"Removed existing destination results folder: {destination_results}\")\n",
        "    try:\n",
        "        shutil.copytree(source_results, destination_results)\n",
        "        print(\"Results transfer complete.\")\n",
        "        # Verify contents\n",
        "        source_files = os.listdir(source_results)\n",
        "        destination_files = os.listdir(destination_results)\n",
        "        print(f\"Source files in results: {source_files}\")\n",
        "        print(f\"Destination files in results: {destination_files}\")\n",
        "        if set(source_files) == set(destination_files) and len(source_files) > 0:\n",
        "            print(\"Results contents successfully verified.\")\n",
        "        elif len(source_files) == 0:\n",
        "             print(\"Source results folder is empty.\")\n",
        "        else:\n",
        "             print(\"Warning: Results contents may not have been fully copied.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during results transfer: {e}\")\n",
        "else:\n",
        "    print(f\"Source results folder not found: {source_results}\")\n",
        "\n",
        "print(\"\\nTransfer process finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eni0xc9lROGF"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "posit_nn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05fbc5784b7c4ac2aceb0117e73bec1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "132f300edfcf448cbf3f0d4457f0eb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2027ce8946bb4d7c995dd2b9e9c05b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c27457a3aba64afa9a09541740f31dbd",
            "max": 2749,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_132f300edfcf448cbf3f0d4457f0eb15",
            "value": 2749
          }
        },
        "208833f5c22b4298a7070363cfb3eeb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c2ecd1799d9460f898f4ba34f905d70",
              "IPY_MODEL_2027ce8946bb4d7c995dd2b9e9c05b46",
              "IPY_MODEL_a0d8542fdecd4eb79ab74ce6bdecff6f"
            ],
            "layout": "IPY_MODEL_5a3d220ca4984ba087317a19d2a0ffb1"
          }
        },
        "2a68a9c162504d079433b14e3a2c9486": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fb221bdfd1f4c459ce7c6d5c2bada65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353f4c230fcd4491bbed14a7042fcfc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05fbc5784b7c4ac2aceb0117e73bec1d",
            "max": 138595810,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca3f9c49a9ec40678202491e6a9b444d",
            "value": 138595810
          }
        },
        "3a89491b9cc7431899758b2ce285aab4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a6f3f5994e947d78e24d047ac97b095": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5674198464e846e4b9135c5d31b36d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "576b7be713b248d8af68dbb257f6ac07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7760bc39170e4a578724d1a9766406b1",
              "IPY_MODEL_7147be28ef8a4feca9a1f36162506804",
              "IPY_MODEL_86a1f0e9f6f64425a69a60ae949822c7"
            ],
            "layout": "IPY_MODEL_b3ad216bc9174627b39b224d08f201f8"
          }
        },
        "5a3d220ca4984ba087317a19d2a0ffb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eac55ad9f4543feaeece364932efe2c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c2ecd1799d9460f898f4ba34f905d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb50bed2b3f248cead2ef1282c647b0f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d43d85bea2f34e4da080003146f092e6",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "6e5df2d2a61e4a71a49cf97a45d2b7b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7147be28ef8a4feca9a1f36162506804": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a89491b9cc7431899758b2ce285aab4",
            "max": 720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c5ccbfb353342eea79f25585522f439",
            "value": 720
          }
        },
        "7760bc39170e4a578724d1a9766406b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a68a9c162504d079433b14e3a2c9486",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d7cde14fe3bd42229a49826000ccc342",
            "value": "dataset_infos.json:â€‡100%"
          }
        },
        "7db862cdfbc44ec982c4c203ffa3c9f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86a1f0e9f6f64425a69a60ae949822c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a11ed5b605e54b8fa80718c491ce98af",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dba10b82b9424edfbd650294726440bb",
            "value": "â€‡720/720â€‡[00:00&lt;00:00,â€‡36.1kB/s]"
          }
        },
        "8ef33255cd59430296314c33959134bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c5ccbfb353342eea79f25585522f439": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0d8542fdecd4eb79ab74ce6bdecff6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7db862cdfbc44ec982c4c203ffa3c9f0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5674198464e846e4b9135c5d31b36d9e",
            "value": "â€‡2749/2749â€‡[00:00&lt;00:00,â€‡3644.31â€‡examples/s]"
          }
        },
        "a11ed5b605e54b8fa80718c491ce98af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3d263cd9b8646aea1c258cfa57f8ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7c6723b741a4202a49d3e54c68a2373",
              "IPY_MODEL_353f4c230fcd4491bbed14a7042fcfc2",
              "IPY_MODEL_dc5df4a030d542f8b91ed37006357395"
            ],
            "layout": "IPY_MODEL_2fb221bdfd1f4c459ce7c6d5c2bada65"
          }
        },
        "b3ad216bc9174627b39b224d08f201f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c6723b741a4202a49d3e54c68a2373": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a6f3f5994e947d78e24d047ac97b095",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8ef33255cd59430296314c33959134bc",
            "value": "data/train-00000-of-00001-38cc4fa96c139e(â€¦):â€‡100%"
          }
        },
        "bb50bed2b3f248cead2ef1282c647b0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27457a3aba64afa9a09541740f31dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca3f9c49a9ec40678202491e6a9b444d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d43d85bea2f34e4da080003146f092e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7cde14fe3bd42229a49826000ccc342": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dba10b82b9424edfbd650294726440bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc5df4a030d542f8b91ed37006357395": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5eac55ad9f4543feaeece364932efe2c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6e5df2d2a61e4a71a49cf97a45d2b7b0",
            "value": "â€‡139M/139Mâ€‡[00:03&lt;00:00,â€‡74.6MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

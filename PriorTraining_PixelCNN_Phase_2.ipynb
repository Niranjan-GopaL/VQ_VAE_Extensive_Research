{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc89c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VQ-VAE Emoji Generation - Phase 2: Prior Training, Generation & Analysis\n",
    "Compatible with Phase 1's config system and checkpoints\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import linalg\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# Phase 2 Configuration (Extends Phase 1)\n",
    "# ============================================================================\n",
    "\n",
    "EXPERIMENT_CONFIGS_PHASE2 = {\n",
    "    # Prior (PixelCNN)\n",
    "    \"num_epochs_prior\": 50,\n",
    "    \"learning_rate_prior\": 1e-4,\n",
    "    \"pixelcnn_layers\": 12,\n",
    "    \"pixelcnn_hidden\": 64,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \n",
    "    # Generation\n",
    "    \"num_samples\": 64,\n",
    "    \"num_interpolation_steps\": 10,\n",
    "    \"temperature\": 1.0,\n",
    "    \n",
    "    # Experiment metadata for Phase 2\n",
    "    \"phase2_experiment_name\": \"prior_baseline\",\n",
    "    \"phase2_notes\": \"PixelCNN autoregressive prior training\"\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration Management\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class that loads from JSON dict\"\"\"\n",
    "    def __init__(self, config_dict):\n",
    "        # Define expected types for automatic conversion\n",
    "        self.type_mappings = {\n",
    "            'image_size': int,\n",
    "            'batch_size': int,\n",
    "            'num_workers': int,\n",
    "            'num_hiddens': int,\n",
    "            'num_residual_hiddens': int,\n",
    "            'num_residual_layers': int,\n",
    "            'embedding_dim': int,\n",
    "            'num_embeddings': int,\n",
    "            'commitment_cost': float,\n",
    "            'decay': float,\n",
    "            'num_epochs_vqvae': int,\n",
    "            'learning_rate_vqvae': float,\n",
    "            'min_codebook_usage': float,\n",
    "            'check_usage_every': int,\n",
    "            'num_epochs_prior': int,\n",
    "            'learning_rate_prior': float,\n",
    "            'pixelcnn_layers': int,\n",
    "            'pixelcnn_hidden': int,\n",
    "            'grad_clip': float,\n",
    "            'num_samples': int,\n",
    "            'num_interpolation_steps': int,\n",
    "            'temperature': float,\n",
    "        }\n",
    "        \n",
    "        for key, value in config_dict.items():\n",
    "            # Convert to proper type if needed\n",
    "            if key in self.type_mappings and isinstance(value, str):\n",
    "                try:\n",
    "                    value = self.type_mappings[key](value)\n",
    "                except (ValueError, TypeError):\n",
    "                    pass  # Keep as string if conversion fails\n",
    "            setattr(self, key, value)\n",
    "        \n",
    "        # Add device (not in JSON as it's system-dependent)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert config back to dictionary for CSV export\"\"\"\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith('_') and k != 'type_mappings'}\n",
    "\n",
    "# ============================================================================\n",
    "# Dataset (Same as Phase 1)\n",
    "# ============================================================================\n",
    "\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_size=64, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_size = image_size\n",
    "        self.image_files = [f for f in os.listdir(data_dir)\n",
    "                           if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(image)\n",
    "\n",
    "# ============================================================================\n",
    "# VQ-VAE Components (Must match Phase 1 exactly)\n",
    "# ============================================================================\n",
    "\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay=0.99, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        embed = torch.randn(num_embeddings, embedding_dim)\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(num_embeddings))\n",
    "        self.register_buffer(\"embed_avg\", embed.clone())\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        input_shape = inputs.shape\n",
    "        flat_input = inputs.permute(0, 2, 3, 1).contiguous().view(-1, self.embedding_dim)\n",
    "        \n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
    "                    + torch.sum(self.embed**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self.embed.t()))\n",
    "        \n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        quantized = torch.matmul(encodings, self.embed)\n",
    "        \n",
    "        if self.training:\n",
    "            self.cluster_size.data.mul_(self.decay).add_(encodings.sum(0), alpha=1 - self.decay)\n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self.embed_avg.data.mul_(self.decay).add_(dw, alpha=1 - self.decay)\n",
    "            \n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = ((self.cluster_size + self.epsilon) / (n + self.num_embeddings * self.epsilon) * n)\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "        \n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), flat_input)\n",
    "        loss = self.commitment_cost * e_latent_loss\n",
    "        quantized = flat_input + (quantized - flat_input).detach()\n",
    "        \n",
    "        quantized = quantized.view(input_shape[0], input_shape[2], input_shape[3], self.embedding_dim)\n",
    "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        return quantized, loss, perplexity, encoding_indices\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, num_residual_hiddens, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_residual_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_residual_hiddens, num_hiddens, 1, bias=False),\n",
    "            nn.BatchNorm2d(num_hiddens)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            ResidualBlock(in_channels, num_hiddens, num_residual_hiddens)\n",
    "            for _ in range(num_residual_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return F.relu(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, num_hiddens // 2, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(num_hiddens // 2, num_hiddens, 4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(num_hiddens, num_hiddens, 3, padding=1)\n",
    "        self.residual_stack = ResidualStack(num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return self.residual_stack(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(embedding_dim, num_hiddens, 3, padding=1)\n",
    "        self.residual_stack = ResidualStack(num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "        self.conv_trans1 = nn.ConvTranspose2d(num_hiddens, num_hiddens // 2, 4, stride=2, padding=1)\n",
    "        self.conv_trans2 = nn.ConvTranspose2d(num_hiddens // 2, 3, 4, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.residual_stack(x)\n",
    "        x = F.relu(self.conv_trans1(x))\n",
    "        return torch.tanh(self.conv_trans2(x))\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(3, config.num_hiddens, config.num_residual_layers, config.num_residual_hiddens)\n",
    "        self.pre_vq_conv = nn.Conv2d(config.num_hiddens, config.embedding_dim, 1)\n",
    "        self.vq = VectorQuantizerEMA(config.num_embeddings, config.embedding_dim, config.commitment_cost, decay=config.decay)\n",
    "        self.decoder = Decoder(config.embedding_dim, config.num_hiddens, config.num_residual_layers, config.num_residual_hiddens)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = self.pre_vq_conv(z)\n",
    "        quantized, vq_loss, perplexity, encoding_indices = self.vq(z)\n",
    "        x_recon = self.decoder(quantized)\n",
    "        return x_recon, vq_loss, perplexity, encoding_indices\n",
    "    \n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = self.pre_vq_conv(z)\n",
    "        _, _, _, encoding_indices = self.vq(z)\n",
    "        B = x.shape[0]\n",
    "        return encoding_indices.view(B, -1)\n",
    "\n",
    "# ============================================================================\n",
    "# PixelCNN Prior\n",
    "# ============================================================================\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.register_buffer('mask', torch.zeros_like(self.weight))\n",
    "        self.create_mask(mask_type)\n",
    "    \n",
    "    def create_mask(self, mask_type):\n",
    "        k = self.kernel_size[0]\n",
    "        self.mask[:, :, :k//2, :] = 1\n",
    "        self.mask[:, :, k//2, :k//2] = 1\n",
    "        if mask_type == 'B':\n",
    "            self.mask[:, :, k//2, k//2] = 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super().forward(x)\n",
    "\n",
    "class PixelCNNResidualBlock(nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            MaskedConv2d('B', h, h, 1),\n",
    "            nn.BatchNorm2d(h),\n",
    "            nn.ReLU(),\n",
    "            MaskedConv2d('B', h, h, 1),\n",
    "            nn.BatchNorm2d(h)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.conv(x)\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, num_embeddings, num_layers=12, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.input_conv = MaskedConv2d('A', num_embeddings, hidden_dim, 7, padding=3)\n",
    "        self.residual_blocks = nn.ModuleList([PixelCNNResidualBlock(hidden_dim) for _ in range(num_layers)])\n",
    "        self.output = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            MaskedConv2d('B', hidden_dim, hidden_dim, 1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, num_embeddings, 1)\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, MaskedConv2d)):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_onehot = F.one_hot(x, self.num_embeddings).float()\n",
    "        x_onehot = x_onehot.permute(0, 3, 1, 2).contiguous()\n",
    "        x = self.input_conv(x_onehot)\n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "        return self.output(x)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size, spatial_size, device, temperature=1.0):\n",
    "        samples = torch.zeros(batch_size, spatial_size, spatial_size, dtype=torch.long, device=device)\n",
    "        for i in range(spatial_size):\n",
    "            for j in range(spatial_size):\n",
    "                logits = self(samples)\n",
    "                probs = F.softmax(logits[:, :, i, j] / temperature, dim=1)\n",
    "                samples[:, i, j] = torch.multinomial(probs, 1).squeeze(-1)\n",
    "        return samples\n",
    "\n",
    "# ============================================================================\n",
    "# Decoding Helper\n",
    "# ============================================================================\n",
    "\n",
    "def decode_codes(vqvae, encoding_indices, spatial_size):\n",
    "    \"\"\"Decode from discrete codes\"\"\"\n",
    "    codes = encoding_indices.view(-1, spatial_size, spatial_size)\n",
    "    quantized = F.embedding(codes, vqvae.vq.embed)\n",
    "    quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "    return vqvae.decoder(quantized)\n",
    "\n",
    "# ============================================================================\n",
    "# Prior Trainer\n",
    "# ============================================================================\n",
    "\n",
    "class PriorTrainer:\n",
    "    def __init__(self, prior, vqvae, config):\n",
    "        self.prior = prior\n",
    "        self.vqvae = vqvae\n",
    "        self.config = config\n",
    "        self.optimizer = torch.optim.Adam(prior.parameters(), lr=config.learning_rate_prior)\n",
    "        self.history = {'loss': [], 'epoch': []}\n",
    "        self.start_epoch = 0\n",
    "    \n",
    "    def save_checkpoint(self, epoch, filepath):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.prior.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'history': self.history\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"✓ Prior checkpoint saved: {filepath}\")\n",
    "    \n",
    "    def load_checkpoint(self, filepath):\n",
    "        if os.path.exists(filepath):\n",
    "            try:\n",
    "                checkpoint = torch.load(filepath, map_location=self.config.device, weights_only=False)\n",
    "                self.prior.load_state_dict(checkpoint['model_state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                self.history = checkpoint['history']\n",
    "                self.start_epoch = checkpoint['epoch'] + 1\n",
    "                print(f\"✓ Prior checkpoint loaded, resuming from epoch {self.start_epoch}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error loading prior checkpoint: {e}\")\n",
    "                return False\n",
    "        return False\n",
    "    \n",
    "    def train_epoch(self, dataloader, spatial_size):\n",
    "        self.prior.train()\n",
    "        self.vqvae.eval()\n",
    "        epoch_loss = 0\n",
    "        num_valid = 0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=\"Training Prior\")\n",
    "        for batch in pbar:\n",
    "            batch = batch.to(self.config.device)\n",
    "            \n",
    "            # Get codes from VQ-VAE\n",
    "            with torch.no_grad():\n",
    "                codes = self.vqvae.encode(batch)\n",
    "                codes = codes.view(-1, spatial_size, spatial_size)\n",
    "            \n",
    "            # Train PixelCNN\n",
    "            logits = self.prior(codes)\n",
    "            loss = F.cross_entropy(logits, codes)\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                print(\"Warning: NaN loss detected, skipping batch\")\n",
    "                continue\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.prior.parameters(), self.config.grad_clip)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_valid += 1\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        return epoch_loss / num_valid if num_valid > 0 else float('inf')\n",
    "    \n",
    "    def train(self, train_loader, spatial_size):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TRAINING PIXELCNN PRIOR\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        for epoch in range(self.start_epoch, self.config.num_epochs_prior):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs_prior}\")\n",
    "            \n",
    "            loss = self.train_epoch(train_loader, spatial_size)\n",
    "            \n",
    "            if loss == float('inf'):\n",
    "                print(\"Training stopped due to invalid loss\")\n",
    "                break\n",
    "            \n",
    "            self.history['loss'].append(loss)\n",
    "            self.history['epoch'].append(epoch + 1)\n",
    "            \n",
    "            print(f\"Loss: {loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoints\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self.save_checkpoint(epoch, os.path.join(self.config.checkpoint_dir, f'prior_epoch_{epoch+1}.pt'))\n",
    "        \n",
    "        # Save final checkpoint\n",
    "        self.save_checkpoint(self.config.num_epochs_prior - 1, os.path.join(self.config.checkpoint_dir, 'prior_final.pt'))\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "# ============================================================================\n",
    "# FID Calculation\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_fid(real_features, fake_features):\n",
    "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "    \n",
    "    ssdiff = np.sum((mu1 - mu2)**2)\n",
    "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    return ssdiff + np.trace(sigma1 + sigma2 - 2*covmean)\n",
    "\n",
    "def get_inception_features(images, model, device):\n",
    "    from torchvision.models import inception_v3\n",
    "    if model is None:\n",
    "        model = inception_v3(pretrained=True, transform_input=False)\n",
    "        model.fc = nn.Identity()\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Normalize from [-1, 1] to [0, 1] for Inception\n",
    "        images_norm = (images + 1) / 2\n",
    "        images_resized = F.interpolate(images_norm, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        features = model(images_resized)\n",
    "    \n",
    "    return features.cpu().numpy(), model\n",
    "\n",
    "# ============================================================================\n",
    "# Visualization Functions\n",
    "# ============================================================================\n",
    "\n",
    "def plot_generated_samples(samples, save_path, title=\"Generated Samples\"):\n",
    "    samples = (samples + 1) / 2\n",
    "    samples = torch.clamp(samples, 0, 1)\n",
    "    grid = make_grid(samples, nrow=8, padding=2)\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Saved: {save_path}\")\n",
    "\n",
    "def visualize_latent_interpolation(vqvae, dataloader, device, save_path, num_steps=10):\n",
    "    vqvae.eval()\n",
    "    real_batch = next(iter(dataloader))[:2].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        codes = vqvae.encode(real_batch)\n",
    "        spatial_size = int(np.sqrt(codes.shape[1]))\n",
    "        codes = codes.view(2, spatial_size, spatial_size)\n",
    "        \n",
    "        interpolations = []\n",
    "        for alpha in np.linspace(0, 1, num_steps):\n",
    "            interp_code = (1 - alpha) * codes[0:1] + alpha * codes[1:2]\n",
    "            interp_code = interp_code.long()\n",
    "            interp_img = decode_codes(vqvae, interp_code, spatial_size)\n",
    "            interpolations.append(interp_img)\n",
    "        \n",
    "        interpolation_grid = torch.cat(interpolations)\n",
    "    \n",
    "    plot_generated_samples(interpolation_grid, save_path, \"Latent Space Interpolation\")\n",
    "\n",
    "def visualize_tsne(vqvae, dataloader, device, save_path):\n",
    "    print(\"Creating t-SNE visualization...\")\n",
    "    vqvae.eval()\n",
    "    all_codes = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= 10:  # Limit samples for speed\n",
    "                break\n",
    "            codes = vqvae.encode(batch.to(device))\n",
    "            all_codes.append(codes.cpu())\n",
    "    \n",
    "    all_codes = torch.cat(all_codes, dim=0).numpy()\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    codes_2d = tsne.fit_transform(all_codes[:1000])\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(codes_2d[:, 0], codes_2d[:, 1], alpha=0.5, s=10)\n",
    "    plt.title('t-SNE Visualization of Latent Codes', fontsize=14)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Saved: {save_path}\")\n",
    "\n",
    "def visualize_clustering(vqvae, dataloader, device, save_path, n_clusters=10):\n",
    "    print(\"Performing clustering analysis...\")\n",
    "    vqvae.eval()\n",
    "    all_codes = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= 10:\n",
    "                break\n",
    "            codes = vqvae.encode(batch.to(device))\n",
    "            all_codes.append(codes.cpu())\n",
    "    \n",
    "    all_codes = torch.cat(all_codes, dim=0).numpy()\n",
    "    \n",
    "    # K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(all_codes[:1000])\n",
    "    \n",
    "    # t-SNE for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    codes_2d = tsne.fit_transform(all_codes[:1000])\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(codes_2d[:, 0], codes_2d[:, 1], c=cluster_labels, \n",
    "                         alpha=0.6, s=10, cmap='tab10')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.title(f'K-means Clustering (k={n_clusters}) of Latent Codes', fontsize=14)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Saved: {save_path}\")\n",
    "\n",
    "def plot_prior_training_history(history, save_path):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['epoch'], history['loss'], linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "    plt.title('PixelCNN Prior Training Loss', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Saved: {save_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CSV Export\n",
    "# ============================================================================\n",
    "\n",
    "def save_results_to_csv(config, results, save_path):\n",
    "    \"\"\"Save experiment config and results to CSV\"\"\"\n",
    "    row_data = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        **config.to_dict(),\n",
    "        **results\n",
    "    }\n",
    "    \n",
    "    file_exists = os.path.exists(save_path)\n",
    "    \n",
    "    with open(save_path, 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=row_data.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_data)\n",
    "    \n",
    "    print(f\"✓ Results appended to: {save_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Main Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*80)\n",
    "    print(\"PHASE 2: PRIOR TRAINING & COMPLETE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Load Phase 1 Results and Config\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Check if Phase 1 completed\n",
    "    phase1_summary_path = './results/phase1_summary.json'\n",
    "    if not os.path.exists(phase1_summary_path):\n",
    "        print(\"\\n✗ ERROR: Phase 1 not completed. Run phase 1 first!\")\n",
    "        print(\"  Expected file: ./results/phase1_summary.json\")\n",
    "        return\n",
    "    \n",
    "    with open(phase1_summary_path, 'r') as f:\n",
    "        phase1_summary = json.load(f)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PHASE 1 SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Codebook usage: {phase1_summary['codebook_usage_percent']:.2f}%\")\n",
    "    print(f\"Active codes: {phase1_summary['active_codes']}/{phase1_summary['total_codes']}\")\n",
    "    print(f\"Target achieved: {phase1_summary['target_achieved']}\")\n",
    "    \n",
    "    # Load Phase 1 config from CSV\n",
    "    csv_path = './results/experiment_results.csv'\n",
    "    phase1_config_dict = {}\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        with open(csv_path, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            rows = list(reader)\n",
    "            if rows:\n",
    "                phase1_config_dict = rows[-1]  # Get last experiment\n",
    "                print(f\"\\n✓ Loaded Phase 1 config: {phase1_config_dict.get('experiment_name', 'unknown')}\")\n",
    "    else:\n",
    "        print(\"\\n⚠ Warning: No CSV found, using default Phase 1 config\")\n",
    "    \n",
    "    # Warn if codebook usage is low\n",
    "    if not phase1_summary['target_achieved']:\n",
    "        print(f\"\\n⚠ WARNING: Phase 1 codebook usage was {phase1_summary['codebook_usage_percent']:.2f}%\")\n",
    "        min_usage = float(phase1_config_dict.get('min_codebook_usage', 50.0))\n",
    "        print(f\"  Target is {min_usage}%. Generation quality may be affected.\")\n",
    "        response = input(\"\\nContinue anyway? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            return\n",
    "    \n",
    "    # Merge configs (Phase 1 + Phase 2)\n",
    "    merged_config = {**phase1_config_dict, **EXPERIMENT_CONFIGS_PHASE2}\n",
    "    config = Config(merged_config)\n",
    "    \n",
    "    print(f\"\\n✓ Using device: {config.device}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Load Dataset\n",
    "    # ========================================================================\n",
    "    \n",
    "    dataset = EmojiDataset(config.data_dir, config.image_size)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, \n",
    "                             shuffle=True, num_workers=config.num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, \n",
    "                           shuffle=False, num_workers=config.num_workers, pin_memory=True)\n",
    "    \n",
    "    print(f\"\\n✓ Dataset: {len(train_dataset)} train, {len(val_dataset)} val\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Load Pre-trained VQ-VAE\n",
    "    # ========================================================================\n",
    "    \n",
    "    vqvae = VQVAE(config).to(config.device)\n",
    "    vqvae_checkpoint_path = os.path.join(config.checkpoint_dir, 'vqvae_final.pt')\n",
    "    \n",
    "    if not os.path.exists(vqvae_checkpoint_path):\n",
    "        print(f\"\\n✗ ERROR: VQ-VAE checkpoint not found!\")\n",
    "        print(f\"  Expected: {vqvae_checkpoint_path}\")\n",
    "        return\n",
    "    \n",
    "    vqvae_checkpoint = torch.load(vqvae_checkpoint_path, map_location=config.device, weights_only=False)\n",
    "    vqvae.load_state_dict(vqvae_checkpoint['model_state_dict'])\n",
    "    vqvae.eval()\n",
    "    \n",
    "    print(f\"✓ VQ-VAE loaded successfully from epoch {vqvae_checkpoint['epoch'] + 1}\")\n",
    "    \n",
    "    # Calculate spatial size\n",
    "    sample_batch = next(iter(train_loader))[:4].to(config.device)\n",
    "    with torch.no_grad():\n",
    "        sample_codes = vqvae.encode(sample_batch)\n",
    "    spatial_size = int(np.sqrt(sample_codes.shape[1]))\n",
    "    \n",
    "    print(f\"✓ Latent spatial size: {spatial_size}x{spatial_size}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Train PixelCNN Prior\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PIXELCNN PRIOR TRAINING\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    prior = PixelCNN(\n",
    "        num_embeddings=config.num_embeddings,\n",
    "        num_layers=config.pixelcnn_layers,\n",
    "        hidden_dim=config.pixelcnn_hidden\n",
    "    ).to(config.device)\n",
    "    \n",
    "    prior_trainer = PriorTrainer(prior, vqvae, config)\n",
    "    \n",
    "    # Try to load existing checkpoint\n",
    "    if not prior_trainer.load_checkpoint(os.path.join(config.checkpoint_dir, 'prior_final.pt')):\n",
    "        print(\"\\n✓ Training prior from scratch...\")\n",
    "        prior_history = prior_trainer.train(train_loader, spatial_size)\n",
    "        \n",
    "        # Plot training history\n",
    "        plot_prior_training_history(prior_history, os.path.join(config.results_dir, 'prior_training.png'))\n",
    "    else:\n",
    "        print(\"\\n✓ Using pre-trained prior\")\n",
    "        prior_history = prior_trainer.history\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Generate Novel Emojis\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING NOVEL EMOJIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    prior.eval()\n",
    "    vqvae.eval()\n",
    "    \n",
    "    print(f\"\\nGenerating {config.num_samples} samples...\")\n",
    "    with torch.no_grad():\n",
    "        generated_codes = prior.sample(\n",
    "            batch_size=config.num_samples,\n",
    "            spatial_size=spatial_size,\n",
    "            device=config.device,\n",
    "            temperature=config.temperature\n",
    "        )\n",
    "        generated_images = decode_codes(vqvae, generated_codes, spatial_size)\n",
    "    \n",
    "    plot_generated_samples(\n",
    "        generated_images, \n",
    "        os.path.join(config.results_dir, 'generated_samples.png'),\n",
    "        f\"Generated Emojis (Temperature={config.temperature})\"\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Latent Space Interpolation\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LATENT SPACE ANALYSIS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(\"Performing interpolation...\")\n",
    "    visualize_latent_interpolation(\n",
    "        vqvae, val_loader, config.device,\n",
    "        os.path.join(config.results_dir, 'interpolation.png'),\n",
    "        num_steps=config.num_interpolation_steps\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # t-SNE Visualization\n",
    "    # ========================================================================\n",
    "    \n",
    "    visualize_tsne(\n",
    "        vqvae, val_loader, config.device,\n",
    "        os.path.join(config.results_dir, 'tsne_codes.png')\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Clustering Analysis\n",
    "    # ========================================================================\n",
    "    \n",
    "    visualize_clustering(\n",
    "        vqvae, val_loader, config.device,\n",
    "        os.path.join(config.results_dir, 'clustering.png'),\n",
    "        n_clusters=10\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FID Score Calculation\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CALCULATING FID SCORE\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Collect real images\n",
    "    print(\"Collecting real images...\")\n",
    "    real_images = []\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        if i >= 4:  # Use ~256 images\n",
    "            break\n",
    "        real_images.append(batch)\n",
    "    real_images = torch.cat(real_images, dim=0)[:256].to(config.device)\n",
    "    \n",
    "    print(f\"✓ Real images: {real_images.shape[0]}\")\n",
    "    \n",
    "    # Generate matching number of fake images\n",
    "    print(\"Generating images for FID calculation...\")\n",
    "    gen_images = []\n",
    "    num_batches = (len(real_images) + 63) // 64\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            batch_size = min(64, len(real_images) - i * 64)\n",
    "            if batch_size <= 0:\n",
    "                break\n",
    "            codes = prior.sample(batch_size, spatial_size, config.device, temperature=config.temperature)\n",
    "            gen_batch = decode_codes(vqvae, codes, spatial_size)\n",
    "            gen_images.append(gen_batch)\n",
    "    \n",
    "    gen_images = torch.cat(gen_images, dim=0)[:len(real_images)].to(config.device)\n",
    "    print(f\"✓ Generated images: {gen_images.shape[0]}\")\n",
    "    \n",
    "    # Extract Inception features\n",
    "    print(\"\\nExtracting Inception features...\")\n",
    "    inception_model = None\n",
    "    real_features, inception_model = get_inception_features(real_images, inception_model, config.device)\n",
    "    gen_features, _ = get_inception_features(gen_images, inception_model, config.device)\n",
    "    \n",
    "    print(f\"✓ Real features: {real_features.shape}\")\n",
    "    print(f\"✓ Generated features: {gen_features.shape}\")\n",
    "    \n",
    "    # Calculate FID\n",
    "    fid_score = calculate_fid(real_features, gen_features)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FID SCORE: {fid_score:.2f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Save Comprehensive Results\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Prepare comprehensive results for CSV\n",
    "    results = {\n",
    "        'phase': 'phase2',\n",
    "        'fid_score': float(fid_score),\n",
    "        'final_prior_loss': prior_history['loss'][-1] if prior_history['loss'] else None,\n",
    "        'num_generated_samples': config.num_samples,\n",
    "        'spatial_size': spatial_size,\n",
    "        'temperature': config.temperature,\n",
    "        'phase1_codebook_usage': phase1_summary['codebook_usage_percent'],\n",
    "        'phase1_active_codes': phase1_summary['active_codes'],\n",
    "        'phase1_target_achieved': phase1_summary['target_achieved'],\n",
    "        'generation_completed': True,\n",
    "        'num_prior_epochs': config.num_epochs_prior,\n",
    "        'pixelcnn_layers': config.pixelcnn_layers,\n",
    "        'pixelcnn_hidden': config.pixelcnn_hidden\n",
    "    }\n",
    "    \n",
    "    # Save to CSV (appends to same file as Phase 1)\n",
    "    csv_path = os.path.join(config.results_dir, 'experiment_results.csv')\n",
    "    save_results_to_csv(config, results, csv_path)\n",
    "    \n",
    "    # Save JSON report for backward compatibility\n",
    "    final_report = {\n",
    "        'fid_score': float(fid_score),\n",
    "        'num_embeddings': config.num_embeddings,\n",
    "        'embedding_dim': config.embedding_dim,\n",
    "        'num_training_samples': len(train_dataset),\n",
    "        'phase1_codebook_usage': phase1_summary['codebook_usage_percent'],\n",
    "        'phase1_active_codes': phase1_summary['active_codes'],\n",
    "        'spatial_size': spatial_size,\n",
    "        'temperature': config.temperature,\n",
    "        'prior_layers': config.pixelcnn_layers,\n",
    "        'prior_hidden': config.pixelcnn_hidden,\n",
    "        'final_prior_loss': prior_history['loss'][-1] if prior_history['loss'] else None\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(config.results_dir, 'final_report.json'), 'w') as f:\n",
    "        json.dump(final_report, f, indent=2)\n",
    "    \n",
    "    print(\"✓ Final report saved: final_report.json\")\n",
    "    \n",
    "    # Create comprehensive summary document\n",
    "    summary_text = f\"\"\"\n",
    "{'='*80}\n",
    "VQ-VAE EMOJI GENERATION - COMPLETE PIPELINE RESULTS\n",
    "{'='*80}\n",
    "\n",
    "EXPERIMENT: {config.phase2_experiment_name}\n",
    "TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "{'='*80}\n",
    "PHASE 1: VQ-VAE TRAINING\n",
    "{'='*80}\n",
    "Codebook Size: {config.num_embeddings}\n",
    "Embedding Dim: {config.embedding_dim}\n",
    "Commitment Cost: {config.commitment_cost}\n",
    "Decay: {config.decay}\n",
    "\n",
    "Results:\n",
    "- Codebook Usage: {phase1_summary['codebook_usage_percent']:.2f}%\n",
    "- Active Codes: {phase1_summary['active_codes']}/{phase1_summary['total_codes']}\n",
    "- Target Achieved: {'✓' if phase1_summary['target_achieved'] else '✗'}\n",
    "\n",
    "{'='*80}\n",
    "PHASE 2: PRIOR TRAINING & GENERATION\n",
    "{'='*80}\n",
    "Prior Architecture: PixelCNN\n",
    "- Layers: {config.pixelcnn_layers}\n",
    "- Hidden Dim: {config.pixelcnn_hidden}\n",
    "- Learning Rate: {config.learning_rate_prior}\n",
    "- Epochs: {config.num_epochs_prior}\n",
    "\n",
    "Generation:\n",
    "- Spatial Size: {spatial_size}x{spatial_size}\n",
    "- Temperature: {config.temperature}\n",
    "- Samples Generated: {config.num_samples}\n",
    "\n",
    "Results:\n",
    "- FID Score: {fid_score:.2f}\n",
    "- Final Prior Loss: {prior_history['loss'][-1]:.4f if prior_history['loss'] else 'N/A'}\n",
    "\n",
    "{'='*80}\n",
    "OUTPUTS GENERATED\n",
    "{'='*80}\n",
    "✓ generated_samples.png - Novel emoji generations\n",
    "✓ interpolation.png - Latent space interpolations\n",
    "✓ tsne_codes.png - t-SNE visualization\n",
    "✓ clustering.png - K-means clustering analysis\n",
    "✓ prior_training.png - Prior training curves\n",
    "✓ final_report.json - Detailed results\n",
    "✓ experiment_results.csv - Complete experiment log\n",
    "\n",
    "{'='*80}\n",
    "NOTES\n",
    "{'='*80}\n",
    "Phase 1: {phase1_config_dict.get('notes', 'N/A')}\n",
    "Phase 2: {config.phase2_notes}\n",
    "\n",
    "{'='*80}\n",
    "END OF REPORT\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "    \n",
    "    with open(os.path.join(config.results_dir, 'comprehensive_summary.txt'), 'w') as f:\n",
    "        f.write(summary_text)\n",
    "    \n",
    "    print(\"✓ Comprehensive summary saved: comprehensive_summary.txt\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Final Output\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(summary_text)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PHASE 2 COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"✓ All results saved to: {config.results_dir}\")\n",
    "    print(f\"✓ FID Score: {fid_score:.2f}\")\n",
    "    print(f\"✓ Codebook Usage: {phase1_summary['codebook_usage_percent']:.2f}%\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXPERIMENT TRACKING\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"CSV Log: {csv_path}\")\n",
    "    print(f\"Phase 1 Experiment: {phase1_config_dict.get('experiment_name', 'N/A')}\")\n",
    "    print(f\"Phase 2 Experiment: {config.phase2_experiment_name}\")\n",
    "    print(f\"\\nCopy the CSV row to your Excel tracking sheet for comparison!\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

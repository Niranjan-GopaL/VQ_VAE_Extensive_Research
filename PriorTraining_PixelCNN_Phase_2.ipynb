{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c66ca1",
   "metadata": {},
   "source": [
    "# PixelCNN arch didn't match with the VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645754f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VQ-VAE Emoji Generation - Phase 2: Prior Training, Generation & Analysis\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import linalg\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# Phase 2 Configuration (Extends Phase 1)\n",
    "# ============================================================================\n",
    "\n",
    "EXPERIMENT_CONFIGS_PHASE2 = {\n",
    "    # Prior (PixelCNN)\n",
    "    \"num_epochs_prior\": 200,\n",
    "    \"learning_rate_prior\": 1e-4,\n",
    "    \"pixelcnn_layers\": 15,\n",
    "    \"pixelcnn_hidden\": 128,\n",
    "    \"grad_clip\": 1.0,\n",
    "\n",
    "    # Generation\n",
    "    \"num_samples\": 64,\n",
    "    \"num_interpolation_steps\": 10,\n",
    "    \"temperature\": 1.0,\n",
    "\n",
    "    # Experiment metadata for Phase 2\n",
    "    \"phase2_experiment_name\": \"prior_baseline\",\n",
    "    \"phase2_notes\": \"PixelCNN autoregressive prior training\"\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration Management\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class that loads from JSON dict\"\"\"\n",
    "    def __init__(self, config_dict):\n",
    "        # Define expected types for automatic conversion\n",
    "        self.type_mappings = {\n",
    "            'image_size': int,\n",
    "            'batch_size': int,\n",
    "            'num_workers': int,\n",
    "            'num_hiddens': int,\n",
    "            'num_residual_hiddens': int,\n",
    "            'num_residual_layers': int,\n",
    "            'embedding_dim': int,\n",
    "            'num_embeddings': int,\n",
    "            'commitment_cost': float,\n",
    "            'decay': float,\n",
    "            'num_epochs_vqvae': int,\n",
    "            'learning_rate_vqvae': float,\n",
    "            'min_codebook_usage': float,\n",
    "            'check_usage_every': int,\n",
    "            'num_epochs_prior': int,\n",
    "            'learning_rate_prior': float,\n",
    "            'pixelcnn_layers': int,\n",
    "            'pixelcnn_hidden': int,\n",
    "            'grad_clip': float,\n",
    "            'num_samples': int,\n",
    "            'num_interpolation_steps': int,\n",
    "            'temperature': float,\n",
    "        }\n",
    "\n",
    "        for key, value in config_dict.items():\n",
    "            # Skip None keys and empty strings\n",
    "            if key is None or key == '':\n",
    "                continue\n",
    "                \n",
    "            # Convert to proper type if needed\n",
    "            if key in self.type_mappings and isinstance(value, str):\n",
    "                try:\n",
    "                    value = self.type_mappings[key](value)\n",
    "                except (ValueError, TypeError):\n",
    "                    pass  # Keep as string if conversion fails\n",
    "            setattr(self, key, value)\n",
    "\n",
    "        # Add device (not in JSON as it's system-dependent)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert config back to dictionary for CSV export\"\"\"\n",
    "        return {k: v for k, v in self.__dict__.items() if not k.startswith('_') and k != 'type_mappings'}\n",
    "\n",
    "# ============================================================================\n",
    "# Dataset (Same as Phase 1)\n",
    "# ============================================================================\n",
    "\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_size=64, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_size = image_size\n",
    "        self.image_files = [f for f in os.listdir(data_dir)\n",
    "                           if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(image)\n",
    "\n",
    "# ============================================================================\n",
    "# VQ-VAE Components (Must match Phase 1 exactly)\n",
    "# ============================================================================\n",
    "\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay=0.99, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        embed = torch.randn(num_embeddings, embedding_dim)\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(num_embeddings))\n",
    "        self.register_buffer(\"embed_avg\", embed.clone())\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        input_shape = inputs.shape\n",
    "        flat_input = inputs.permute(0, 2, 3, 1).contiguous().view(-1, self.embedding_dim)\n",
    "\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
    "                    + torch.sum(self.embed**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self.embed.t()))\n",
    "\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        quantized = torch.matmul(encodings, self.embed)\n",
    "\n",
    "        if self.training:\n",
    "            self.cluster_size.data.mul_(self.decay).add_(encodings.sum(0), alpha=1 - self.decay)\n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self.embed_avg.data.mul_(self.decay).add_(dw, alpha=1 - self.decay)\n",
    "\n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = ((self.cluster_size + self.epsilon) / (n + self.num_embeddings * self.epsilon) * n)\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), flat_input)\n",
    "        loss = self.commitment_cost * e_latent_loss\n",
    "        quantized = flat_input + (quantized - flat_input).detach()\n",
    "\n",
    "        quantized = quantized.view(input_shape[0], input_shape[2], input_shape[3], self.embedding_dim)\n",
    "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        return quantized, loss, perplexity, encoding_indices\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, num_residual_hiddens, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_residual_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_residual_hiddens, num_hiddens, 1, bias=False),\n",
    "            nn.BatchNorm2d(num_hiddens)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            ResidualBlock(in_channels, num_hiddens, num_residual_hiddens)\n",
    "            for _ in range(num_residual_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return F.relu(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, num_hiddens // 2, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(num_hiddens // 2, num_hiddens, 4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(num_hiddens, num_hiddens, 3, padding=1)\n",
    "        self.residual_stack = ResidualStack(num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return self.residual_stack(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(embedding_dim, num_hiddens, 3, padding=1)\n",
    "        self.residual_stack = ResidualStack(num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "        self.conv_trans1 = nn.ConvTranspose2d(num_hiddens, num_hiddens // 2, 4, stride=2, padding=1)\n",
    "        self.conv_trans2 = nn.ConvTranspose2d(num_hiddens // 2, 3, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.residual_stack(x)\n",
    "        x = F.relu(self.conv_trans1(x))\n",
    "        return torch.tanh(self.conv_trans2(x))\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(3, config.num_hiddens, config.num_residual_layers, config.num_residual_hiddens)\n",
    "        self.pre_vq_conv = nn.Conv2d(config.num_hiddens, config.embedding_dim, 1)\n",
    "        self.vq = VectorQuantizerEMA(config.num_embeddings, config.embedding_dim, config.commitment_cost, decay=config.decay)\n",
    "        self.decoder = Decoder(config.embedding_dim, config.num_hiddens, config.num_residual_layers, config.num_residual_hiddens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = self.pre_vq_conv(z)\n",
    "        quantized, vq_loss, perplexity, encoding_indices = self.vq(z)\n",
    "        x_recon = self.decoder(quantized)\n",
    "        return x_recon, vq_loss, perplexity, encoding_indices\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = self.pre_vq_conv(z)\n",
    "        _, _, _, encoding_indices = self.vq(z)\n",
    "        B = x.shape[0]\n",
    "        return encoding_indices.view(B, -1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIXED PixelCNN Implementation\n",
    "# ============================================================================\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    \"\"\"Fixed masked convolution that doesn't break gradients\"\"\"\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.register_buffer('mask', torch.zeros_like(self.weight))\n",
    "        self.create_mask(mask_type)\n",
    "\n",
    "    def create_mask(self, mask_type):\n",
    "        k = self.kernel_size[0]\n",
    "        self.mask[:, :, :k//2, :] = 1\n",
    "        self.mask[:, :, k//2, :k//2] = 1\n",
    "        if mask_type == 'B':\n",
    "            self.mask[:, :, k//2, k//2] = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # âœ“ FIXED: Don't modify weights in-place, use functional API\n",
    "        masked_weight = self.weight * self.mask\n",
    "        return F.conv2d(x, masked_weight, self.bias, self.stride, \n",
    "                       self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "class GatedResidualBlock(nn.Module):\n",
    "    \"\"\"Gated activation improves PixelCNN performance\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = MaskedConv2d('B', hidden_dim, hidden_dim * 2, 3, padding=1)\n",
    "        self.conv2 = MaskedConv2d('B', hidden_dim, hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.conv1(F.relu(x))\n",
    "        # Split into two halves for gated activation\n",
    "        h1, h2 = h.chunk(2, dim=1)\n",
    "        h = torch.tanh(h1) * torch.sigmoid(h2)\n",
    "        h = self.conv2(h)\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class ImprovedPixelCNN(nn.Module):\n",
    "    \"\"\"Enhanced PixelCNN with better architecture\"\"\"\n",
    "    def __init__(self, num_embeddings, spatial_h, spatial_w, \n",
    "                 num_layers=15, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.spatial_h = spatial_h\n",
    "        self.spatial_w = spatial_w\n",
    "        \n",
    "        # Input projection with larger kernel\n",
    "        self.input_conv = nn.Sequential(\n",
    "            MaskedConv2d('A', num_embeddings, hidden_dim, 7, padding=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Gated residual blocks\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            GatedResidualBlock(hidden_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection with better initialization\n",
    "        self.output = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            MaskedConv2d('B', hidden_dim, hidden_dim, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, num_embeddings, 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Better initialization strategy\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, MaskedConv2d)):\n",
    "                # âœ“ FIXED: Use proper initialization gain\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, H, W) - discrete codes\n",
    "        x_onehot = F.one_hot(x, self.num_embeddings).float()\n",
    "        x_onehot = x_onehot.permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        h = self.input_conv(x_onehot)\n",
    "        \n",
    "        for block in self.residual_blocks:\n",
    "            h = block(h)\n",
    "        \n",
    "        logits = self.output(h)\n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size, device, temperature=1.0):\n",
    "        \"\"\"Improved sampling with temperature control\"\"\"\n",
    "        samples = torch.zeros(batch_size, self.spatial_h, self.spatial_w,\n",
    "                            dtype=torch.long, device=device)\n",
    "        \n",
    "        self.eval()\n",
    "        \n",
    "        for i in range(self.spatial_h):\n",
    "            for j in range(self.spatial_w):\n",
    "                logits = self(samples)\n",
    "                logits = logits[:, :, i, j] / temperature\n",
    "                \n",
    "                # Add top-k sampling for better quality\n",
    "                if temperature < 1.0:\n",
    "                    # Greedy sampling for low temperature\n",
    "                    samples[:, i, j] = logits.argmax(dim=1)\n",
    "                else:\n",
    "                    # Probabilistic sampling\n",
    "                    probs = F.softmax(logits, dim=1)\n",
    "                    samples[:, i, j] = torch.multinomial(probs, 1).squeeze(-1)\n",
    "        \n",
    "        return samples\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED Prior Trainer\n",
    "# ============================================================================\n",
    "\n",
    "class ImprovedPriorTrainer:\n",
    "    def __init__(self, prior, vqvae, config):\n",
    "        self.prior = prior\n",
    "        self.vqvae = vqvae\n",
    "        self.config = config\n",
    "        \n",
    "        # âœ“ FIXED: Add learning rate scheduler\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            prior.parameters(), \n",
    "            lr=config.learning_rate_prior,\n",
    "            weight_decay=0.01  # Add weight decay\n",
    "        )\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, \n",
    "            T_max=config.num_epochs_prior,\n",
    "            eta_min=config.learning_rate_prior * 0.1\n",
    "        )\n",
    "        \n",
    "        self.history = {'loss': [], 'accuracy': [], 'perplexity': [], 'epoch': []}\n",
    "        self.start_epoch = 0\n",
    "        self.best_loss = float('inf')\n",
    "    \n",
    "    def save_checkpoint(self, epoch, filepath):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.prior.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'history': self.history,\n",
    "            'best_loss': self.best_loss\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"âœ“ Prior checkpoint saved: {filepath}\")\n",
    "    \n",
    "    def load_checkpoint(self, filepath):\n",
    "        if os.path.exists(filepath):\n",
    "            try:\n",
    "                checkpoint = torch.load(filepath, map_location=self.config.device)\n",
    "                self.prior.load_state_dict(checkpoint['model_state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                \n",
    "                if 'scheduler_state_dict' in checkpoint:\n",
    "                    self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "                \n",
    "                self.history = checkpoint['history']\n",
    "                self.start_epoch = checkpoint['epoch'] + 1\n",
    "                self.best_loss = checkpoint.get('best_loss', float('inf'))\n",
    "                \n",
    "                print(f\"âœ“ Prior checkpoint loaded, resuming from epoch {self.start_epoch}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Error loading prior checkpoint: {e}\")\n",
    "                return False\n",
    "        return False\n",
    "    \n",
    "    def train_epoch(self, dataloader, spatial_h, spatial_w):\n",
    "        self.prior.train()\n",
    "        self.vqvae.eval()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        epoch_perplexity = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=\"Training Prior\")\n",
    "        \n",
    "        for batch in pbar:\n",
    "            batch = batch.to(self.config.device)\n",
    "            \n",
    "            # Get codes from VQ-VAE\n",
    "            with torch.no_grad():\n",
    "                codes = self.vqvae.encode(batch)\n",
    "                codes = codes.view(-1, spatial_h, spatial_w)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = self.prior(codes)\n",
    "            loss = F.cross_entropy(logits, codes)\n",
    "            \n",
    "            # âœ“ FIXED: Add perplexity and accuracy tracking\n",
    "            with torch.no_grad():\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                avg_probs = probs.mean(dim=[0, 2, 3])\n",
    "                perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "                \n",
    "                pred = logits.argmax(dim=1)\n",
    "                accuracy = (pred == codes).float().mean()\n",
    "            \n",
    "            # Check for NaN\n",
    "            if torch.isnan(loss):\n",
    "                print(\"Warning: NaN loss detected, skipping batch\")\n",
    "                continue\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # âœ“ FIXED: Adaptive gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.prior.parameters(), self.config.grad_clip)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_accuracy += accuracy.item()\n",
    "            epoch_perplexity += perplexity.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'acc': f'{accuracy.item():.3f}',\n",
    "                'ppl': f'{perplexity.item():.1f}'\n",
    "            })\n",
    "        \n",
    "        # Update learning rate\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return {\n",
    "            'loss': epoch_loss / num_batches,\n",
    "            'accuracy': epoch_accuracy / num_batches,\n",
    "            'perplexity': epoch_perplexity / num_batches\n",
    "        }\n",
    "    \n",
    "    def train(self, train_loader, spatial_h, spatial_w):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TRAINING IMPROVED PIXELCNN PRIOR\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        for epoch in range(self.start_epoch, self.config.num_epochs_prior):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs_prior}\")\n",
    "            print(f\"Learning Rate: {self.scheduler.get_last_lr()[0]:.6f}\")\n",
    "            \n",
    "            metrics = self.train_epoch(train_loader, spatial_h, spatial_w)\n",
    "            \n",
    "            # Update history\n",
    "            self.history['loss'].append(metrics['loss'])\n",
    "            self.history['accuracy'].append(metrics['accuracy'])\n",
    "            self.history['perplexity'].append(metrics['perplexity'])\n",
    "            self.history['epoch'].append(epoch + 1)\n",
    "            \n",
    "            print(f\"Loss: {metrics['loss']:.4f}, \"\n",
    "                  f\"Acc: {metrics['accuracy']:.3f}, \"\n",
    "                  f\"Perplexity: {metrics['perplexity']:.1f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if metrics['loss'] < self.best_loss:\n",
    "                self.best_loss = metrics['loss']\n",
    "                self.save_checkpoint(\n",
    "                    epoch, \n",
    "                    os.path.join(self.config.checkpoint_dir, 'prior_best.pt')\n",
    "                )\n",
    "                print(f\"âœ“ New best model saved!\")\n",
    "            \n",
    "            # Regular checkpoints\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self.save_checkpoint(\n",
    "                    epoch, \n",
    "                    os.path.join(self.config.checkpoint_dir, f'prior_epoch_{epoch+1}.pt')\n",
    "                )\n",
    "        \n",
    "        # Save final checkpoint\n",
    "        self.save_checkpoint(\n",
    "            self.config.num_epochs_prior - 1,\n",
    "            os.path.join(self.config.checkpoint_dir, 'prior_final.pt')\n",
    "        )\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Decoding Helper\n",
    "# ============================================================================\n",
    "\n",
    "def decode_codes(vqvae, encoding_indices, spatial_h, spatial_w):\n",
    "    \"\"\"Decode from discrete codes with correct spatial dimensions\"\"\"\n",
    "    # encoding_indices shape: (B, H, W)\n",
    "    batch_size = encoding_indices.shape[0]\n",
    "\n",
    "    # Flatten for embedding lookup\n",
    "    flat_codes = encoding_indices.view(-1)  # (B*H*W)\n",
    "    quantized = F.embedding(flat_codes, vqvae.vq.embed)  # (B*H*W, embedding_dim)\n",
    "\n",
    "    # Reshape back to spatial format\n",
    "    quantized = quantized.view(batch_size, spatial_h, spatial_w, -1)  # (B, H, W, embedding_dim)\n",
    "    quantized = quantized.permute(0, 3, 1, 2).contiguous()  # (B, embedding_dim, H, W)\n",
    "\n",
    "    return vqvae.decoder(quantized)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Diagnostic Tools for VQ-VAE Code Analysis\n",
    "Run this to understand why PixelCNN is struggling\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def analyze_code_spatial_structure(vqvae, dataloader, device, spatial_h, spatial_w):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of VQ-VAE codes to diagnose PixelCNN issues\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DIAGNOSING VQ-VAE CODE STRUCTURE\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    vqvae.eval()\n",
    "    all_codes = []\n",
    "    \n",
    "    # Collect codes from dataset\n",
    "    print(\"Collecting codes from dataset...\")\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= 20:  # Sample 20 batches\n",
    "                break\n",
    "            batch = batch.to(device)\n",
    "            codes = vqvae.encode(batch)\n",
    "            codes = codes.view(-1, spatial_h, spatial_w)\n",
    "            all_codes.append(codes.cpu())\n",
    "    \n",
    "    all_codes = torch.cat(all_codes, dim=0)  # (N, H, W)\n",
    "    N, H, W = all_codes.shape\n",
    "    \n",
    "    print(f\"âœ“ Collected {N} code maps of size {H}Ã—{W}\\n\")\n",
    "    \n",
    "    # =======================================================================\n",
    "    # 1. CODE DISTRIBUTION ANALYSIS\n",
    "    # =======================================================================\n",
    "    print(\"1. CODE DISTRIBUTION ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    flat_codes = all_codes.view(-1).numpy()\n",
    "    unique_codes, counts = np.unique(flat_codes, return_counts=True)\n",
    "    \n",
    "    print(f\"Unique codes used: {len(unique_codes)}\")\n",
    "    print(f\"Total possible codes: {vqvae.vq.num_embeddings}\")\n",
    "    print(f\"Usage: {len(unique_codes)/vqvae.vq.num_embeddings*100:.1f}%\")\n",
    "    \n",
    "    # Check if distribution is uniform or skewed\n",
    "    code_probs = counts / counts.sum()\n",
    "    code_entropy = entropy(code_probs)\n",
    "    max_entropy = np.log(len(unique_codes))\n",
    "    \n",
    "    print(f\"\\nDistribution entropy: {code_entropy:.2f}\")\n",
    "    print(f\"Max possible entropy: {max_entropy:.2f}\")\n",
    "    print(f\"Entropy ratio: {code_entropy/max_entropy:.2%}\")\n",
    "    \n",
    "    if code_entropy/max_entropy < 0.7:\n",
    "        print(\"âš  WARNING: Code distribution is skewed (some codes dominate)\")\n",
    "    else:\n",
    "        print(\"âœ“ Code distribution is relatively uniform\")\n",
    "    \n",
    "    # =======================================================================\n",
    "    # 2. SPATIAL AUTOCORRELATION ANALYSIS\n",
    "    # =======================================================================\n",
    "    print(\"\\n2. SPATIAL AUTOCORRELATION ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Check horizontal and vertical neighbors\n",
    "    horizontal_same = (all_codes[:, :, :-1] == all_codes[:, :, 1:]).float().mean().item()\n",
    "    vertical_same = (all_codes[:, :-1, :] == all_codes[:, 1:, :]).float().mean().item()\n",
    "    diagonal_same = (all_codes[:, :-1, :-1] == all_codes[:, 1:, 1:]).float().mean().item()\n",
    "    \n",
    "    print(f\"Horizontal neighbor similarity: {horizontal_same:.2%}\")\n",
    "    print(f\"Vertical neighbor similarity: {vertical_same:.2%}\")\n",
    "    print(f\"Diagonal neighbor similarity: {diagonal_same:.2%}\")\n",
    "    print(f\"Average spatial correlation: {(horizontal_same + vertical_same)/2:.2%}\")\n",
    "    \n",
    "    if (horizontal_same + vertical_same)/2 < 0.3:\n",
    "        print(\"\\nâš  CRITICAL: Very low spatial correlation!\")\n",
    "        print(\"  This explains why PixelCNN struggles - codes are too 'random'\")\n",
    "        print(\"  Recommendation: Reduce codebook size or increase commitment cost\")\n",
    "    elif (horizontal_same + vertical_same)/2 < 0.5:\n",
    "        print(\"\\nâš  WARNING: Moderate spatial correlation\")\n",
    "        print(\"  PixelCNN will struggle. Consider architectural changes.\")\n",
    "    else:\n",
    "        print(\"\\nâœ“ Good spatial correlation - suitable for PixelCNN\")\n",
    "    \n",
    "    # =======================================================================\n",
    "    # 3. PREDICTABILITY ANALYSIS\n",
    "    # =======================================================================\n",
    "    print(\"\\n3. PREDICTABILITY ANALYSIS (Oracle Test)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Simple oracle: what if we predict each pixel from its left neighbor?\n",
    "    oracle_correct = 0\n",
    "    oracle_total = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        for h in range(H):\n",
    "            for w in range(1, W):  # Skip first column\n",
    "                if all_codes[i, h, w-1] == all_codes[i, h, w]:\n",
    "                    oracle_correct += 1\n",
    "                oracle_total += 1\n",
    "    \n",
    "    oracle_accuracy = oracle_correct / oracle_total\n",
    "    print(f\"Left-neighbor oracle accuracy: {oracle_accuracy:.2%}\")\n",
    "    print(f\"Random chance: {1/vqvae.vq.num_embeddings:.2%}\")\n",
    "    print(f\"PixelCNN current accuracy: ~42%\")\n",
    "    \n",
    "    if oracle_accuracy < 0.4:\n",
    "        print(\"\\nâš  CRITICAL: Even simple oracle fails!\")\n",
    "        print(\"  Codes lack autoregressive structure. PixelCNN won't work well.\")\n",
    "    elif oracle_accuracy < 0.6:\n",
    "        print(\"\\nâš  WARNING: Oracle accuracy is low\")\n",
    "        print(\"  PixelCNN will need many layers to capture patterns\")\n",
    "    else:\n",
    "        print(\"\\nâœ“ Codes have predictable structure\")\n",
    "    \n",
    "    # =======================================================================\n",
    "    # 4. VISUALIZE CODE PATTERNS\n",
    "    # =======================================================================\n",
    "    print(\"\\n4. GENERATING VISUALIZATIONS...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Show 8 random code maps\n",
    "    for idx in range(8):\n",
    "        ax = axes[idx // 4, idx % 4]\n",
    "        sample_codes = all_codes[np.random.randint(N)]\n",
    "        im = ax.imshow(sample_codes.numpy(), cmap='tab20', interpolation='nearest')\n",
    "        ax.set_title(f'Sample {idx+1}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.colorbar(im, ax=axes.ravel().tolist(), label='Code Index')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./results/code_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"âœ“ Saved: ./results/code_visualization.png\")\n",
    "    \n",
    "    # =======================================================================\n",
    "    # 5. CODE TRANSITION MATRIX\n",
    "    # =======================================================================\n",
    "    print(\"\\n5. CODE TRANSITION ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Build transition matrix for horizontal neighbors\n",
    "    transition_counts = np.zeros((vqvae.vq.num_embeddings, vqvae.vq.num_embeddings))\n",
    "    \n",
    "    for i in range(N):\n",
    "        for h in range(H):\n",
    "            for w in range(W-1):\n",
    "                curr = all_codes[i, h, w].item()\n",
    "                next_code = all_codes[i, h, w+1].item()\n",
    "                transition_counts[curr, next_code] += 1\n",
    "    \n",
    "    # Normalize to probabilities\n",
    "    row_sums = transition_counts.sum(axis=1, keepdims=True)\n",
    "    transition_probs = np.divide(transition_counts, row_sums, \n",
    "                                 where=row_sums>0, out=np.zeros_like(transition_counts))\n",
    "    \n",
    "    # Measure transition entropy (how predictable are transitions?)\n",
    "    transition_entropy = []\n",
    "    for i in range(vqvae.vq.num_embeddings):\n",
    "        if transition_probs[i].sum() > 0:\n",
    "            trans_ent = entropy(transition_probs[i] + 1e-10)\n",
    "            transition_entropy.append(trans_ent)\n",
    "    \n",
    "    avg_transition_entropy = np.mean(transition_entropy) if transition_entropy else 0\n",
    "    max_transition_entropy = np.log(vqvae.vq.num_embeddings)\n",
    "    \n",
    "    print(f\"Average transition entropy: {avg_transition_entropy:.2f}\")\n",
    "    print(f\"Max transition entropy: {max_transition_entropy:.2f}\")\n",
    "    print(f\"Predictability: {1 - avg_transition_entropy/max_transition_entropy:.2%}\")\n",
    "    \n",
    "    if avg_transition_entropy/max_transition_entropy > 0.8:\n",
    "        print(\"\\nâš  CRITICAL: Transitions are nearly random!\")\n",
    "        print(\"  Each code can be followed by almost any other code equally\")\n",
    "    \n",
    "    # =======================================================================\n",
    "    # 6. RECOMMENDATIONS\n",
    "    # =======================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Compute overall \"learnability score\"\n",
    "    spatial_corr = (horizontal_same + vertical_same) / 2\n",
    "    predictability = 1 - avg_transition_entropy/max_transition_entropy\n",
    "    \n",
    "    score = (spatial_corr * 0.5 + oracle_accuracy * 0.3 + predictability * 0.2) * 100\n",
    "    \n",
    "    print(f\"Code Learnability Score: {score:.1f}/100\")\n",
    "    \n",
    "    if score < 30:\n",
    "        print(\"\\nðŸ”´ POOR - PixelCNN will struggle significantly\")\n",
    "        print(\"\\nSuggested fixes (in order of impact):\")\n",
    "        print(\"1. REDUCE codebook size: 256 â†’ 128 or 64\")\n",
    "        print(\"2. INCREASE commitment cost: Try 0.25, 0.5, or even 1.0\")\n",
    "        print(\"3. DECREASE spatial compression: Change encoder strides\")\n",
    "        print(\"4. Consider Transformer prior instead of PixelCNN\")\n",
    "        \n",
    "    elif score < 50:\n",
    "        print(\"\\nðŸŸ¡ MODERATE - PixelCNN needs help\")\n",
    "        print(\"\\nSuggested improvements:\")\n",
    "        print(\"1. Use multi-scale PixelCNN (condition on lower resolution)\")\n",
    "        print(\"2. Increase model capacity: 256-512 hidden dims, 20+ layers\")\n",
    "        print(\"3. Train much longer: 200-300 epochs\")\n",
    "        print(\"4. Try hierarchical VQ-VAE (2-level)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nðŸŸ¢ GOOD - PixelCNN should work\")\n",
    "        print(\"\\nFine-tuning suggestions:\")\n",
    "        print(\"1. Increase training epochs to 150-200\")\n",
    "        print(\"2. Use larger model: 256 hidden, 20 layers\")\n",
    "        print(\"3. Try different learning rate schedules\")\n",
    "    \n",
    "    return {\n",
    "        'spatial_correlation': spatial_corr,\n",
    "        'oracle_accuracy': oracle_accuracy,\n",
    "        'transition_predictability': predictability,\n",
    "        'learnability_score': score,\n",
    "        'unique_codes': len(unique_codes),\n",
    "        'code_entropy': code_entropy / max_entropy\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # PixelCNN Prior\n",
    "# # ============================================================================\n",
    "\n",
    "# class MaskedConv2d(nn.Conv2d):\n",
    "#     def __init__(self, mask_type, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.register_buffer('mask', torch.zeros_like(self.weight))\n",
    "#         self.create_mask(mask_type)\n",
    "\n",
    "#     def create_mask(self, mask_type):\n",
    "#         k = self.kernel_size[0]\n",
    "#         self.mask[:, :, :k//2, :] = 1\n",
    "#         self.mask[:, :, k//2, :k//2] = 1\n",
    "#         if mask_type == 'B':\n",
    "#             self.mask[:, :, k//2, k//2] = 1\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         self.weight.data *= self.mask\n",
    "#         return super().forward(x)\n",
    "\n",
    "# class PixelCNNResidualBlock(nn.Module):\n",
    "#     def __init__(self, h):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.ReLU(),\n",
    "#             MaskedConv2d('B', h, h, 1),\n",
    "#             nn.BatchNorm2d(h),\n",
    "#             nn.ReLU(),\n",
    "#             MaskedConv2d('B', h, h, 1),\n",
    "#             nn.BatchNorm2d(h)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.conv(x)\n",
    "\n",
    "# class PixelCNN(nn.Module):\n",
    "#     def __init__(self, num_embeddings, spatial_h, spatial_w, num_layers=12, hidden_dim=64):\n",
    "#         super().__init__()\n",
    "#         self.num_embeddings = num_embeddings\n",
    "#         self.spatial_h = spatial_h\n",
    "#         self.spatial_w = spatial_w\n",
    "\n",
    "#         # Input projection\n",
    "#         self.input_conv = MaskedConv2d('A', num_embeddings, hidden_dim, 7, padding=3)\n",
    "\n",
    "#         # Residual blocks\n",
    "#         self.residual_blocks = nn.ModuleList([\n",
    "#             PixelCNNResidualBlock(hidden_dim) for _ in range(num_layers)\n",
    "#         ])\n",
    "\n",
    "#         # Output projection\n",
    "#         self.output = nn.Sequential(\n",
    "#             nn.ReLU(),\n",
    "#             MaskedConv2d('B', hidden_dim, hidden_dim, 1),\n",
    "#             nn.BatchNorm2d(hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(hidden_dim, num_embeddings, 1)\n",
    "#         )\n",
    "\n",
    "#         self._init_weights()\n",
    "\n",
    "#     def _init_weights(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, (nn.Conv2d, MaskedConv2d)):\n",
    "#                 nn.init.xavier_uniform_(m.weight, gain=0.1)\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 nn.init.constant_(m.weight, 1)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x shape: (B, H, W)\n",
    "#         x_onehot = F.one_hot(x, self.num_embeddings).float()  # (B, H, W, num_embeddings)\n",
    "#         x_onehot = x_onehot.permute(0, 3, 1, 2).contiguous()  # (B, num_embeddings, H, W)\n",
    "\n",
    "#         x = self.input_conv(x_onehot)\n",
    "#         for block in self.residual_blocks:\n",
    "#             x = block(x)\n",
    "#         logits = self.output(x)  # (B, num_embeddings, H, W)\n",
    "\n",
    "#         return logits\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def sample(self, batch_size, device, temperature=1.0):\n",
    "#         samples = torch.zeros(batch_size, self.spatial_h, self.spatial_w,\n",
    "#                             dtype=torch.long, device=device)\n",
    "\n",
    "#         # Sample pixel by pixel\n",
    "#         for i in range(self.spatial_h):\n",
    "#             for j in range(self.spatial_w):\n",
    "#                 logits = self(samples)  # (B, num_embeddings, H, W)\n",
    "#                 probs = F.softmax(logits[:, :, i, j] / temperature, dim=1)\n",
    "#                 samples[:, i, j] = torch.multinomial(probs, 1).squeeze(-1)\n",
    "\n",
    "#         return samples\n",
    "\n",
    "# # ============================================================================\n",
    "# # Prior Trainer\n",
    "# # ============================================================================\n",
    "\n",
    "# class PriorTrainer:\n",
    "#     def __init__(self, prior, vqvae, config):\n",
    "#         self.prior = prior\n",
    "#         self.vqvae = vqvae\n",
    "#         self.config = config\n",
    "#         self.optimizer = torch.optim.Adam(prior.parameters(), lr=config.learning_rate_prior)\n",
    "#         self.history = {'loss': [], 'epoch': []}\n",
    "#         self.start_epoch = 0\n",
    "\n",
    "#     def save_checkpoint(self, epoch, filepath):\n",
    "#         checkpoint = {\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': self.prior.state_dict(),\n",
    "#             'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "#             'history': self.history\n",
    "#         }\n",
    "#         torch.save(checkpoint, filepath)\n",
    "#         print(f\"âœ“ Prior checkpoint saved: {filepath}\")\n",
    "\n",
    "#     def load_checkpoint(self, filepath):\n",
    "#         if os.path.exists(filepath):\n",
    "#             try:\n",
    "#                 checkpoint = torch.load(filepath, map_location=self.config.device, weights_only=False)\n",
    "#                 self.prior.load_state_dict(checkpoint['model_state_dict'])\n",
    "#                 self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#                 self.history = checkpoint['history']\n",
    "#                 self.start_epoch = checkpoint['epoch'] + 1\n",
    "#                 print(f\"âœ“ Prior checkpoint loaded, resuming from epoch {self.start_epoch}\")\n",
    "#                 return True\n",
    "#             except Exception as e:\n",
    "#                 print(f\"âœ— Error loading prior checkpoint: {e}\")\n",
    "#                 return False\n",
    "#         return False\n",
    "\n",
    "#     def train_epoch(self, dataloader, spatial_h, spatial_w):\n",
    "#         self.prior.train()\n",
    "#         self.vqvae.eval()\n",
    "#         epoch_loss = 0\n",
    "#         num_valid = 0\n",
    "\n",
    "#         pbar = tqdm(dataloader, desc=\"Training Prior\")\n",
    "#         for batch in pbar:\n",
    "#             batch = batch.to(self.config.device)\n",
    "\n",
    "#             # Get codes from VQ-VAE with correct reshaping\n",
    "#             with torch.no_grad():\n",
    "#                 codes = self.vqvae.encode(batch)  # (B, H*W)\n",
    "#                 codes = codes.view(-1, spatial_h, spatial_w)  # (B, H, W)\n",
    "\n",
    "#             # Train PixelCNN\n",
    "#             logits = self.prior(codes)  # (B, num_embeddings, H, W)\n",
    "#             loss = F.cross_entropy(logits, codes)\n",
    "\n",
    "#             if torch.isnan(loss):\n",
    "#                 print(\"Warning: NaN loss detected, skipping batch\")\n",
    "#                 continue\n",
    "\n",
    "#             self.optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(self.prior.parameters(), self.config.grad_clip)\n",
    "#             self.optimizer.step()\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "#             num_valid += 1\n",
    "#             pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "#         return epoch_loss / num_valid if num_valid > 0 else float('inf')\n",
    "\n",
    "#     def train(self, train_loader, spatial_h, spatial_w):  # FIXED: Added spatial_h, spatial_w parameters\n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(f\"TRAINING PIXELCNN PRIOR\")\n",
    "#         print(f\"{'='*80}\\n\")\n",
    "\n",
    "#         for epoch in range(self.start_epoch, self.config.num_epochs_prior):\n",
    "#             print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs_prior}\")\n",
    "\n",
    "#             loss = self.train_epoch(train_loader, spatial_h, spatial_w)\n",
    "\n",
    "#             if loss == float('inf'):\n",
    "#                 print(\"Training stopped due to invalid loss\")\n",
    "#                 break\n",
    "\n",
    "#             self.history['loss'].append(loss)\n",
    "#             self.history['epoch'].append(epoch + 1)\n",
    "\n",
    "#             print(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "#             # Save checkpoints\n",
    "#             if (epoch + 1) % 10 == 0:\n",
    "#                 self.save_checkpoint(epoch, os.path.join(self.config.checkpoint_dir, f'prior_epoch_{epoch+1}.pt'))\n",
    "\n",
    "#         # Save final checkpoint\n",
    "#         self.save_checkpoint(self.config.num_epochs_prior - 1, os.path.join(self.config.checkpoint_dir, 'prior_final.pt'))\n",
    "\n",
    "#         return self.history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FID Calculation\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_fid(real_features, fake_features):\n",
    "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "\n",
    "    ssdiff = np.sum((mu1 - mu2)**2)\n",
    "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    return ssdiff + np.trace(sigma1 + sigma2 - 2*covmean)\n",
    "\n",
    "def get_inception_features(images, model, device):\n",
    "    from torchvision.models import inception_v3\n",
    "    if model is None:\n",
    "        model = inception_v3(pretrained=True, transform_input=False)\n",
    "        model.fc = nn.Identity()\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Normalize from [-1, 1] to [0, 1] for Inception\n",
    "        images_norm = (images + 1) / 2\n",
    "        images_resized = F.interpolate(images_norm, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        features = model(images_resized)\n",
    "\n",
    "    return features.cpu().numpy(), model\n",
    "\n",
    "# ============================================================================\n",
    "# Visualization Functions\n",
    "# ============================================================================\n",
    "\n",
    "def plot_generated_samples(samples, save_path, title=\"Generated Samples\"):\n",
    "    samples = (samples + 1) / 2\n",
    "    samples = torch.clamp(samples, 0, 1)\n",
    "    grid = make_grid(samples, nrow=8, padding=2)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ“ Saved: {save_path}\")\n",
    "\n",
    "def visualize_latent_interpolation(vqvae, dataloader, device, spatial_h, spatial_w, save_path, num_steps=10):  # FIXED: Added spatial parameters\n",
    "    vqvae.eval()\n",
    "    real_batch = next(iter(dataloader))[:2].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        codes = vqvae.encode(real_batch)\n",
    "        codes = codes.view(2, spatial_h, spatial_w)  # FIXED: Use actual spatial dimensions\n",
    "\n",
    "        interpolations = []\n",
    "        for alpha in np.linspace(0, 1, num_steps):\n",
    "            interp_code = (1 - alpha) * codes[0:1] + alpha * codes[1:2]\n",
    "            interp_code = interp_code.long()\n",
    "            interp_img = decode_codes(vqvae, interp_code, spatial_h, spatial_w)  # FIXED\n",
    "            interpolations.append(interp_img)\n",
    "\n",
    "        interpolation_grid = torch.cat(interpolations)\n",
    "\n",
    "    plot_generated_samples(interpolation_grid, save_path, \"Latent Space Interpolation\")\n",
    "\n",
    "def visualize_tsne(vqvae, dataloader, device, save_path):\n",
    "    print(\"Creating t-SNE visualization...\")\n",
    "    vqvae.eval()\n",
    "    all_codes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= 10:  # Limit samples for speed\n",
    "                break\n",
    "            codes = vqvae.encode(batch.to(device))\n",
    "            all_codes.append(codes.cpu())\n",
    "\n",
    "    all_codes = torch.cat(all_codes, dim=0).numpy()\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    codes_2d = tsne.fit_transform(all_codes[:1000])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(codes_2d[:, 0], codes_2d[:, 1], alpha=0.5, s=10)\n",
    "    plt.title('t-SNE Visualization of Latent Codes', fontsize=14)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ“ Saved: {save_path}\")\n",
    "\n",
    "def visualize_clustering(vqvae, dataloader, device, save_path, n_clusters=10):\n",
    "    print(\"Performing clustering analysis...\")\n",
    "    vqvae.eval()\n",
    "    all_codes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= 10:\n",
    "                break\n",
    "            codes = vqvae.encode(batch.to(device))\n",
    "            all_codes.append(codes.cpu())\n",
    "\n",
    "    all_codes = torch.cat(all_codes, dim=0).numpy()\n",
    "\n",
    "    # K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(all_codes[:1000])\n",
    "\n",
    "    # t-SNE for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    codes_2d = tsne.fit_transform(all_codes[:1000])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(codes_2d[:, 0], codes_2d[:, 1], c=cluster_labels,\n",
    "                         alpha=0.6, s=10, cmap='tab10')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.title(f'K-means Clustering (k={n_clusters}) of Latent Codes', fontsize=14)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ“ Saved: {save_path}\")\n",
    "\n",
    "def plot_prior_training_history(history, save_path):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['epoch'], history['loss'], linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "    plt.title('PixelCNN Prior Training Loss', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ“ Saved: {save_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CSV Export\n",
    "# ============================================================================\n",
    "\n",
    "def save_results_to_csv(config, results, save_path):\n",
    "    \"\"\"Save experiment config and results to CSV\"\"\"\n",
    "    row_data = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        **config.to_dict(),\n",
    "        **results\n",
    "    }\n",
    "\n",
    "    file_exists = os.path.exists(save_path)\n",
    "\n",
    "    with open(save_path, 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=row_data.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_data)\n",
    "\n",
    "    print(f\"âœ“ Results appended to: {save_path}\")\n",
    "\n",
    "def verify_pipeline(vqvae, prior, dataloader, device, spatial_h, spatial_w):\n",
    "    \"\"\"\n",
    "    Verify that the VQ-VAE and Prior are compatible\n",
    "    Diagnostic function to verify everything is working\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ” VERIFYING PIPELINE COMPATIBILITY...\")\n",
    "\n",
    "    vqvae.eval()\n",
    "    prior.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Test with real data\n",
    "        real_batch = next(iter(dataloader))[:4].to(device)\n",
    "\n",
    "        # Encode with VQ-VAE\n",
    "        real_codes = vqvae.encode(real_batch)\n",
    "        real_codes = real_codes.view(-1, spatial_h, spatial_w)\n",
    "        print(f\"Real codes shape: {real_codes.shape}\")\n",
    "\n",
    "        # Test prior forward pass\n",
    "        logits = prior(real_codes)\n",
    "        print(f\"Prior logits shape: {logits.shape}\")\n",
    "\n",
    "        # Test sampling\n",
    "        sample_codes = prior.sample(batch_size=4, device=device)\n",
    "        print(f\"Sampled codes shape: {sample_codes.shape}\")\n",
    "\n",
    "        # Test decoding\n",
    "        reconstructed = decode_codes(vqvae, sample_codes, spatial_h, spatial_w)\n",
    "        print(f\"Decoded images shape: {reconstructed.shape}\")\n",
    "\n",
    "        # Check value ranges\n",
    "        print(f\"Decoded range: [{reconstructed.min():.3f}, {reconstructed.max():.3f}]\")\n",
    "\n",
    "    print(\"âœ“ Pipeline verification complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# Main Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*80)\n",
    "    print(\"PHASE 2: PRIOR TRAINING & COMPLETE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # ========================================================================\n",
    "    # Load Phase 1 Results and Config\n",
    "    # ========================================================================\n",
    "\n",
    "    # Check if Phase 1 completed\n",
    "    phase1_summary_path = './results/phase1_summary.json'\n",
    "    if not os.path.exists(phase1_summary_path):\n",
    "        print(\"\\nâœ— ERROR: Phase 1 not completed. Run phase 1 first!\")\n",
    "        print(\"  Expected file: ./results/phase1_summary.json\")\n",
    "        return\n",
    "\n",
    "    with open(phase1_summary_path, 'r') as f:\n",
    "        phase1_summary = json.load(f)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PHASE 1 SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Codebook usage: {phase1_summary['codebook_usage_percent']:.2f}%\")\n",
    "    print(f\"Active codes: {phase1_summary['active_codes']}/{phase1_summary['total_codes']}\")\n",
    "    print(f\"Target achieved: {phase1_summary['target_achieved']}\")\n",
    "\n",
    "    # Load Phase 1 config from CSV\n",
    "    csv_path = './results/experiment_results.csv'\n",
    "    phase1_config_dict = {}\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        with open(csv_path, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            rows = list(reader)\n",
    "            if rows:\n",
    "                phase1_config_dict = rows[-1]  # Get last experiment\n",
    "                # FIX: Clean up the dictionary by removing None/empty keys\n",
    "                phase1_config_dict = {k: v for k, v in phase1_config_dict.items() \n",
    "                                    if k is not None and k != '' and v is not None and v != ''}\n",
    "                print(f\"\\nâœ“ Loaded Phase 1 config: {phase1_config_dict.get('experiment_name', 'unknown')}\")\n",
    "    else:\n",
    "        print(\"\\nâš  Warning: No CSV found, using default Phase 1 config\")\n",
    "\n",
    "    # Warn if codebook usage is low\n",
    "    if not phase1_summary['target_achieved']:\n",
    "        print(f\"\\nâš  WARNING: Phase 1 codebook usage was {phase1_summary['codebook_usage_percent']:.2f}%\")\n",
    "        min_usage = float(phase1_config_dict.get('min_codebook_usage', 50.0))\n",
    "        print(f\"  Target is {min_usage}%. Generation quality may be affected.\")\n",
    "        response = input(\"\\nContinue anyway? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            return\n",
    "\n",
    "    # Merge configs (Phase 1 + Phase 2)\n",
    "    merged_config = {**phase1_config_dict, **EXPERIMENT_CONFIGS_PHASE2}\n",
    "    config = Config(merged_config)\n",
    "\n",
    "    print(f\"\\nâœ“ Using device: {config.device}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Load Dataset\n",
    "    # ========================================================================\n",
    "\n",
    "    dataset = EmojiDataset(config.data_dir, config.image_size)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size,\n",
    "                             shuffle=True, num_workers=config.num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size,\n",
    "                           shuffle=False, num_workers=config.num_workers, pin_memory=True)\n",
    "\n",
    "    print(f\"\\nâœ“ Dataset: {len(train_dataset)} train, {len(val_dataset)} val\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Load Pre-trained VQ-VAE and Calculate ACTUAL Spatial Size\n",
    "    # ========================================================================\n",
    "\n",
    "    vqvae = VQVAE(config).to(config.device)\n",
    "    vqvae_checkpoint_path = os.path.join(config.checkpoint_dir, 'vqvae_final.pt')\n",
    "\n",
    "    if not os.path.exists(vqvae_checkpoint_path):\n",
    "        print(f\"\\nâœ— ERROR: VQ-VAE checkpoint not found!\")\n",
    "        print(f\"  Expected: {vqvae_checkpoint_path}\")\n",
    "        return\n",
    "\n",
    "    vqvae_checkpoint = torch.load(vqvae_checkpoint_path, map_location=config.device, weights_only=False)\n",
    "    vqvae.load_state_dict(vqvae_checkpoint['model_state_dict'])\n",
    "    vqvae.eval()\n",
    "\n",
    "    print(f\"âœ“ VQ-VAE loaded successfully from epoch {vqvae_checkpoint['epoch'] + 1}\")\n",
    "\n",
    "\n",
    "    # Calculate ACTUAL spatial size\n",
    "    def calculate_spatial_size(vqvae, config, device):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, config.image_size, config.image_size).to(device)\n",
    "            encoded = vqvae.encoder(dummy_input)\n",
    "            spatial_h, spatial_w = encoded.shape[2], encoded.shape[3]\n",
    "            print(f\"âœ“ Actual latent spatial size: {spatial_h}x{spatial_w}\")\n",
    "            return spatial_h, spatial_w\n",
    "\n",
    "    spatial_h, spatial_w = calculate_spatial_size(vqvae, config, config.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Diagnose why PixelCNN is struggling\n",
    "    diagnostics = analyze_code_spatial_structure(\n",
    "        vqvae, train_loader, config.device, spatial_h, spatial_w\n",
    "    )\n",
    "\n",
    "    # Save diagnostics\n",
    "    with open(os.path.join(config.results_dir, 'code_diagnostics.json'), 'w') as f:\n",
    "        json.dump(diagnostics, f, indent=2)\n",
    "\n",
    "    # Decide whether to continue\n",
    "    if diagnostics['learnability_score'] < 30:\n",
    "        response = input(\"\\nâš  Codes have poor learnability. Continue anyway? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            return\n",
    "\n",
    "    # ========================================================================\n",
    "    # Train PixelCNN Prior with CORRECT Spatial Dimensions\n",
    "    # ========================================================================\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PIXELCNN PRIOR TRAINING\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # prior = PixelCNN(\n",
    "    #     num_embeddings=config.num_embeddings,\n",
    "    #     spatial_h=spatial_h,\n",
    "    #     spatial_w=spatial_w,\n",
    "    #     num_layers=config.pixelcnn_layers,\n",
    "    #     hidden_dim=config.pixelcnn_hidden\n",
    "    # ).to(config.device)\n",
    "\n",
    "    # prior_trainer = PriorTrainer(prior, vqvae, config)\n",
    "\n",
    "\n",
    "    prior = ImprovedPixelCNN(\n",
    "        num_embeddings=config.num_embeddings,\n",
    "        spatial_h=spatial_h,\n",
    "        spatial_w=spatial_w,\n",
    "        num_layers=config.pixelcnn_layers,  \n",
    "        hidden_dim=config.pixelcnn_hidden  \n",
    "    ).to(config.device)\n",
    "\n",
    "    prior_trainer = ImprovedPriorTrainer(prior, vqvae, config)\n",
    "\n",
    "    # Verify pipeline compatibility with correct spatial dimensions\n",
    "    verify_pipeline(vqvae, prior, train_loader, config.device, spatial_h, spatial_w)\n",
    "\n",
    "    # Update the trainer call to pass both spatial dimensions\n",
    "    if not prior_trainer.load_checkpoint(os.path.join(config.checkpoint_dir, 'prior_final.pt')):\n",
    "        print(\"\\nâœ“ Training prior from scratch...\")\n",
    "        prior_history = prior_trainer.train(train_loader, spatial_h, spatial_w)  # FIXED: Pass both dimensions\n",
    "\n",
    "        # Plot training history\n",
    "        plot_prior_training_history(prior_history, os.path.join(config.results_dir, 'prior_training.png'))\n",
    "    else:\n",
    "        print(\"\\nâœ“ Using pre-trained prior\")\n",
    "        prior_history = prior_trainer.history\n",
    "\n",
    "    # ========================================================================\n",
    "    # Generate Novel Emojis with CORRECT Decoding\n",
    "    # ========================================================================\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING NOVEL EMOJIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    prior.eval()\n",
    "    vqvae.eval()\n",
    "\n",
    "    print(f\"\\nGenerating {config.num_samples} samples...\")\n",
    "    with torch.no_grad():\n",
    "        generated_codes = prior.sample(\n",
    "            batch_size=config.num_samples,\n",
    "            device=config.device,\n",
    "            temperature=config.temperature\n",
    "        )\n",
    "        generated_images = decode_codes(vqvae, generated_codes, spatial_h, spatial_w)\n",
    "\n",
    "    plot_generated_samples(\n",
    "        generated_images,\n",
    "        os.path.join(config.results_dir, 'generated_samples.png'),\n",
    "        f\"Generated Emojis (Temperature={config.temperature})\"\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Latent Space Interpolation\n",
    "    # ========================================================================\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LATENT SPACE ANALYSIS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    print(\"Performing interpolation...\")\n",
    "    visualize_latent_interpolation(\n",
    "        vqvae, val_loader, config.device, spatial_h, spatial_w,  # FIXED: Added spatial dimensions\n",
    "        os.path.join(config.results_dir, 'interpolation.png'),\n",
    "        num_steps=config.num_interpolation_steps\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # t-SNE Visualization\n",
    "    # ========================================================================\n",
    "\n",
    "    visualize_tsne(\n",
    "        vqvae, val_loader, config.device,\n",
    "        os.path.join(config.results_dir, 'tsne_codes.png')\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Clustering Analysis\n",
    "    # ========================================================================\n",
    "\n",
    "    visualize_clustering(\n",
    "        vqvae, val_loader, config.device,\n",
    "        os.path.join(config.results_dir, 'clustering.png'),\n",
    "        n_clusters=10\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # FID Score Calculation - FIXED SECTION\n",
    "    # ========================================================================\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CALCULATING FID SCORE\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Collect real images\n",
    "    print(\"Collecting real images...\")\n",
    "    real_images = []\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        if i >= 4:  # Use ~256 images\n",
    "            break\n",
    "        real_images.append(batch)\n",
    "    real_images = torch.cat(real_images, dim=0)[:256].to(config.device)\n",
    "\n",
    "    print(f\"âœ“ Real images: {real_images.shape[0]}\")\n",
    "\n",
    "    # Generate matching number of fake images\n",
    "    print(\"Generating images for FID calculation...\")\n",
    "    gen_images = []\n",
    "    num_batches = (len(real_images) + 63) // 64\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            batch_size = min(64, len(real_images) - i * 64)\n",
    "            if batch_size <= 0:\n",
    "                break\n",
    "            # FIXED: Use correct sampling call\n",
    "            codes = prior.sample(batch_size, config.device, temperature=config.temperature)\n",
    "            gen_batch = decode_codes(vqvae, codes, spatial_h, spatial_w)  # FIXED: Use spatial_h, spatial_w\n",
    "            gen_images.append(gen_batch)\n",
    "\n",
    "    gen_images = torch.cat(gen_images, dim=0)[:len(real_images)].to(config.device)\n",
    "    print(f\"âœ“ Generated images: {gen_images.shape[0]}\")\n",
    "\n",
    "    # Extract Inception features\n",
    "    print(\"\\nExtracting Inception features...\")\n",
    "    inception_model = None\n",
    "    real_features, inception_model = get_inception_features(real_images, inception_model, config.device)\n",
    "    gen_features, _ = get_inception_features(gen_images, inception_model, config.device)\n",
    "\n",
    "    print(f\"âœ“ Real features: {real_features.shape}\")\n",
    "    print(f\"âœ“ Generated features: {gen_features.shape}\")\n",
    "\n",
    "    # Calculate FID\n",
    "    fid_score = calculate_fid(real_features, gen_features)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FID SCORE: {fid_score:.2f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Save Comprehensive Results - FIXED SECTION\n",
    "    # ========================================================================\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Prepare comprehensive results for CSV\n",
    "    results = {\n",
    "        'phase': 'phase2',\n",
    "        'fid_score': float(fid_score),\n",
    "        'final_prior_loss': prior_history['loss'][-1] if prior_history['loss'] else None,\n",
    "        'num_generated_samples': config.num_samples,\n",
    "        'spatial_h': spatial_h,  # FIXED: Save both dimensions\n",
    "        'spatial_w': spatial_w,\n",
    "        'temperature': config.temperature,\n",
    "        'phase1_codebook_usage': phase1_summary['codebook_usage_percent'],\n",
    "        'phase1_active_codes': phase1_summary['active_codes'],\n",
    "        'phase1_target_achieved': phase1_summary['target_achieved'],\n",
    "        'generation_completed': True,\n",
    "        'num_prior_epochs': config.num_epochs_prior,\n",
    "        'pixelcnn_layers': config.pixelcnn_layers,\n",
    "        'pixelcnn_hidden': config.pixelcnn_hidden\n",
    "    }\n",
    "\n",
    "    # Save to CSV (appends to same file as Phase 1)\n",
    "    csv_path = os.path.join(config.results_dir, 'experiment_results.csv')\n",
    "    save_results_to_csv(config, results, csv_path)\n",
    "\n",
    "    # Save JSON report for backward compatibility\n",
    "    final_report = {\n",
    "        'fid_score': float(fid_score),\n",
    "        'num_embeddings': config.num_embeddings,\n",
    "        'embedding_dim': config.embedding_dim,\n",
    "        'num_training_samples': len(train_dataset),\n",
    "        'phase1_codebook_usage': phase1_summary['codebook_usage_percent'],\n",
    "        'phase1_active_codes': phase1_summary['active_codes'],\n",
    "        'spatial_h': spatial_h,  # FIXED\n",
    "        'spatial_w': spatial_w,\n",
    "        'temperature': config.temperature,\n",
    "        'prior_layers': config.pixelcnn_layers,\n",
    "        'prior_hidden': config.pixelcnn_hidden,\n",
    "        'final_prior_loss': prior_history['loss'][-1] if prior_history['loss'] else None\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(config.results_dir, 'final_report.json'), 'w') as f:\n",
    "        json.dump(final_report, f, indent=2)\n",
    "\n",
    "    print(\"âœ“ Final report saved: final_report.json\")\n",
    "\n",
    "\n",
    "    # Create comprehensive summary document\n",
    "    summary_text = f\"\"\"\n",
    "{'='*80}\n",
    "VQ-VAE EMOJI GENERATION - COMPLETE PIPELINE RESULTS\n",
    "{'='*80}\n",
    "\n",
    "EXPERIMENT: {config.phase2_experiment_name}\n",
    "TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "{'='*80}\n",
    "PHASE 1: VQ-VAE TRAINING\n",
    "{'='*80}\n",
    "Codebook Size: {config.num_embeddings}\n",
    "Embedding Dim: {config.embedding_dim}\n",
    "Commitment Cost: {config.commitment_cost}\n",
    "Decay: {config.decay}\n",
    "\n",
    "Results:\n",
    "- Codebook Usage: {phase1_summary['codebook_usage_percent']:.2f}%\n",
    "- Active Codes: {phase1_summary['active_codes']}/{phase1_summary['total_codes']}\n",
    "- Target Achieved: {'âœ“' if phase1_summary['target_achieved'] else 'âœ—'}\n",
    "\n",
    "{'='*80}\n",
    "PHASE 2: PRIOR TRAINING & GENERATION\n",
    "{'='*80}\n",
    "Prior Architecture: PixelCNN\n",
    "- Layers: {config.pixelcnn_layers}\n",
    "- Hidden Dim: {config.pixelcnn_hidden}\n",
    "- Learning Rate: {config.learning_rate_prior}\n",
    "- Epochs: {config.num_epochs_prior}\n",
    "\n",
    "Generation:\n",
    "- Spatial Size: {spatial_h}x{spatial_w}  # FIXED\n",
    "- Temperature: {config.temperature}\n",
    "- Samples Generated: {config.num_samples}\n",
    "\n",
    "Results:\n",
    "- FID Score: {fid_score:.2f}\n",
    "\n",
    "{'='*80}\n",
    "OUTPUTS GENERATED\n",
    "{'='*80}\n",
    "âœ“ generated_samples.png - Novel emoji generations\n",
    "âœ“ interpolation.png - Latent space interpolations\n",
    "âœ“ tsne_codes.png - t-SNE visualization\n",
    "âœ“ clustering.png - K-means clustering analysis\n",
    "âœ“ prior_training.png - Prior training curves\n",
    "âœ“ final_report.json - Detailed results\n",
    "âœ“ experiment_results.csv - Complete experiment log\n",
    "\n",
    "{'='*80}\n",
    "NOTES\n",
    "{'='*80}\n",
    "Phase 1: {phase1_config_dict.get('notes', 'N/A')}\n",
    "Phase 2: {config.phase2_notes}\n",
    "\n",
    "{'='*80}\n",
    "END OF REPORT\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "    with open(os.path.join(config.results_dir, 'comprehensive_summary.txt'), 'w') as f:\n",
    "        f.write(summary_text)\n",
    "\n",
    "    print(\"âœ“ Comprehensive summary saved: comprehensive_summary.txt\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Final Output\n",
    "    # ========================================================================\n",
    "\n",
    "    print(summary_text)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PHASE 2 COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"âœ“ All results saved to: {config.results_dir}\")\n",
    "    print(f\"âœ“ FID Score: {fid_score:.2f}\")\n",
    "    print(f\"âœ“ Codebook Usage: {phase1_summary['codebook_usage_percent']:.2f}%\")\n",
    "    print(f\"âœ“ Latent Spatial Size: {spatial_h}x{spatial_w}\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXPERIMENT TRACKING\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"CSV Log: {csv_path}\")\n",
    "    print(f\"Phase 1 Experiment: {phase1_config_dict.get('experiment_name', 'N/A')}\")\n",
    "    print(f\"Phase 2 Experiment: {config.phase2_experiment_name}\")\n",
    "    print(f\"\\nCopy the CSV row to your Excel tracking sheet for comparison!\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

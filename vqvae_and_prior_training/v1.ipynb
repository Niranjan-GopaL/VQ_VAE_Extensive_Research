{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import linalg\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Paths\n",
    "    data_dir = './emoji_data'\n",
    "    checkpoint_dir = './checkpoints'\n",
    "    results_dir = './results'\n",
    "\n",
    "    # Data\n",
    "    image_size = 64\n",
    "    batch_size = 64\n",
    "    num_workers = 2\n",
    "\n",
    "    # VQ-VAE Architecture\n",
    "    num_hiddens = 128\n",
    "    num_residual_hiddens = 32\n",
    "    num_residual_layers = 2\n",
    "    embedding_dim = 64\n",
    "    num_embeddings = 512\n",
    "    commitment_cost = 0.25\n",
    "    decay = 0.999\n",
    "\n",
    "    # Training\n",
    "    num_epochs_vqvae = 100\n",
    "    learning_rate_vqvae = 3e-4\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Prior (PixelCNN)\n",
    "    num_epochs_prior = 50\n",
    "    learning_rate_prior = 1e-4\n",
    "    pixelcnn_layers = 12\n",
    "    pixelcnn_hidden = 64\n",
    "    grad_clip = 1.0\n",
    "\n",
    "    # Generation\n",
    "    num_samples = 64\n",
    "    num_interpolation_steps = 10\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(config.results_dir, exist_ok=True)\n",
    "os.makedirs(config.data_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {config.device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Dataset\n",
    "# ============================================================================\n",
    "\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_size=64, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_size = image_size\n",
    "        self.image_files = [f for f in os.listdir(data_dir)\n",
    "                           if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(image)\n",
    "\n",
    "def download_emoji_dataset():\n",
    "    print(\"Downloading emoji dataset...\")\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        dataset = load_dataset(\"valhalla/emoji-dataset\", split=\"train\")\n",
    "\n",
    "        print(f\"Downloaded {len(dataset)} emojis\")\n",
    "        print(\"Saving images to disk...\")\n",
    "\n",
    "        for idx, item in enumerate(tqdm(dataset)):\n",
    "            img = item['image']\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            img.save(os.path.join(config.data_dir, f'emoji_{idx:05d}.png'))\n",
    "\n",
    "        print(f\"Saved {len(dataset)} emoji images to {config.data_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        print(\"Please manually download emojis to the data directory\")\n",
    "\n",
    "# ============================================================================\n",
    "# VQ-VAE Components\n",
    "# ============================================================================\n",
    "\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "    \"\"\"Vector Quantization with EMA updates\"\"\"\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay=0.99, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        embed = torch.randn(num_embeddings, embedding_dim)\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(num_embeddings))\n",
    "        self.register_buffer(\"embed_avg\", embed.clone())\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        input_shape = inputs.shape\n",
    "        flat_input = inputs.permute(0, 2, 3, 1).contiguous().view(-1, self.embedding_dim)\n",
    "\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
    "                    + torch.sum(self.embed**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self.embed.t()))\n",
    "\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        quantized = torch.matmul(encodings, self.embed)\n",
    "\n",
    "        if self.training:\n",
    "            self.cluster_size.data.mul_(self.decay).add_(encodings.sum(0), alpha=1 - self.decay)\n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self.embed_avg.data.mul_(self.decay).add_(dw, alpha=1 - self.decay)\n",
    "\n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = ((self.cluster_size + self.epsilon) / (n + self.num_embeddings * self.epsilon) * n)\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), flat_input)\n",
    "        loss = self.commitment_cost * e_latent_loss\n",
    "        quantized = flat_input + (quantized - flat_input).detach()\n",
    "\n",
    "        quantized = quantized.view(input_shape[0], input_shape[2], input_shape[3], self.embedding_dim)\n",
    "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        return quantized, loss, perplexity, encoding_indices\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, num_residual_hiddens, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_residual_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_residual_hiddens, num_hiddens, 1, bias=False),\n",
    "            nn.BatchNorm2d(num_hiddens)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            ResidualBlock(in_channels, num_hiddens, num_residual_hiddens)\n",
    "            for _ in range(num_residual_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return F.relu(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, num_hiddens // 2, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(num_hiddens // 2, num_hiddens, 4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(num_hiddens, num_hiddens, 3, padding=1)\n",
    "        self.residual_stack = ResidualStack(num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return self.residual_stack(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(embedding_dim, num_hiddens, 3, padding=1)\n",
    "        self.residual_stack = ResidualStack(num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "        self.conv_trans1 = nn.ConvTranspose2d(num_hiddens, num_hiddens // 2, 4, stride=2, padding=1)\n",
    "        self.conv_trans2 = nn.ConvTranspose2d(num_hiddens // 2, 3, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.residual_stack(x)\n",
    "        x = F.relu(self.conv_trans1(x))\n",
    "        return torch.tanh(self.conv_trans2(x))\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(3, config.num_hiddens, config.num_residual_layers, config.num_residual_hiddens)\n",
    "        self.pre_vq_conv = nn.Conv2d(config.num_hiddens, config.embedding_dim, 1)\n",
    "        self.vq = VectorQuantizerEMA(config.num_embeddings, config.embedding_dim, config.commitment_cost, decay=config.decay)\n",
    "        self.decoder = Decoder(config.embedding_dim, config.num_hiddens, config.num_residual_layers, config.num_residual_hiddens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = self.pre_vq_conv(z)\n",
    "        quantized, vq_loss, perplexity, encoding_indices = self.vq(z)\n",
    "        x_recon = self.decoder(quantized)\n",
    "        return x_recon, vq_loss, perplexity, encoding_indices\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = self.pre_vq_conv(z)\n",
    "        _, _, _, encoding_indices = self.vq(z)\n",
    "        B = x.shape[0]\n",
    "        return encoding_indices.view(B, -1)\n",
    "\n",
    "    def decode_codes(self, encoding_indices, spatial_size):\n",
    "        \"\"\"FIXED: Proper decoding from codes\"\"\"\n",
    "        codes = encoding_indices.view(-1, spatial_size, spatial_size)\n",
    "        quantized = F.embedding(codes, self.vq.embed)\n",
    "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "        return self.decoder(quantized)\n",
    "\n",
    "# ============================================================================\n",
    "# PixelCNN Prior\n",
    "# ============================================================================\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.register_buffer('mask', torch.zeros_like(self.weight))\n",
    "        self.create_mask(mask_type)\n",
    "\n",
    "    def create_mask(self, mask_type):\n",
    "        k = self.kernel_size[0]\n",
    "        self.mask[:, :, :k//2, :] = 1\n",
    "        self.mask[:, :, k//2, :k//2] = 1\n",
    "        if mask_type == 'B':\n",
    "            self.mask[:, :, k//2, k//2] = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super().forward(x)\n",
    "\n",
    "class PixelCNNResidualBlock(nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            MaskedConv2d('B', h, h, 1),\n",
    "            nn.BatchNorm2d(h),\n",
    "            nn.ReLU(),\n",
    "            MaskedConv2d('B', h, h, 1),\n",
    "            nn.BatchNorm2d(h)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv(x)\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, num_embeddings, num_layers=12, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.input_conv = MaskedConv2d('A', num_embeddings, hidden_dim, 7, padding=3)\n",
    "        self.residual_blocks = nn.ModuleList([PixelCNNResidualBlock(hidden_dim) for _ in range(num_layers)])\n",
    "        self.output = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            MaskedConv2d('B', hidden_dim, hidden_dim, 1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, num_embeddings, 1)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, MaskedConv2d)):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_onehot = F.one_hot(x, self.num_embeddings).float()\n",
    "        x_onehot = x_onehot.permute(0, 3, 1, 2).contiguous()\n",
    "        x = self.input_conv(x_onehot)\n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "        return self.output(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size, spatial_size, device, temperature=1.0):\n",
    "        samples = torch.zeros(batch_size, spatial_size, spatial_size, dtype=torch.long, device=device)\n",
    "        for i in range(spatial_size):\n",
    "            for j in range(spatial_size):\n",
    "                logits = self(samples)\n",
    "                probs = F.softmax(logits[:, :, i, j] / temperature, dim=1)\n",
    "                samples[:, i, j] = torch.multinomial(probs, 1).squeeze(-1)\n",
    "        return samples\n",
    "\n",
    "# ============================================================================\n",
    "# Metrics\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_mse(original, reconstructed):\n",
    "    return F.mse_loss(reconstructed, original).item()\n",
    "\n",
    "def calculate_psnr(original, reconstructed, max_val=2.0):\n",
    "    mse = F.mse_loss(reconstructed, original)\n",
    "    return (10 * torch.log10(max_val**2 / mse)).item()\n",
    "\n",
    "def calculate_ssim(original, reconstructed):\n",
    "    orig_np = (original.detach().cpu().numpy() + 1) / 2\n",
    "    recon_np = (reconstructed.detach().cpu().numpy() + 1) / 2\n",
    "    ssim_scores = [ssim(orig_np[i].transpose(1, 2, 0), recon_np[i].transpose(1, 2, 0),\n",
    "                       channel_axis=2, data_range=1.0) for i in range(orig_np.shape[0])]\n",
    "    return np.mean(ssim_scores)\n",
    "\n",
    "def calculate_fid(real_features, fake_features):\n",
    "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2)**2)\n",
    "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    return ssdiff + np.trace(sigma1 + sigma2 - 2*covmean)\n",
    "\n",
    "def get_inception_features(images, model, device):\n",
    "    from torchvision.models import inception_v3\n",
    "    if model is None:\n",
    "        model = inception_v3(pretrained=True, transform_input=False)\n",
    "        model.fc = nn.Identity()\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "    with torch.no_grad():\n",
    "        images_resized = F.interpolate(images, size=(299, 299), mode='bilinear')\n",
    "        features = model(images_resized)\n",
    "    return features.cpu().numpy(), model\n",
    "\n",
    "# ============================================================================\n",
    "# Trainers - FIXED CHECKPOINT LOADING\n",
    "# ============================================================================\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate_vqvae)\n",
    "        self.history = {'recon_loss': [], 'vq_loss': [], 'total_loss': [], 'perplexity': [], 'mse': [], 'psnr': [], 'ssim': []}\n",
    "        self.start_epoch = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch, filepath):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'history': self.history\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Checkpoint saved: {filepath}\")\n",
    "\n",
    "    def load_checkpoint(self, filepath):\n",
    "        \"\"\"FIXED: Proper checkpoint loading\"\"\"\n",
    "        if os.path.exists(filepath):\n",
    "            try:\n",
    "                checkpoint = torch.load(filepath, map_location=self.config.device)\n",
    "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                self.history = checkpoint['history']\n",
    "                self.start_epoch = checkpoint['epoch'] + 1\n",
    "                print(f\"Checkpoint loaded: {filepath}, resuming from epoch {self.start_epoch}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading checkpoint: {e}\")\n",
    "                return False\n",
    "        return False\n",
    "\n",
    "    def train_epoch(self, dataloader):\n",
    "        self.model.train()\n",
    "        epoch_recon_loss = epoch_vq_loss = epoch_perplexity = 0\n",
    "        pbar = tqdm(dataloader, desc=\"Training\")\n",
    "        for batch in pbar:\n",
    "            batch = batch.to(self.config.device)\n",
    "            recon, vq_loss, perplexity, _ = self.model(batch)\n",
    "            recon_loss = F.mse_loss(recon, batch)\n",
    "            loss = recon_loss + vq_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_recon_loss += recon_loss.item()\n",
    "            epoch_vq_loss += vq_loss.item()\n",
    "            epoch_perplexity += perplexity.item()\n",
    "            pbar.set_postfix({'recon_loss': recon_loss.item(), 'vq_loss': vq_loss.item(), 'perplexity': perplexity.item()})\n",
    "        n = len(dataloader)\n",
    "        return {'recon_loss': epoch_recon_loss/n, 'vq_loss': epoch_vq_loss/n, 'perplexity': epoch_perplexity/n}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        all_original, all_recon = [], []\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(self.config.device)\n",
    "            recon, _, _, _ = self.model(batch)\n",
    "            all_original.append(batch)\n",
    "            all_recon.append(recon)\n",
    "        original = torch.cat(all_original, dim=0)\n",
    "        reconstructed = torch.cat(all_recon, dim=0)\n",
    "        return {'mse': calculate_mse(original, reconstructed),\n",
    "                'psnr': calculate_psnr(original, reconstructed),\n",
    "                'ssim': calculate_ssim(original[:64], reconstructed[:64])}\n",
    "\n",
    "    def train(self, train_loader, val_loader):\n",
    "        print(f\"Starting training from epoch {self.start_epoch}\")\n",
    "        for epoch in range(self.start_epoch, self.config.num_epochs_vqvae):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs_vqvae}\")\n",
    "            train_metrics = self.train_epoch(train_loader)\n",
    "            val_metrics = self.evaluate(val_loader)\n",
    "            metrics = {**train_metrics, **val_metrics}\n",
    "            for key, value in metrics.items():\n",
    "                self.history[key].append(value)\n",
    "            print(f\"Recon: {metrics['recon_loss']:.4f}, VQ: {metrics['vq_loss']:.4f}, Perplexity: {metrics['perplexity']:.2f}\")\n",
    "            print(f\"MSE: {metrics['mse']:.4f}, PSNR: {metrics['psnr']:.2f}, SSIM: {metrics['ssim']:.4f}\")\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_checkpoint(epoch, os.path.join(self.config.checkpoint_dir, f'vqvae_epoch_{epoch+1}.pt'))\n",
    "        self.save_checkpoint(self.config.num_epochs_vqvae - 1, os.path.join(self.config.checkpoint_dir, 'vqvae_final.pt'))\n",
    "        return self.history\n",
    "\n",
    "class PriorTrainer:\n",
    "    def __init__(self, prior, vqvae, config):\n",
    "        self.prior = prior\n",
    "        self.vqvae = vqvae\n",
    "        self.config = config\n",
    "        self.optimizer = torch.optim.Adam(prior.parameters(), lr=config.learning_rate_prior)\n",
    "        self.history = {'loss': []}\n",
    "        self.start_epoch = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch, filepath):\n",
    "        torch.save({'epoch': epoch, 'model_state_dict': self.prior.state_dict(),\n",
    "                   'optimizer_state_dict': self.optimizer.state_dict(), 'history': self.history}, filepath)\n",
    "\n",
    "    def load_checkpoint(self, filepath):\n",
    "        if os.path.exists(filepath):\n",
    "            try:\n",
    "                checkpoint = torch.load(filepath, map_location=self.config.device)\n",
    "                self.prior.load_state_dict(checkpoint['model_state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                self.history = checkpoint['history']\n",
    "                self.start_epoch = checkpoint['epoch'] + 1\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading prior checkpoint: {e}\")\n",
    "                return False\n",
    "        return False\n",
    "\n",
    "    def train_epoch(self, dataloader):\n",
    "        self.prior.train()\n",
    "        self.vqvae.eval()\n",
    "        epoch_loss = num_valid = 0\n",
    "        pbar = tqdm(dataloader, desc=\"Training Prior\")\n",
    "        for batch in pbar:\n",
    "            batch = batch.to(self.config.device)\n",
    "            with torch.no_grad():\n",
    "                codes = self.vqvae.encode(batch)\n",
    "                spatial_size = int(np.sqrt(codes.shape[1]))\n",
    "                codes = codes.view(-1, spatial_size, spatial_size)\n",
    "            logits = self.prior(codes)\n",
    "            loss = F.cross_entropy(logits, codes)\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.prior.parameters(), self.config.grad_clip)\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            num_valid += 1\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        return epoch_loss / num_valid if num_valid > 0 else float('inf')\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        for epoch in range(self.start_epoch, self.config.num_epochs_prior):\n",
    "            loss = self.train_epoch(train_loader)\n",
    "            if loss == float('inf'):\n",
    "                break\n",
    "            self.history['loss'].append(loss)\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_checkpoint(epoch, os.path.join(self.config.checkpoint_dir, f'prior_epoch_{epoch+1}.pt'))\n",
    "        self.save_checkpoint(self.config.num_epochs_prior - 1, os.path.join(self.config.checkpoint_dir, 'prior_final.pt'))\n",
    "        return self.history\n",
    "\n",
    "# ============================================================================\n",
    "# Visualization\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes[0, 0].plot(history['recon_loss'])\n",
    "    axes[0, 0].set_title('Reconstruction Loss')\n",
    "    axes[0, 1].plot(history['vq_loss'])\n",
    "    axes[0, 1].set_title('VQ Loss')\n",
    "    axes[0, 2].plot(history['perplexity'])\n",
    "    axes[0, 2].set_title('Perplexity')\n",
    "    axes[1, 0].plot(history['mse'])\n",
    "    axes[1, 0].set_title('MSE')\n",
    "    axes[1, 1].plot(history['psnr'])\n",
    "    axes[1, 1].set_title('PSNR')\n",
    "    axes[1, 2].plot(history['ssim'])\n",
    "    axes[1, 2].set_title('SSIM')\n",
    "    for ax in axes.flat:\n",
    "        ax.grid(True)\n",
    "        ax.set_xlabel('Epoch')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_reconstructions(model, dataloader, device, save_path, num_images=8):\n",
    "    model.eval()\n",
    "    images = next(iter(dataloader))[:num_images].to(device)\n",
    "    with torch.no_grad():\n",
    "        reconstructions, _, _, _ = model(images)\n",
    "    images, reconstructions = (images + 1) / 2, (reconstructions + 1) / 2\n",
    "    comparison = torch.cat([images, reconstructions])\n",
    "    grid = make_grid(comparison, nrow=num_images, padding=2)\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.title('Top: Original, Bottom: Reconstructed')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_generated_samples(samples, save_path, title=\"Generated Samples\"):\n",
    "    samples = (samples + 1) / 2\n",
    "    grid = make_grid(samples, nrow=8, padding=2)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Main\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*80)\n",
    "    print(\"VQ-VAE Emoji Generation Project\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Dataset preparation\n",
    "    if not os.listdir(config.data_dir):\n",
    "        download_emoji_dataset()\n",
    "\n",
    "    dataset = EmojiDataset(config.data_dir, config.image_size)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers, pin_memory=True)\n",
    "\n",
    "    print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "    # Train VQ-VAE\n",
    "    vqvae = VQVAE(config).to(config.device)\n",
    "    trainer = Trainer(vqvae, config)\n",
    "\n",
    "    if not trainer.load_checkpoint(os.path.join(config.checkpoint_dir, 'vqvae_final.pt')):\n",
    "        history = trainer.train(train_loader, val_loader)\n",
    "        plot_training_history(history, os.path.join(config.results_dir, 'training_history.png'))\n",
    "\n",
    "    # Evaluate\n",
    "    plot_reconstructions(vqvae, val_loader, config.device, os.path.join(config.results_dir, 'reconstructions.png'))\n",
    "\n",
    "    # Train Prior\n",
    "    sample_batch = next(iter(train_loader) )\n",
    "    spatial_size = vqvae.encode(sample_batch.to(config.device)).shape[1]\n",
    "    spatial_size = int(np.sqrt(spatial_size))\n",
    "\n",
    "    prior = PixelCNN(config.num_embeddings, config.pixelcnn_layers, config.pixelcnn_hidden).to(config.device)\n",
    "    prior_trainer = PriorTrainer(prior, vqvae, config)\n",
    "\n",
    "    if not prior_trainer.load_checkpoint(os.path.join(config.checkpoint_dir, 'prior_final.pt')):\n",
    "        prior_trainer.train(train_loader)\n",
    "\n",
    "    # Generate new samples\n",
    "    print(\"Generating new samples...\")\n",
    "    with torch.no_grad():\n",
    "        generated_codes = prior.sample(config.num_samples, spatial_size, config.device)\n",
    "        generated_images = vqvae.decode_codes(generated_codes, spatial_size)\n",
    "\n",
    "    plot_generated_samples(generated_images, os.path.join(config.results_dir, 'generated_samples.png'), \"Generated Emojis\")\n",
    "\n",
    "    # Interpolation\n",
    "    print(\"Performing interpolation...\")\n",
    "    with torch.no_grad():\n",
    "        real_batch = next(iter(val_loader))[:2].to(config.device)\n",
    "        codes = vqvae.encode(real_batch)\n",
    "        codes = codes.view(2, spatial_size, spatial_size)\n",
    "\n",
    "        interpolations = []\n",
    "        for alpha in np.linspace(0, 1, config.num_interpolation_steps):\n",
    "            interp_code = (1 - alpha) * codes[0:1] + alpha * codes[1:2]\n",
    "            interp_code = interp_code.long()\n",
    "            interp_img = vqvae.decode_codes(interp_code, spatial_size)\n",
    "            interpolations.append(interp_img)\n",
    "\n",
    "        interpolation_grid = torch.cat(interpolations)\n",
    "        plot_generated_samples(interpolation_grid, os.path.join(config.results_dir, 'interpolation.png'),\n",
    "                              \"Latent Space Interpolation\")\n",
    "\n",
    "    # Codebook analysis\n",
    "    print(\"Analyzing codebook...\")\n",
    "    with torch.no_grad():\n",
    "        all_codes = []\n",
    "        for batch in val_loader:\n",
    "            codes = vqvae.encode(batch.to(config.device))\n",
    "            all_codes.append(codes.cpu())\n",
    "        all_codes = torch.cat(all_codes, dim=0).numpy()\n",
    "\n",
    "    # t-SNE visualization\n",
    "    if all_codes.shape[1] > 2:\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        codes_2d = tsne.fit_transform(all_codes[:1000])\n",
    "    else:\n",
    "        codes_2d = all_codes[:1000]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(codes_2d[:, 0], codes_2d[:, 1], alpha=0.6)\n",
    "    plt.title('t-SNE Visualization of Latent Codes')\n",
    "    plt.savefig(os.path.join(config.results_dir, 'tsne_codes.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Clustering analysis\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(all_codes[:1000])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(codes_2d[:, 0], codes_2d[:, 1], c=cluster_labels, alpha=0.6, cmap='tab10')\n",
    "    plt.title('K-means Clustering of Latent Codes')\n",
    "    plt.savefig(os.path.join(config.results_dir, 'clustering.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Quantitative evaluation\n",
    "    print(\"Performing quantitative evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        real_images = []\n",
    "        gen_images = []\n",
    "\n",
    "        # Get real images\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            if i >= 4:  # Use 256 real images\n",
    "                break\n",
    "            real_images.append(batch.cpu())\n",
    "        real_images = torch.cat(real_images, dim=0)\n",
    "\n",
    "        # Generate matching number of fake images\n",
    "        for i in range(0, len(real_images), config.num_samples):\n",
    "            batch_size = min(config.num_samples, len(real_images) - i)\n",
    "            codes = prior.sample(batch_size, spatial_size, config.device)\n",
    "            gen_batch = vqvae.decode_codes(codes, spatial_size)\n",
    "            gen_images.append(gen_batch.cpu())\n",
    "        gen_images = torch.cat(gen_images, dim=0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    real_images = real_images.to(config.device)\n",
    "    gen_images = gen_images.to(config.device)\n",
    "\n",
    "    # FID calculation\n",
    "    inception_model = None\n",
    "    real_features, inception_model = get_inception_features(real_images, inception_model, config.device)\n",
    "    gen_features, _ = get_inception_features(gen_images, inception_model, config.device)\n",
    "    fid_score = calculate_fid(real_features, gen_features)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"FID Score: {fid_score:.2f}\")\n",
    "\n",
    "    # Save final report\n",
    "    report = {\n",
    "        'fid_score': fid_score,\n",
    "        'num_embeddings': config.num_embeddings,\n",
    "        'embedding_dim': config.embedding_dim,\n",
    "        'num_training_samples': len(train_dataset),\n",
    "        'final_vq_loss': trainer.history['vq_loss'][-1] if trainer.history['vq_loss'] else None,\n",
    "        'final_perplexity': trainer.history['perplexity'][-1] if trainer.history['perplexity'] else None\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(config.results_dir, 'final_report.json'), 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    print(\"\\nGeneration completed successfully!\")\n",
    "    print(f\"Results saved to: {config.results_dir}\")\n",
    "    print(f\"Checkpoints saved to: {config.checkpoint_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
